<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Data Mining 阅读笔记 | Seline's blog</title><meta name="author" content="Seline"><meta name="copyright" content="Seline"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第二章 数据预处理 2.1 简介 收集到的数据很可能是非结构化的，需要从其中提取特征，因此需要数据预处理。 数据预处理的几个步骤：  特征提取和可移植性  数据有多个来源，需要集成到单个数据库；有些算法只能使用特定的数据类型，而数据可能包含不同类型的数据，因此需要数据类型的可移植性   数据清洗  删除&#x2F;插补条目   数据消减、选择和转换  数据量减少：通过采样&#x2F;降维 特征选择：和具体问题高度相关">
<meta property="og:type" content="article">
<meta property="og:title" content="Data Mining 阅读笔记">
<meta property="og:url" content="https://seline02.github.io/2022/09/24/Data-Mining-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Seline&#39;s blog">
<meta property="og:description" content="第二章 数据预处理 2.1 简介 收集到的数据很可能是非结构化的，需要从其中提取特征，因此需要数据预处理。 数据预处理的几个步骤：  特征提取和可移植性  数据有多个来源，需要集成到单个数据库；有些算法只能使用特定的数据类型，而数据可能包含不同类型的数据，因此需要数据类型的可移植性   数据清洗  删除&#x2F;插补条目   数据消减、选择和转换  数据量减少：通过采样&#x2F;降维 特征选择：和具体问题高度相关">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2022-09-24T11:35:08.000Z">
<meta property="article:modified_time" content="2022-11-24T10:23:01.593Z">
<meta property="article:author" content="Seline">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://seline02.github.io/2022/09/24/Data-Mining-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Data Mining 阅读笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-11-24 18:23:01'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Seline's blog</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Data Mining 阅读笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-24T11:35:08.000Z" title="发表于 2022-09-24 19:35:08">2022-09-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-24T10:23:01.593Z" title="更新于 2022-11-24 18:23:01">2022-11-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Data Mining 阅读笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="第二章-数据预处理">第二章 数据预处理</h1>
<h2 id="2-1-简介">2.1 简介</h2>
<p>收集到的数据很可能是非结构化的，需要从其中提取特征，因此需要数据预处理。</p>
<p>数据预处理的几个步骤：</p>
<ul>
<li>特征提取和可移植性
<ul>
<li>数据有多个来源，需要集成到单个数据库；有些算法只能使用特定的数据类型，而数据可能包含不同类型的数据，因此需要数据类型的可移植性</li>
</ul>
</li>
<li>数据清洗
<ul>
<li>删除/插补条目</li>
</ul>
</li>
<li>数据消减、选择和转换
<ul>
<li>数据量减少：通过采样/降维</li>
<li>特征选择：和具体问题高度相关</li>
</ul>
</li>
</ul>
<h2 id="2-2-特征提取和可移植性">2.2 特征提取和可移植性</h2>
<p>收集到的数据是原始、非结构化的（raw, unstructed)，需要转换成统一的形式</p>
<h3 id="2-2-1-特征提取">2.2.1 特征提取</h3>
<p>在某些情况下，特征提取与数据类型可移植性的概念密切相关，其中一种类型的低级特征可能转换为另一种类型的高级特征。特征提取的性质取决于数据应用的领域：</p>
<ul>
<li>**传感器数据：**传感器数据通常以大量的低电平信号（low-level signals）收集。有时使用小波变换或傅里叶变换将低电平信号转换为更高级的特征。</li>
<li><strong>图像数据：</strong>
<ul>
<li>原始形式：图像数据被表示为像素</li>
<li>稍高级别：可以使用颜色直方图来表示图像不同部分的特征（直方图只能描述颜色的分布，不能描述数据几何上的信息）</li>
<li>视觉词袋：类比NLP中的词袋模型。用来表示图像的含义。</li>
<li>图像处理中的一个挑战是数据量大</li>
</ul>
</li>
<li><strong>Web日志</strong>：Web日志通常以预先指定的格式表示为文本字符串。由于这些日志中的字段被明确指定和分隔，因此将Web访问日志转换为分类和数字属性的多维表示相对比较容易。</li>
<li><strong>网络流量</strong>：网络数据包的特征用于分析入侵等。可以从这些数据包中提取各种特征，例如传输的字节数、使用的网络协议等。</li>
<li><strong>文档数据</strong>：文档数据通常是原始和非结构化的。一种方法是删除停用词，词干提取（stemming），并使用词袋模型表示。也可以使用实体抽取。</li>
</ul>
<p>命名实体识别是信息提取的重要子任务。可以用来理解句子和复杂事件的结构。也可以用于填充更传统的关系元素数据库或者更容易分析的原子实体序列。</p>
<h3 id="2-2-2-数据类型可移植性">2.2.2 数据类型可移植性</h3>
<p>数据类型的混合也限制了分析人员使用现成的工具进行处理的能力。在某些情况下移植数据类型确实会丢失表达能力和准确性。理想情况下，最好根据特定的数据类型组合来定制算法以优化结果，但这是耗时的。</p>
<p>数字数据类型是数据挖掘算法中最简单和研究最广泛的数据类型，所以研究如何将不同数据类型转换为数字数据类型尤其有用。</p>
<h4 id="2-2-2-1-数字到分类数据：离散化">2.2.2.1 数字到分类数据：离散化</h4>
<p>离散化过程将数字属性的范围划分为φ个范围。然后，根据原始属性所在的范围，假定该属性包含从1到φ的φ个不同分类标注值。例如，考虑年龄属性。人们可以创建范围[0,10]，[11,20]，[21,30]等等。范围[11,20]中任何记录的符号值为“2”，范围[21,30]中记录的符号值为“3”。由于这些是符号值，因此在值“2”和“3”之间不会进行排序。此外，一个范围内的变化在离散化之后是不可区分的。因此，离散化过程的确会失去一些挖掘过程的信息。</p>
<p>离散化的一个挑战是数据可能在不同的时间间隔内不均匀分布。例如工资，使用相同大小的范围可能对区分不同数据段不是很有帮助。</p>
<p>离散化过程可以根据应用特定目标以各种方式执行：</p>
<h4 id="2-2-2-4-从时序到离散序列">2.2.2.4 从时序到离散序列</h4>
<p>SAX：表示的独特之处在于它允许降维，它还允许在符号表示上定义距离度量</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/340483923">https://zhuanlan.zhihu.com/p/340483923</a></p>
<img src="image-20220930163122771.png" alt="image-20220930163122771" style="zoom:50%;">
<h4 id="2-4-3-1-主成分分析">2.4.3.1 主成分分析</h4>
<img src="image-20220930171447684.png" alt="image-20220930171447684" style="zoom:50%;">
<p>本来的C矩阵是样本d个维度间的协方差，现在投影后就一个维度，变方差而不是协方差了</p>
<h4 id="2-4-3-2-奇异值分解">2.4.3.2 奇异值分解</h4>
<img src="image-20220930182436519.png" alt="image-20220930182436519" style="zoom:67%;">
<p>PCA 捕获尽可能多的数据方差（或者到数据中心点的欧几里得距离平方），而SVD捕获尽可能多的到原点的欧几里得距离平方和</p>
<img src="image-20220930182903334.png" alt="image-20220930182903334" style="zoom:50%;">
<p>可用特征值之和（即奇异值平方后的加和）=矩阵对角线元素之和（方差之和）解释</p>
<h4 id="2-4-3-3-潜在语义分析">2.4.3.3 潜在语义分析</h4>
<img src="image-20220930190237128.png" alt="image-20220930190237128" style="zoom:50%;">
<p>不懂</p>
<h3 id="2-4-4-用类型转换降维">2.4.4 用类型转换降维</h3>
<p>本节将研究两种这样的转换方法：</p>
<p>1、时间序列到多维：使用了许多方法，例如离散傅里叶变换和离散小波变换。 虽然这些方法也可以看作由上下文属性的各种时间戳定义的轴系统的旋转，但数据在旋转后不再依赖于相关性。（不具有相关性，可能指变换到三角正交基上了？） 因此，可以以类似于多维数据的方式处理结果数据集。</p>
<h4 id="2-4-4-1-哈尔-haar-小波变换">2.4.4.1 哈尔(Haar)小波变换</h4>
<img src="image-20220929225513595.png" alt="image-20220929225513595" style="zoom:50%;">
<p>可以看成8个点，系数=左边一半的平均值减右边一半的平均值再除以2，第一个相当于（8-6）/2=1，也相当于那个WAVELET SHAPE和SERIES AVERAGES做内积 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>8</mn><mo>⋅</mo><mn>1</mn><mo>+</mo><mn>6</mn><mo>⋅</mo><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>2</mn><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">(8\cdot 1+6\cdot (-1))/2=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord">/</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></p>
<p>这些基两两正交</p>
<img src="image-20220929225209921.png" alt="image-20220929225209921" style="zoom:50%;">
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>⋅</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2\cdot i-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.74285em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 指的是第几段</p>
<img src="image-20220929230527268.png" alt="image-20220929230527268" style="zoom:50%;">
<p>原始序列中的每个值都可以表示为log2(8)=3小波系数的总和，前面加上正号或负号。</p>
<p>Energy的定义（在奇异值/PCA分解中都有涉及）：energy, which is the sum of squared Euclidean distances from the origin</p>
<p>可以看第46页</p>
<img src="image-20220929233253295.png" alt="image-20220929233253295" style="zoom:50%;">
<p>本小节关于energy的：</p>
<img src="image-20220929233539859.png" alt="image-20220929233539859" style="zoom:50%;">
<img src="image-20220929233605925.png" alt="image-20220929233605925" style="zoom:50%;">
<h4 id="2-4-4-3-谱转换和图嵌入">2.4.4.3 谱转换和图嵌入</h4>
<img src="image-20220930193806998.png" alt="image-20220930193806998" style="zoom:50%;">
<p>PCA是投影到方差大的轴上，图嵌入是使之间有边的两个节点尽量地靠在一起，直观来说就是让方差变小，即聚类</p>
<p>放回/不放回</p>
<h4 id="3-2-1-2-高维数据的影响">3.2.1.2 高维数据的影响</h4>
<img src="image-20220927192124598.png" alt="image-20220927192124598" style="zoom:50%;">
<p>反映从原点出发的最大距离和最小距离之间差异的程度</p>
<h4 id="3-2-1-3-局部不相关特征的影响">3.2.1.3 局部不相关特征的影响</h4>
<p>The additive effffects of the natural variations in the many attribute values may be quite signifificant.没理解</p>
<p>对于一个包含糖尿病患者的集合，特定的某些属性对于距离的计算更加重要，例如血糖水平。另一方面，对于包含癫痫病患者的集合，另一组特征会更加重要。</p>
<p>这里需要理解的关键点是，和距离计算相关的几个特定特征可能有时会对于被比较的特定对象对非常敏感。在预处理过程中全局地筛选特征子集不能解决这个问题，因为特征是否相关是由被考虑的对象对<em>局部地</em>决定的。全局来说，所有特征可能都是相关的。 （没理解）</p>
<p>当许多特征不相关时，不相关特征的加性噪声效果有时可能会表现为距离值的集中。在任意情况下，这样的不相关的特征总是会在距离计算中引入误差。因为高维数据集经常包含各种特征，许多是不相关的，像L2−范数这样使用平方和的方式计算距离，其加性效果会非常具有破坏性。</p>
<h4 id="3-2-1-4-不同-l-p-范数的影响">3.2.1.4 不同 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub><mo>−</mo></mrow><annotation encoding="application/x-tex">L_p-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">−</span></span></span></span>范数的影响</h4>
<p>无穷范数：表示两个对象最不相似的那个维度。研究相似度的问题，如果两个对象在999个属性上有相似的值，那么这两个对象应该被定义为非常相似。但是，单独的一个不相关的属性，如果两个对象在这个属性上相差很多，那么在无穷范数L∞的度量下，这两个对象的距离会被拉的很远。换句话说，局部的相似的属性被无穷范数L∞忽视了。总体上，对于较大的pp值，Lp−范数都是这样：不相关的属性被强调了。</p>
<h4 id="3-2-1-5-基于匹配的相似度计算">3.2.1.5 基于匹配的相似度计算</h4>
<p>由于不相关特征的噪声变化，一对语义相似的对象可能包含不相似的特征值（在沿着该维度的一个标准偏差的水平）。（比如有一个服从于高斯分布的噪声？）</p>
<p>欧几里德度量（通常是Lp−范数）通过使用属性值差异的平方和来达到完全相反的效果。结果，来自不相关属性的“噪声”分量主宰了计算并掩盖了大量相关属性的相似效应。无穷范数是一个极端的例子</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span> 维样本，每个维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>，样本被均分成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">k_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 份</p>
<img src="image-20220927204351968.png" alt="image-20220927204351968" style="zoom:50%;">
<p>这种方法忽略了遥远维度上的不相似程度，因为它常常受噪声支配。</p>
<p>关于d的kd的选择确保了对于低维应用，它通过使用大部分维度而与Lp−范数有一些相似之处;而对于高维应用，它通过在匹配属性上使用相似性来表现类似于类似于文本域的相似性函数。距离函数也被证明对原型最近邻分类应用更有效。（低维的话，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">k_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 设小一点，极端一点，设为样本数，那么肯定在同一个桶中，那么一对数据的每个维度都会被计算相似度，类似于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 范数；高维的话，有些维度噪声大，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">k_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 设大一点，那么只要在某个维度上差异比较大，就不会在一个桶中，这个就不会被计算相似度）</p>
<h4 id="3-2-1-6-数据分布的影响">3.2.1.6 数据分布的影响</h4>
<img src="image-20220927210601133.png" alt="image-20220927210601133" style="zoom:50%;">
<p>图中A,B到O是等距的，但OA轴方差大，OB轴方差小，从统计上来说OA的距离应该小于OB</p>
<p>马氏距离：只需要将变量<code>按照主成分进行旋转</code>，让维度间相互<strong>独立</strong>，然后进行<code>标准化</code>，让维度<strong>同分布</strong>就OK了</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46626607">https://zhuanlan.zhihu.com/p/46626607</a></p>
<h4 id="3-2-1-7-非线性分布：isomap">3.2.1.7 非线性分布：ISOMAP</h4>
<img src="image-20220927214157086.png" alt="image-20220927214157086" style="zoom:50%;">
<p>从欧式距离来说AB更接近，但从全局的数据分布来说不是</p>
<p>直观的基本原理是，只有短暂的点到点跳转才能准确测量该点生成过程中的微小变化。因此，点到点跳跃的总和反映了比点之间的直线距离更准确地从一点到另一（远）点的总变化（距离）。这种距离被称为测地距离。</p>
<p>隐含的假设是，非线性分布是符合<em>局部</em>欧几里得的，但<em>总体来说</em>是非欧几里得的</p>
<p>embedding还没看</p>
<p>先用ISOMAP，再用MDS低维嵌入？</p>
<p>通常，高维数据沿着非线性低维形状排列，这也称为<em>流形</em>。这些流形可以被“扁平化”成为一种新的表示形式，可以有效地使用公制距离。</p>
<h4 id="3-2-1-8-局部数据分布的影响">3.2.1.8 局部数据分布的影响</h4>
<img src="image-20220927224217256.png" alt="image-20220927224217256" style="zoom:50%;">
<p>CD和AB分别的绝对距离一样，但考虑到局部的数据分布，CD的距离应该大于AB</p>
<img src="image-20220927225053444.png" alt="image-20220927225053444" style="zoom:50%;">
<p>所在轴的方差不同，实际上CD间的距离应该比AB间的距离大（可用马氏距离）</p>
<p>Shared Nearest-Neighbor Similarity（针对的是图a的问题，局部数据分布不同）：这个度量是局部敏感的，因为它取决于公共<em>邻居</em>的数量，而不取决于距离的绝对值。</p>
<p>Shared nearest-neighbor methods can be used to defifine a similarity graph：两个点如果有至少一个共享邻居的话，它们之间就连一条边。基于相似图的方法几乎总是局部敏感的，因为它们局限于k−最近邻分布。</p>
<p>Generic Methods：使用各种聚类方法将数据划分为局部区域。在对中的每个对象属于不同区域的情况下，可以使用全局分布，或者可以使用两个局部区域来计算平均值。如果是同一个区域，用局部的马氏距离。另一个问题是算法的第一步（分区过程）本身需要一个用于聚类的距离的概念。</p>
<h4 id="3-2-1-9-计算考虑">3.2.1.9 计算考虑</h4>
<p>For example, methods such as ISOMAP are computationally expensive and hard to implement for very large data sets because these methods scale with at least the <strong>square of the data size</strong>. 例如，像ISOMAP这样的方法计算起来很复杂，而且对于非常大的数据集很难实现，因为这些方法至少按照数据大小的平方进行缩放。     为什么？</p>
<p>新样本不好映射到低维空间（西瓜书）</p>
<h3 id="3-2-2-分类数据">3.2.2 分类数据</h3>
<p>Distance functions are naturally computed as functions of value difffferences along dimensions in <strong>numeric data</strong>, which is ordered. 这个有序指的是空间上的顺序？</p>
<p>分类数据：二值化</p>
<p>对于分类数据的情况，使用相似性函数而不是距离函数更常见，因为离散值可以更自然地匹配。</p>
<img src="image-20220927234301659.png" alt="image-20220927234301659" style="zoom:50%;">
<p>最简单的选择是当xi=yi时将S(xi,yi)设置为1，否则设置0。这也被称为重叠(overlap)(overlap)度量。这一措施的主要缺点是它没有考虑不同属性之间的相对频率。</p>
<p>这一措施的主要缺点是它没有考虑不同属性之间的相对频率。</p>
<p>例如，考虑一个分类属性，其中99％的记录属性值为“正常”，其余记录为“癌症”或“糖尿病”。显然，如果两个记录对这个变量有一个“正常”的值，那么这并没有提供关于相似性的统计意义上的重要信息，因为大多数的对都可能只是碰巧显示了这个模式。但是，如果这两个记录具有与此相匹配的“癌症”或“糖尿病”值，那么它提供了相似性的重要统计证据。这个论点与之前关于全局数据分布重要性的论点类似。<strong>异常的异同在统计上比那些常见的更重要。</strong></p>
<p>在分类数据的情况下，数据集的<em>总体统计特性</em>应该用于计算相似性。这类似于使用马氏距离如何使用全局统计数据更准确地计算相似性。这个想法是，对一个分类属性的异常值的匹配应该比经常出现的值的权重更大。</p>
<p>类似于文本中的IDF（逆文档频率），分类数据用逆出现频率（inverse occurrence frequency）来度量</p>
<img src="image-20220928105318845.png" alt="image-20220928105318845" style="zoom:50%;">
<p>变体：Goodall measure</p>
<p>a higher similarity value is assigned to a match when the value is infrequent</p>
<img src="image-20220928111345131.png" alt="image-20220928111345131" style="zoom:50%;">
<h3 id="3-2-3-混合定量和分类数据">3.2.3 混合定量和分类数据</h3>
<img src="image-20220928112037998.png" alt="image-20220928112037998" style="zoom:50%;">
<p>缺少邻域知识的前提下（不知道哪一种数据更重要）， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span> 可以取全部属性中数值型属性的占比</p>
<p>此外，数值数据中的接近度通常使用距离函数而不是相似度函数来计算。但是，距离值也可以转换为相似度值。对于dist的距离值，常用的方法是使用产生相似度值为1/(1+dist)的核映射（不懂）</p>
<p>normalization：</p>
<img src="image-20220928113942957.png" alt="image-20220928113942957" style="zoom:50%;">
<h2 id="3-3-文本相似性度量">3.3 文本相似性度量</h2>
<p>只用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub><mo>−</mo></mrow><annotation encoding="application/x-tex">L_p-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">−</span></span></span></span> 范数来度量文本之间的距离，会受文本长度的影响</p>
<p>可以用余弦相似度，计算两个文档之间的角度</p>
<img src="image-20220928132923078.png" alt="image-20220928132923078" style="zoom:50%;">
<p>上述措施仅使用属性之间的原始频率。可用全局统计量改进。例如，如果两个文档匹配一个不常见的单词，则它比两个文档匹配一个常见单词的情况更具有相似性。</p>
<p>方法一：逆文档频率：inverse document frequency <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">id_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault">i</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<img src="image-20220928133156159.png" alt="image-20220928133156159" style="zoom:50%;">
<p>随着第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span> 个词在文档中出现频率增加而减小</p>
<p>方法二：在相似性计算之前，可以对频率应用一个如平方根或对数的衰减函数f(⋅)。确保单个词的过度存在不会影响相似性度量。</p>
<img src="image-20220928133617475.png" alt="image-20220928133617475" style="zoom:50%;">
<p>因此，可以如下定义第i个词的归一化频率h(xi):</p>
<img src="image-20220928133757784.png" alt="image-20220928133757784" style="zoom:50%;">
<img src="image-20220928133840356.png" alt="image-20220928133840356" style="zoom:50%;">
<h3 id="3-3-1-二进制和集合数据">3.3.1 二进制和集合数据</h3>
<p>Jaccard 系数：稀疏二进制数据集。它可以被认为是文本数据的一个特殊情况，其中词频是0或1。</p>
<img src="image-20220928140112020.png" alt="image-20220928140112020" style="zoom:50%;">
<p>非对称，比起0值更让注重1值</p>
<p>M11表示A和B对应位都是1的属性的数量</p>
<p>M10表示A中为1，B中对应位为0的总数量</p>
<p>M01表示A中为0，B中对应位为1的总数量</p>
<p>M00表示对应位都为0的总数量</p>
<img src="https://img-blog.csdn.net/20180119101008236?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjgzNjM1NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img" style="zoom:80%;">
<img src="image-20220928140248374.png" alt="image-20220928140248374" style="zoom:50%;">
<p>这个式子和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mover accent="true"><mi>X</mi><mo>ˉ</mo></mover><mo separator="true">,</mo><mover accent="true"><mi>Y</mi><mo>ˉ</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\bar{X},\bar{Y})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.07011em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 第一个等号后的式子一样</p>
<h2 id="3-4-时间相似性度量">3.4 时间相似性度量</h2>
<p>离散序列数据并不总是具有时序性的，因为上下文属性可能表示位置</p>
<ol start="2">
<li>Temporal (contextual) attribute translation: 没看懂</li>
</ol>
<h4 id="3-4-1-2-l-p-范数">3.4.1.2 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>-范数</h4>
<p>在p=2的特殊情况下，如果在表示中保留了大部分较大的小波系数，则可以用小波表示获得精确的距离计算。（不懂）</p>
<h4 id="3-4-1-4-基于窗口的方法">3.4.1.4  基于窗口的方法</h4>
<p>对于长时间序列，全局匹配变得越来越不可能。唯一合理的选择是使用窗口来测量分段的相似度。（长时间序列点对点匹配太复杂，所以换成窗口对窗口匹配？）</p>
<h3 id="3-4-2-离散序列相似性度量">3.4.2 离散序列相似性度量</h3>
<p>离散序列数据的应用领域通常是一对一映射不存在的。</p>
<h4 id="3-4-2-2-最长的公共子序列">3.4.2.2 最长的公共子序列</h4>
<p>长度较长的子序列表示字符串之间的匹配程度更高。</p>
<p>与编辑距离不同，最长的公共子序列（LCSS）是一个相似性函数，因为较高的值表示较大的相似性。</p>
<h2 id="3-5-图的相似性度量">3.5 图的相似性度量</h2>
<h3 id="3-5-1-单个图中两个节点之间的相似性">3.5.1 单个图中两个节点之间的相似性</h3>
<p>通常，<em>距离</em>函数与成本一起工作，而<em>相似</em>函数与权重一起工作。</p>
<h4 id="3-5-1-1-基于结构距离的测量">3.5.1.1 基于结构距离的测量</h4>
<p>基于结构距离的测量不会显著增加一对节点之间路径的多样性，因为它们只关注原始结构距离。（没有考虑一对节点之间是否有多条路径（路径数越多越相关））</p>
<h4 id="3-5-1-2-随机行走相似度">3.5.1.2 随机行走相似度</h4>
<p>此外，在任何给定的节点处，允许以被称为<em>重新启动概率</em>的概率“跳回”到源节点s。这将导致严重偏向源节点的概率分布。与s更类似的节点访问的概率较高。这种方法将很好地适应图3.10所示的情况，因为将更频繁地访问B。</p>
<p>PageRank/SimRank没看</p>
<h3 id="3-5-2-两个图之间的相似性">3.5.2 两个图之间的相似性</h3>
<p>图同构问题：NP-hard</p>
<h2 id="3-7-总结">3.7 总结</h2>
<p>The determination of time-series and discrete-sequence similarity measures is closely related because the latter can be considered the categorical version of the former.</p>
<p>不懂</p>
<p>Similarity and Distance</p>
<p>在数据挖掘的几大问题，比如聚类、异常检测、分类中，都涉及到数据对象之间相似度或者距离的计算。书中用Sim dist这两个函数进行表示</p>
<p>相似度函数下，值越大意味着相似性越强；距离函数下，值越小意味着相似性越强。这是显而易见的</p>
<p>在某些领域，比如空间数据中，我们用距离函数来度量比较自然，在其他的比如文本数据中，我们用相似度函数来度量会比较自然</p>
<p>这章分成5个模块，前四个模块介绍了不同数据类型的距离度量或者相似性度量方式，第五个模块介绍了有监督的相似性度量。</p>
<p>首先讲的是如何设计多维数据的距离函数，本节中讨论的是多维数据中的定量型数据、类别型数据和混合型数据</p>
<p>对于定量型数据，最常见的度量它的距离函数是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 范数，p=1时是曼哈顿距离，p=2时是欧几里得距离</p>
<img src="image-20221002000152399.png" alt="image-20221002000152399" style="zoom:50%;">
<p>虽然 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 范数的可解释性很好，特别是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">L_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 范数和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 范数，但使用的时候还应该考虑数据的维度、分布、特征相关性等等因素。然后接下来作者就讲了9个使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 范数时候需要考虑的因素。</p>
<p>1、我们可以用邻域知识对特征进行加权，更重要的特征赋予更大的权重，也就是利用广义的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 距离，也叫闵可夫斯基距离。</p>
<p>2、第二个影响因素是高维度，也就是维度诅咒，意思是距离函数可能会随着维度的增加而无法准确地反映数据点之间的相对距离，具体来说比如在聚类算法中，它就会把本来无关的数据点分到一组</p>
<p>作者在这里举了一个例子，考虑一个在非负象限的d维单位立方体，一个顶点在原点，然后计算从它的原点出发到立方体内部随机一个点的曼哈顿距离。比如说这个点设为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>，那么它到原点的曼哈顿距离就是它所有维度坐标值之和，并且这些坐标都服从01均匀分布。用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">Y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示01均匀分布的随机变量，那个曼哈顿距离可以表示为<img src="image-20221001151709757.png" alt="image-20221001151709757" style="zoom:50%;">。这个距离值是一个均值为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">d/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">d</span><span class="mord">/</span><span class="mord">2</span></span></span></span>，方差 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span> 为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi mathvariant="normal">/</mi><mn>12</mn></mrow><annotation encoding="application/x-tex">d/12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">d</span><span class="mord">/</span><span class="mord">1</span><span class="mord">2</span></span></span></span>的随机变量。然后对于一个很大的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span> ，由大数定律可知这个随机变量会大概率地落入 <img src="image-20221001152722288.png" alt="image-20221001152722288" style="zoom:50%;">中。这里用的应该是切比雪夫不等式，</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Pr</mi><mo>⁡</mo><mo stretchy="false">[</mo><mi mathvariant="normal">∣</mi><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mover accent="true"><mi>O</mi><mo>ˉ</mo></mover><mo separator="true">,</mo><mover accent="true"><mi>X</mi><mo>ˉ</mo></mover><mo stretchy="false">)</mo><mo>−</mo><mi>E</mi><mo stretchy="false">[</mo><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mover accent="true"><mi>O</mi><mo>ˉ</mo></mover><mo separator="true">,</mo><mover accent="true"><mi>X</mi><mo>ˉ</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mi mathvariant="normal">∣</mi><mo>≥</mo><mi>σ</mi><mo stretchy="false">]</mo><mo>=</mo><mi>Pr</mi><mo>⁡</mo><mo stretchy="false">[</mo><mi mathvariant="normal">∣</mi><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mover accent="true"><mi>O</mi><mo>ˉ</mo></mover><mo separator="true">,</mo><mover accent="true"><mi>X</mi><mo>ˉ</mo></mover><mo stretchy="false">)</mo><mo>−</mo><mfrac><mi>d</mi><mn>2</mn></mfrac><mi mathvariant="normal">∣</mi><mo>≥</mo><mn>3</mn><mi>σ</mi><mo stretchy="false">]</mo><mspace linebreak="newline"></mspace><mo>≤</mo><mfrac><mn>1</mn><mrow><mn>9</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mi>V</mi><mi>a</mi><mi>r</mi><mo stretchy="false">(</mo><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mover accent="true"><mi>O</mi><mo>ˉ</mo></mover><mo separator="true">,</mo><mover accent="true"><mi>X</mi><mo>ˉ</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>σ</mi><mn>2</mn></msup><mrow><mn>9</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>9</mn></mfrac></mrow><annotation encoding="application/x-tex">\Pr[|Dist(\bar{O},\bar{X})-E[Dist(\bar{O},\bar{X})]|\geq \sigma]
=\Pr[|Dist(\bar{O},\bar{X})-\frac{d}{2}|\geq 3\sigma] \\
\leq \frac{1}{9\sigma^2}Var(Dist(\bar{O},\bar{X}))=\frac{\sigma^2}{9\sigma^2}=\frac{1}{9}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.07011em;vertical-align:-0.25em;"></span><span class="mop">Pr</span><span class="mopen">[</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">O</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.07011em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">O</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">]</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.07011em;vertical-align:-0.25em;"></span><span class="mop">Pr</span><span class="mopen">[</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">O</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.7719400000000001em;vertical-align:-0.13597em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault">i</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">O</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8201099999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.177108em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.491108em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">9</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>也就是说所有数据中，至少有8/9（或88.9%）的数据位于平均数3个标准差范围内。</p>
<p>所以大部分点的距离都在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 之间变化。</p>
<p>然后作者定义了一个关于维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span> 的变化率，它是距离变化量和距离期望的比值。它衡量了到原点的最小和最大距离之间具体有多大差距。可以看到随着维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span> 增加，对比度降低，说明<strong>不同数据点之间越来越难区分</strong>（区分度变差），这对数据挖掘算法肯定是不利的</p>
<img src="image-20221001155715428.png" alt="image-20221001155715428" style="zoom:50%;">
<p>3、第三点是局部不相关特征的影响。是指在高维的数据中，对于某一个特定问题，某些特征是与问题无关的，这些无关特征的叠加很有可能产生很大的噪声，比如欧氏距离中有平方和的叠加，那些无关特征维度产生的噪声可能会使计算出来的距离值很大</p>
<p>4、第四点是使用不同的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 范数会对距离度量产生不同的影响。考虑极端情况，就是用无穷范数来度量，它只会对两个对象间最不相似的维度进行比较。但如果一个1000维的数据，就算两个数据在999维上都很相似，产生决定性影响的也只有那个不相似的维度，也就是过分关注无关属性。而且这种情况对于较大的p值更为明显。</p>
<img src="image-20221001163602195.png" alt="image-20221001163602195" style="zoom:50%;">
<img src="image-20221001163907999.png" alt="image-20221001163907999" style="zoom:50%;">
<p>这两张图可以说明维度越大，p值越大，距离对比度就越小，而且维度越大，距离对比度就下降得越快</p>
<p>3.2 b这张图的纵坐标是不同维度下各个范数的对比度与曼哈顿距离对比度的比值</p>
<p>但是对于二维数据，对比度几乎没怎么下降，说明p的取值在低维数据中不那么重要</p>
<p>然后作者把这个归一化后的p值叫做分数度量。经验上来说，维度越大，p值应该越低</p>
<p>（为啥要指导p的选择？）</p>
<p>5、第五点叫做基于匹配的相似度计算。之前提到距离度量中，无关属性的噪声可能占支配地位，掩盖了其他相似的属性，所以我们现在不计算距离了，而是去计算相似度，更加关注相关的属性。</p>
<p>一种方法叫做邻近阈值。它把每个维度离散成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">k_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 个等高的桶，每个桶里面的数据量一样。</p>
<p>比如对于两个d维数据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">X,Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>，如果它们在第 i 维上的值在同一个桶中，就说X,Y两个数据在第i维上是邻近的。所有邻近的维度用一个集合表示，叫邻近集。我们可以计算X,Y两者的相似度<img src="image-20221001170351432.png" alt="image-20221001170351432" style="zoom:50%;">，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">n_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是它们第i个维度所在桶的上下界。</p>
<p>从这个式子可以看出，它忽略了那些不相似的维度，对于那些相似的维度，用0到1之间的一个值表示某个维度上的相似程度。这个式子的值越大，表示两条数据越相似。</p>
<p>然后 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">k_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 这个值是根据维度 d设置的，经验上来说维度越高，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">k_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 设得越大，会使得对每个维度都筛选的更严格，可以筛掉很多无关的维度；低维的情况下，数据本来维度就少，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">k_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 就设得小一些</p>
<p>作者说这类函数在最近邻分类中比较有效</p>
<p>6、数据分布也会影响 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 范数的距离度量。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 范数计算两点间距离时，并没有考虑全局统计量，但实际上两点间的距离也依赖于其他数据的分布情况。</p>
<img src="image-20221001175042596.png" alt="image-20221001175042596" style="zoom:50%;">
<p>比如在这张图中，A和B到原点O的距离是相等的，但实际上OA方向的方差较大，统计上来说会有更多的点落在离O点较远的地方，而OB方向方差较小，离O点近的可能性更大。因此OA的距离应该小于OB的距离</p>
<p>为了消除数据分布对距离度量的影响，可以用马氏距离。马氏距离只需要将变量<code>按照主成分进行旋转</code>，让维度间相互<strong>独立</strong>，然后进行<code>标准化</code>，就是除以那个维度的标准差</p>
<p>7、第七个它介绍了一下ISOMAP，一个非线性降维的方法。它针对的是数据是非线性分布的情况。比如这张图中如果只考虑欧氏距离，AC离得是最近的，但考虑数据的具体分布，实际上AC是离得最远的。具体的算法在上学期的智能系统课里面已经讲过。主要就是计算高维的流形中点与点之间的测地线距离，得到距离矩阵，然后就可以用MDS算法将数据嵌入到低维</p>
<img src="image-20221001183021327.png" alt="image-20221001183021327" style="zoom:50%;">
<p>8、第八点是设计距离函数的时候还要考虑局部数据的分布。前面只考虑了全局的数据分布，但是在不同的区域，数据的分布也可能不同，具体反映在两点，一个是不同区域数据的密度不同，另一个是簇的形状可能不同。</p>
<p>左边这张图举的例子是数据密度不同，虽然CD，AB距离看上去一样，但考虑到局部的密度，CD间的距离应该更大。LOF（局部异常因子）就是一种基于密度的局部离群点检测算法，它主要的思想就是，如果一个点的密度越小于其周围点的密度，就越可能是离群点；如果一个点的密度越大于其周围点的密度，就越可能是正常点</p>
<p>针对这种局部密度不同的情况，还可以用共享最近邻相似度的方法来度量。在预处理阶段就计算每个数据点的k近邻，将共享最近邻相似度定义为两个数据之间的共同邻居数，这种度量就不用再依赖绝对距离了。在这个基础上可以定义相似图，其中至少有一个共享邻居的两个数据点之间用一条边相连。</p>
<p>右边这张图举得例子是各个簇的形状不同，AB方向的方差明显大于CD方向的方差，不同簇有不同的协方差矩阵，可以先聚类，然后用这些统计量分别计算每个簇的局部的马氏距离。可以得到CD间的实际距离应该更大</p>
<p>9、设计距离函数时还要考虑计算复杂度</p>
<p>预处理和距离计算的<strong>权衡</strong></p>
<p>没讲什么，就举了一个ISOMAP的例子（Floyd算法n的三次方），它在对数据进行预处理时开销很大，但是预处理只需做一次，将数据的表示形式转换后距离的计算就可以加快</p>
<p>接下来讲的是离散的类别型数据如何计算距离。一种方法是用二值化的方法把类别型数据转化成数值型数据，并且一般是稀疏的。而且对于类别型数据，使用相似度来度量比使用距离来度量更加常见</p>
<p>X，Y是两条数据，它们的相似度可以定义为每个维度相似度之和。对每个维度来说，最简单的办法是直接把xi，yi相等时设为1，否则为0，也叫做重叠度量。缺点是没有考虑不同属性的相对频率。比如有一个类别型属性，有三个类别，正常、癌症、糖尿病，99%的概率都是正常的，那么如果这两条数据的这个类别都属于正常，其实并没有在相似性度量上提供太多的信息；但如果这两个都属于癌症，或者糖尿病，那么这种非典型的数据对相似性度量来说应该是比较重要的。可以与之前的全局数据分布进行类比，之前我们用全局统计量先归一化再计算马氏距离，这里我们同样可以利用全局统计量，赋予不常见的类别更大的匹配权重，比如两个都属于癌症的相似度会大于两个都属于正常的相似度。</p>
<p>具体可以用逆出现频率（inverse occurrence frequency）来度量</p>
<img src="image-20220928105318845.png" alt="image-20220928105318845" style="zoom:50%;">
<p>变体：Goodall measure</p>
<img src="image-20220928111345131.png" alt="image-20220928111345131" style="zoom:50%;">
<p>第三种情况是定量型数据和类别型数据的混合。比如这样两条数据，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>Y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">X_n,Y_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是定量型数据，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>c</mi></msub><mo separator="true">,</mo><msub><mi>Y</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">X_c,Y_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是类别型数据，它们整体的相似度可以这样定义</p>
<img src="image-20221001204936825.png" alt="image-20221001204936825" style="zoom:67%;">
<img src="image-20220928112037998.png" alt="image-20220928112037998" style="zoom:50%;">
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span> 用来表示数值型数据和类别型数据的相对重要程度，如果缺少相应的邻域知识， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span> 可以取数值型属性在全部属性中的占比。</p>
<p>另外，数值数据通常用距离函数而不是相似度来度量。但是我们可以把距离转换为相似度值，比如可以用这样一个映射1/(1+dist)。</p>
<p>接下来还可以再做一下归一化，因为衡量这两类属性的相似性的尺度可能不同</p>
<img src="image-20220928113942957.png" alt="image-20220928113942957" style="zoom:50%;">
<p>3.3 文本相似性度量</p>
<p>如果用词袋模型表示文本，文本可以看成是定量型的多维数据，每一维是对应某个词出现的频率。这种表示方法下的数据是稀疏的，并且距离度量受文本长度的影响。（？）</p>
<p>介绍了两种衡量文本间相似度的方法</p>
<p>1、一种解决方法是用余弦度量。它计算的是文本转化成向量后两者间的角度。这里的d指的是词典大小是d</p>
<img src="image-20220928132923078.png" alt="image-20220928132923078" style="zoom:50%;">
<p>这种方法用的是单词原始的出现频率，和之前计算类别型数据的相似度一样，我们可以引入全局统计量。比如，两个文本同时出现了一个不常见的单词，比它们同时出现一个常见单词的相似度更高。</p>
<p>因此一种改进方法是用逆文档频率：inverse document frequency <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">id_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault">i</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<img src="image-20220928133156159.png" alt="image-20220928133156159" style="zoom:50%;">
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span>是文本的数量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">n_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是含有第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span> 个词的文本数量，单词出现越频繁，IDF越小</p>
<p>另一种改进的方法是在计算相似性之前，对词频施加一个平方根或对数的衰减函数f(⋅)。确保频繁出现的词不会影响相似性度量。</p>
<img src="image-20220928133617475.png" alt="image-20220928133617475" style="zoom:50%;">
<p>同时使用两种方法，就可以将第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>个词的频率这样定义 ：</p>
<img src="image-20220928133757784.png" alt="image-20220928133757784" style="zoom:50%;">
<p>然后再使用余弦度量</p>
<img src="image-20220928133840356.png" alt="image-20220928133840356" style="zoom:50%;">
<p>2、另外一种文本度量方式是 Jaccard系数，它主要用在稀疏的二元数据集，但也可以看作是文本数据的一个特殊情况，其中词频是0或1。这里主要讲一下它应用在二元数据中的意义。二元多维数据可以用来表示集合，0表示不在集合中，1表示在集合中。它的定义是交集和并集的比值，表示两个集合的相似度。</p>
<p>也是非对称的一种度量方式，因为它只关心双方共同含有的元素，忽略了那些都没有的元素</p>
<img src="image-20220928140112020.png" alt="image-20220928140112020" style="zoom:50%;">
<img src="image-20221001163024871.png" alt="image-20221001163024871" style="zoom:50%;">
<ul>
<li>CURE聚类：基于质心（代表：K-means）和基于代表对象方法（代表：K-Medoids）之间的中间策略</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55105352">https://zhuanlan.zhihu.com/p/55105352</a></p>
<p><a target="_blank" rel="noopener" href="https://wenku.baidu.com/view/ba21eb315a8102d276a22f4c.html">https://wenku.baidu.com/view/ba21eb315a8102d276a22f4c.html</a> 具体实现</p>
<p>c个代表点体现了簇的物理几何形状；向中心收缩可以降低异常点的影响。两个簇组合后的新簇，则重新选择c个点作为簇的代表。</p>
<p><a target="_blank" rel="noopener" href="https://wenku.baidu.com/view/2c268ec6c381e53a580216fc700abb68a882ad02.html?fr=income6-doc-search%E6%8C%BA%E8%AF%A6%E7%BB%86">https://wenku.baidu.com/view/2c268ec6c381e53a580216fc700abb68a882ad02.html?fr=income6-doc-search挺详细</a> birch都有</p>
<p>7.2 类别型数据的聚类</p>
<p>如果是数值型的数据，我们可以在上面进行距离计算、确定代表点等等，</p>
<p>类别型数据二元化，像购物篮，某个布尔属性代表是否有这个商品</p>
<p><a target="_blank" rel="noopener" href="https://wenku.baidu.com/view/5f4b9022482fb4daa58d4b17.html?fr=income1-wk_app_search_ctr-search">https://wenku.baidu.com/view/5f4b9022482fb4daa58d4b17.html?fr=income1-wk_app_search_ctr-search</a></p>
<p><a target="_blank" rel="noopener" href="https://wenku.baidu.com/view/c35a2f6c443610661ed9ad51f01dc281e53a56eb.html?fr=income1-wk_app_search_ctr-search">https://wenku.baidu.com/view/c35a2f6c443610661ed9ad51f01dc281e53a56eb.html?fr=income1-wk_app_search_ctr-search</a></p>
<p><a target="_blank" rel="noopener" href="https://www.inf.ed.ac.uk/teaching/courses/dme/studpres/Rock.pdf">https://www.inf.ed.ac.uk/teaching/courses/dme/studpres/Rock.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40677836/article/details/120872389">https://blog.csdn.net/weixin_40677836/article/details/120872389</a></p>
<ul>
<li>ROCK的链接指的是两个对象间共有的近邻数</li>
<li>鲁棒指的是：</li>
</ul>
<img src="image-20221022011147155.png" alt="image-20221022011147155" style="zoom:50%;">
<ul>
<li>先介绍有关概念</li>
<li>
<img src="image-20221024002437655.png" alt="image-20221024002437655" style="zoom:50%;">
</li>
<li>
<img src="image-20221024002509063.png" alt="image-20221024002509063" style="zoom:50%;">
</li>
<li>
<img src="image-20221024002652173.png" alt="image-20221024002652173" style="zoom:50%;">
</li>
<li>
<img src="image-20221024002544120.png" alt="image-20221024002544120" style="zoom:50%;">
</li>
<li>
<img src="image-20221024002713908.png" alt="image-20221024002713908" style="zoom:50%;">
</li>
</ul>
<img src="image-20221024002342188.png" alt="image-20221024002342188" style="zoom:50%;">
<img src="image-20221024002740705.png" alt="image-20221024002740705" style="zoom:50%;">
<img src="image-20221024002807625.png" alt="image-20221024002807625" style="zoom:50%;">
<img src="image-20221024002828184.png" alt="image-20221024002828184" style="zoom:50%;">
<p>example：</p>
<img src="image-20221024012622386.png" alt="image-20221024012622386" style="zoom:33%;">
<img src="image-20221024012647162.png" alt="image-20221024012647162" style="zoom:33%;">
<img src="image-20221024012731063.png" alt="image-20221024012731063" style="zoom:33%;">
<img src="image-20221024012745722.png" alt="image-20221024012745722" style="zoom:33%;">
<img src="image-20221024012817338.png" alt="image-20221024012817338" style="zoom:33%;">
<img src="image-20221022012843159.png" alt="image-20221022012843159" style="zoom:50%;">
<ul>
<li>particularly large data-sets</li>
</ul>
<img src="image-20221024192756323.png" alt="image-20221024192756323" style="zoom:33%;">
<img src="image-20221024192838457.png" alt="image-20221024192838457" style="zoom:33%;">
<img src="image-20221024193220168.png" alt="image-20221024193220168" style="zoom:33%;">
<img src="image-20221024193435325.png" alt="image-20221024193435325" style="zoom:50%;">
<img src="image-20221024193445954.png" alt="image-20221024193445954" style="zoom:50%;">
<img src="image-20221024193456290.png" alt="image-20221024193456290" style="zoom:50%;">
<img src="image-20221024193510385.png" alt="image-20221024193510385" style="zoom:50%;">
<img src="image-20221024193551692.png" alt="image-20221024193551692" style="zoom:33%;">
<img src="image-20221024193604509.png" alt="image-20221024193604509" style="zoom:33%;">
<img src="image-20221024192940643.png" alt="image-20221024192940643" style="zoom:50%;">
<img src="image-20221024192957561.png" alt="image-20221024192957561" style="zoom:33%;">
<img src="image-20221024193037591.png" alt="image-20221024193037591" style="zoom:33%;">
<img src="image-20221024193111301.png" alt="image-20221024193111301" style="zoom:33%;">
<img src="image-20221024193144142.png" alt="image-20221024193144142" style="zoom:33%;">
<p>高斯混合聚类：</p>
<p>E步：计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">x_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 由各混合成分生成的后验概率</p>
<p>M步：通过最大化对数似然函数来计算模型参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>（混合系数）、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo separator="true">,</mo><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\mu,\Sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">Σ</span></span></span></span></p>
<img src="image-20221023162019623.png" alt="image-20221023162019623" style="zoom:50%;">
<img src="image-20221023163401076.png" alt="image-20221023163401076" style="zoom:50%;">
<img src="image-20221023164021384.png" alt="image-20221023164021384" style="zoom:50%;">
<img src="image-20221023164258726.png" alt="image-20221023164258726" style="zoom:50%;">
<img src="image-20221023165245820.png" alt="image-20221023165245820" style="zoom:50%;">
<img src="image-20221023165841556.png" alt="image-20221023165841556" style="zoom:50%;">
<img src="image-20221023165901275.png" alt="image-20221023165901275" style="zoom:50%;">
<img src="image-20221023170001256.png" alt="image-20221023170001256" style="zoom:50%;">
<img src="image-20221023170009851.png" alt="image-20221023170009851" style="zoom:50%;">
<img src="image-20221023170053201.png" alt="image-20221023170053201" style="zoom:50%;">
<img src="image-20221023170105838.png" alt="image-20221023170105838" style="zoom:50%;">
<img src="image-20221023170123376.png" alt="image-20221023170123376" style="zoom:50%;">
<img src="image-20221023170140118.png" alt="image-20221023170140118" style="zoom:50%;">
<img src="image-20221023170155282.png" alt="image-20221023170155282" style="zoom:50%;">
<img src="image-20221023170212419.png" alt="image-20221023170212419" style="zoom:50%;">
<p>上一章：讲了几类基本的数据聚类方法：<strong>基于代表点的算法、层次聚类算法、基于概率模型的算法、基于密度的算法</strong></p>
<p>数据的类型、规模和维度都会对聚类造成一定影响，所以前三部分会分别讨论类别型数据的聚类、可扩展的聚类、高维数据的聚类。</p>
<p>有监督聚类、集成聚类</p>
<p>对于类别型数据，也可以用这几大类聚类方法来进行，大框架没有变，只是点与簇，或者簇与簇之间相似度或者距离的衡量方式有所改变，这一小节讲的主要是数据类型改变后，这些细节上的变化</p>
<p>框架：先选取每个簇的代表点，再把每个点分到最近的簇。具体的需要考虑…（主要是对k-means的扩展）</p>
<p>首先有所不同的是 质心的选择</p>
<p>图：二维的类别型数据的簇，有两个属性，所以它的质心由这两个属性的直方图组成</p>
<p>相似性：每个属性值在这个簇中的概率值之和</p>
<p>Data1 blue在这个直方图中的概率值和Square在这个直方图中的概率值之和（<strong>每个属性值对应直方图频率之和</strong>）</p>
<p>下面是两个具体的用于类别型数据聚类的算法</p>
<p>K-modes聚类，把簇中某个属性的众数当作代表点在这个属性上的取值</p>
<p>属性值不平衡，计算相似度时误差大，这种情况下我们应该给罕见的属性值分配更大的权重，可以用总体频率的倒数</p>
<p>ROCK主要针对二元化的类别型数据，典型代表是项集</p>
<p>购物篮数据：二元数据集，每一项都是是否购买某个商品，用01表示，数据集稀疏的，用计算众数的方法计算可能很多簇中的属性都是0</p>
<p>用逆出现频率：<strong>在某个簇中，每个属性值出现的频率去除以这个属性值的全局频率 ，标准化后再计算众数</strong></p>
<p>好处：类别型数据取代直方图，方便用相似度函数计算。前面提到用直方图表示质心，主要是想扩展数值型数据中k-means的质心</p>
<p>k-medoids算法，代表点是数据集中的数据</p>
<p>层次算法：前一章介绍的自底向上凝聚的算法框架，只要把数值型数据的距离计算改成类别型数据的相似度计算即可。这里主要介绍的是另一种专门针对类别型数据的聚类算法</p>
<p>之前讲过的项集就是一种二元化类别型数据的代表</p>
<p>鲁棒体现在用一个更加<strong>全局</strong>的视角来评估两个点的相似性</p>
<p>算法流程：随机采样部分数据是为了更好地适应比较大的数据集</p>
<p>算法框架就是之前讲过的那个自底向上凝聚的过程</p>
<p>直观来看，当两个簇的共享近邻数非常大时，它们应该合并，可以用grouplink来表示它们的共享近邻数；但是越大的簇之间它们的交叉连接理应会越多，会对簇间相似度的衡量造成影响，所以用交叉链接的期望值进行规范化</p>
<p>链接数矩阵（共同邻居数）</p>
<p>基于概率模型的聚类算法</p>
<p>前一章：给定一个<strong>混合生成模型</strong>，可以用EM算法进行迭代计算数据点属于每一个簇的概率，以及模型的参数。扩展到类别型数据，</p>
<p>数值型数据，常把每个簇的分布假设为高斯分布，簇i生成数据点j的概率密度函数为…，在类别型数据中，<strong>概率密度函数</strong>要表示成离散的<strong>概率分布</strong>，这里假设数据的属性之间都是独立的，可以把簇m中的离散概率分布表示为…<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>j</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">j_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05724em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是数据点X在第r个属性上的取值，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>r</mi><msub><mi>j</mi><mi>r</mi></msub><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{rj_rm}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.05724em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>表示簇m中的数据在第r个属性取值为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>j</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">j_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.05724em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的概率</p>
<p>M步中求簇的先验分布 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span> 没变，原来概率密度函数中的参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo separator="true">,</mo><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\mu,\Sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">Σ</span></span></span></span> 变为求概率分布中的参数值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi>i</mi><mi>j</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_{ijm}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi>i</mi><mi>j</mi><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_{ijm}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mord mathdefault mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> ：所有数据分配到簇m的概率之和是分母，分子是<strong>其中第i个属性取值为j的数据分配到簇m的概率之和</strong></p>
<p>可扩展的数据聚类，针对较大规模的数据，这种情况下，内存受限，需要更加高效的算法</p>
<p>k-medoids算法的一个特例是每次要交换k个中心点和剩余的n-k个点，一共交换k(n-k)次，选择目标函数优化最明显的一对交换，对于一个d维的数据集，迭代一次的时间复杂度是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>k</mi><msup><mi>n</mi><mn>2</mn></msup><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(kn^2d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">d</span><span class="mclose">)</span></span></span></span>，对于大的数据集来说很耗时，</p>
<p>CLARA的**主要思想是先进行采样，在小规模的数据上进行PAM算法（f是采样因子），剩下的点分配给最近的簇。**然后重复多次，选效果最好的一次。时间复杂度：当f很小的时候速度快；缺点是如果采样的样本里面有好的中心点，效果就好；如果采不好，效果就不好</p>
<p>CLARANS和CLARA不一样，它是在整个空间内随机搜索，避免了采样带来的问题</p>
<p>maxattempt表示<strong>每次局部迭代时最多进行几次替换操作</strong></p>
<p>CLARA只有在数据采样的时候有随机性；CLARANS在每一步搜索的时候都有随机性</p>
<p>CLARANS不会把搜索限制在局部区域，如果发现一个更好的近邻，CLARANS就移到该近邻节点，处理过程从新开始</p>
<p>避免采样数据质量的影响；在整个数据空间随机搜索</p>
<p>BIRCH：扫描所有数据，建立CF树，数据的信息保留在内存中  ，稀疏的簇可当作离群点删除</p>
<p>CF向量记录了一个簇的统计特性，可以用来计算簇的质心、直径等，它的累加性可用在聚类过程中动态地更新新聚类的CF向量，每一维对应相加即可</p>
<p>簇直径阈值T：一个数据点插入到最近的簇中时，若簇直径超过阈值，则叶节点要分裂；</p>
<p>B/L分别是内部节点和叶节点的分支因子，书里把这两个当作一样的，这里把他们区分开来了，代表每个节点里有不超过几个子簇</p>
<p>CF树建立的过程是逐个扫描数据并插入的过程。一个插入并分裂的例子。插入的时候一路更新分支上的CF向量。分裂时可以将节点内质心距离最远的两个CF向量作为分裂的种子，其他的CF向量分配给它们</p>
<p>加快步骤3的速度，同时可去掉异常簇（小的簇）</p>
<p>第三步全局聚类：对叶节点进一步利用一个<strong>全局性</strong>的聚类算法（比如k-means），改进聚类质量。 由于CF树的叶节点代表的聚类可能不是自然的聚类结果，原因是给定的阈值限制了簇的大小，并且数据的<strong>输入顺序</strong>也会影响到聚类结果（可能两个相同的数据点因为插入顺序而分配到不同的子簇）。因此，需要对叶节点进一步利用一个全局性的聚类算法，改进聚类质量。（有些是为了分裂而分裂，到了那个阈值不得不分裂）</p>
<p>第四步：聚类细化。根据全局聚类步骤产生的聚类质心重新分配所有的数据点。   也可以再次移除异常点</p>
<p>其实第三步也可以看作是可选的</p>
<p>缺点：假设簇是球形的，对非球形数据的聚类效果不好</p>
<p>CURE算法流程：随机采样和划分的组合，对每个部分都进行层次聚类，直到达到期望的簇的数目/满足某个合并的标准</p>
<p>处理大型数据、离群点和具有非球形大小和非均匀大小的簇的数据</p>
<p>基于划分的聚类算法，需要指定划分簇的数量</p>
<p>选代表点：第一个选离质心最远的，第二个离第一个代表点最远的…</p>
<p>•代表点向中心收缩到原来的α倍：降低离群点的影响</p>
<p>用两个簇之间任意一对代表点的最小距离衡量两个簇的距离（单链接），极端的情况，全收缩到质心，就变成用质心的距离衡量簇之间的距离</p>
<p>优点：采样后速度快</p>
<p>朴素贝叶斯分类</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yanqiang/p/11625619.html#:~:text=%E5%A4%9A%E9%A1%B9%E5%BC%8F%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95%20M%20u%20l%20t%20i%20n%20o,%28%E5%85%B6%E4%B8%AD%E6%95%B0%E6%8D%AE%E9%80%9A%E5%B8%B8%E8%A1%A8%E7%A4%BA%E4%B8%BA%E8%AF%8D%E5%90%91%E9%87%8F%E8%AE%A1%E6%95%B0%EF%BC%8C%E8%99%BD%E7%84%B6%20t%20f%20%E2%88%92%20i%20d%20f%20%E5%90%91%E9%87%8F%E5%9C%A8%E5%AE%9E%E8%B7%B5%E4%B8%AD%E4%B9%9F%E5%BE%88%E6%9C%89%E6%95%88%29%E3%80%82">https://www.cnblogs.com/yanqiang/p/11625619.html#:~:text=多项式贝叶斯算法 M u l t i n o,(其中数据通常表示为词向量计数，虽然 t f − i d f 向量在实践中也很有效)。</a></p>
<p>感知机：预测误差 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i-y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 也可以写成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i\cdot y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.59445em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>SVM：优化函数基于最大间隔分割原则</p>
<p>爬山法（Hill Climbing，HC）是一种局部择优的贪心搜索算法，其本质上是梯度下降法。</p>
<p>SVM基础思想：间隔最大化  间隔：距离超平面最近的样本点到超平面的距离</p>
<p>优化问题（原问题）</p>
<p>原问题优化目标是最大化硬间隔，等价地写成这个形式。它是凸二次规划问题，当数据量很大时可以优先求解原问题，否则可以求解它的对偶问题</p>
<p>可以使用拉格朗日乘子法得到SVM的对偶问题，用拉格朗日函数对优化变量w，b求导，再带回拉格朗日函数中得到对偶问题，并且最优解需要满足kkt条件。互补松弛条件表示如果拉格朗日乘子 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span> 不等于0，则有 …=1，这个样本点在间隔的边界上，称为支持向量</p>
<p>书中说可以对拉格朗日乘子α梯度下降来逼近最优解，一般的话还可以用smo算法来求解</p>
<p>当数据不完全可分时，允许支持向量机在一些样本上出错，引入松弛变量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi></mrow><annotation encoding="application/x-tex">\xi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.04601em;">ξ</span></span></span></span> 来放松约束，同样使用拉格朗日乘子法，得到对偶问题，比硬间隔只多了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>&lt;</mo><mo>=</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\alpha &lt;=C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span></span></span></span> 的约束</p>
<p>内点法</p>
<p>另一种是非线性svm，它要解决的问题是数据在低维空间中不可分的情况，我们可以先将特征映射到高维，又因为在对偶问题和最终解中特征仅以内积形式出现，所以我们不需要显式地知道 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi (x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>，只需要这两者的点积，也就是内核相似度，k(xi,xj)。因此实际中只需要选择核函数即可，前提是这个核函数满足mercer定理，也就是它的核矩阵S是半正定的，这个函数就可以作为核函数使用。于是可以把问题改写成</p>
<p>常见的核函数有高斯径向基核，多项式核，sigmoid核</p>
<p>另一个线性分类模型是感知机。它包含两层节点，输入节点的数量等于数据的维度，对于二分类问题，这里假设输出标签是-1或1，权重是需要学的。它最终的决策函数和SVM一样，但是优化的目标不同，SVM是要最大化间隔，感知机优化的是近似的最小二乘误差。</p>
<p>每次选取一个训练样本，用梯度下降法更新权重（y-z）x是平方误差对w的近似负梯度。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span></span></span></span> eta 是学习率，</p>
<p>多层神经网络在此基础上加入多个隐藏层，一个层的节点作为下一层节点的输入</p>
<p>权重的更新包括前向传播和反向传播。前向传播过程计算出的一些中间值可以缓存起来，反向传播就是从最后一层往前逐层更新权重。</p>
<p>可以学习任意形状的决策边界，还可以学习数据在不同区域的不同决策边界，比如隐藏层的几个节点可以分别学习不同区域不同形状的决策边界</p>
<p>前面讲的都是先构造一个分类模型，然后再对一个特定的测试实例分类。在基于实例的学习中，训练被推迟到分类的最后一步，这样的分类器也叫做懒惰的学习器。它基于的一个原则是类似的实例有类似的类标签，一个最简单的例子是KNN</p>
<p>这一节提出了两点改进KNN距离度量的方式，也就是改变距离度量函数中的A，把特征和类别的分布考虑进来。一种是无监督的马氏距离，把矩阵A换成协方差矩阵的逆矩阵，对不同特征进行缩放，来解决不同特征尺度不同的问题，但是它没有考虑数据类别的分布问题。</p>
<p>比如说这样一个例子，x点是一个测试实例，它位于类别A的一侧，应该被分到A类，但是它的周围类别B的数据点比类别A的数据点更多，所以容易分错。</p>
<p>最近邻线性判别分析这种方法，考虑了数据类别的分布。它的主要思想是用矩阵A在判别能力强的方向上压缩近邻点，在判别能力弱的方向上拉伸近邻点，最终达到的效果是在距离函数中给判别能力强的方向设置比较大的权重</p>
<p>最后简单讲了一下分类器的评估问题，给定一个分类模型，要量化它在特定数据集上的准确性。它分成了两个步骤，方法论问题和量化问题。方法论问题针对的是如何划分训练、测试集。这三种是比较常见的划分方法，实际中我们应该分训练集、验证集、测试集，验证集用于选择参数。这几种划分方式既可以用于划分训练集与验证集，也可以用来划分训练集加验证集与测试集</p>
<p>选择了评估的方法后，接下来要度量准确率，分类器的输出有两种，一种是类标签的输出。除了最简单的准确率之外，当类别不均衡的情况下还可以考虑代价敏感准确率。比如对肿瘤的分类，恶行比良性少很多，并且更不希望将恶行的类别误判。n是每个类别的个数，a是每个类别上的准确率。我们可以给更重要的类别更大的误判代价c</p>
<p>另一种是数值型的输出。比如输出的是属于某一个类的概率值。对于二元标签，我们可以设置阈值来区分正例和负例。通过调整阈值，我们可以得到精度-召回率曲线，或者ROC曲线。精度是预测为正类且本身为正类的样本占比，召回率是被预测出来为正的样本占全部正样本的比例。ROC曲线纵轴是真正率，和召回率一样，横轴是假正率，是错误预测为正的负例占所有负例的比例</p>
<p>对于100个样本，四种算法得到的最有可能是正例的排名，完美的预测，1-5号样本</p>
<p>可以看到，ROC曲线是单调的，而P-R曲线不是单调的，相比之下ROC曲线更直观。我们可以把ROC曲线下的面积作为准确率的度量</p>
<p>凸二次规划问题，优化变量是d个，m个约束；当数据很多时，可以直接解原问题；当维度比较高时可以转化为对偶形式再求解</p>
<p>有放回的均匀采样</p>
<p>holdout留出法，每次采一定比例的样本作为训练集，剩下的作为测试集。重复采样多次。它的一个问题是类别分布不均衡，测试集中可能采不到包含稀有的类的数据，解决办法是按照比例独立地对不同类别采样</p>
<p>对偶：优化变量m个，</p>
<p>最终解只与支持向量有关</p>
<p>维度高时/核方法可以解对偶问题，SMO方法</p>
<p>求解更高效，因为只用求解比例系数a，而比例系数a只有支持向量才为非0，其他全为0.</p>
<p>拉格朗日对偶问题的求解：SMO、对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span> 梯度上升法</p>
<p>线性可分支持向量机：当训练数据线性可分，通过硬间隔最大化，学习一个线性的分类器</p>
<p>线性支持向量机：当训练数据近似线性可分，通过软间隔最大化，学习一个线性的分类器</p>
<p>非线性支持向量机：当训练数据线性不可分，通过使用核技巧及软间隔最大化，学习非线性分类器</p>
<p>核函数满足mercer定理</p>
<p>高斯核 通过泰勒展开，可以表示成所有次数的多项式核的线性组合</p>
<p>核 k-means 映射到另外一个特征空间中再进行聚类</p>
<p>KPCA：高维空间线性映射到低维空间 变成非线性映射</p>
<p>感知机：单层神经网络，输入节点通过一个权重和输出节点相连接，这里假设输出是-1或1的类别标签</p>
<p>它学到的函数是这样一个激活函数，学习权重W和偏置b</p>
<p>第t次迭代时权重向量W的更新公式为</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">η</span></span></span></span> 是学习率</p>
<p>近似优化预测值的最小二乘误差</p>
<p>罗列</p>
<p>相信大部分人都听过核函数可以将低维数据映射到高维中，其实准确说核函数只是给出了数据在低维下计算高维内积的方法。</p>
<p>内点法：求带约束的线性规划问题（LP）/ 带约束的二次规划问题（QP）</p>
<p>障碍函数法：Barrier Method的思想就是通过在原始的目标函数中添加一个障碍函数（也可以理解成惩罚函数）来代替约束条件中的不等式约束。</p>
<img src="image-20221123183339798.png" alt="image-20221123183339798" style="zoom:50%;">
<img src="image-20221122164546526.png" alt="image-20221122164546526" style="zoom:30%;">
<p>高斯径向基函数 σ越小，对数据划分越细致，也越容易倒置过拟合</p>
<p>slater条件确保了鞍点的存在</p>
<p>KKT条件便是确保鞍点便是原函数最优解的充分条件</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://seline02.github.io">Seline</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://seline02.github.io/2022/09/24/Data-Mining-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">https://seline02.github.io/2022/09/24/Data-Mining-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://seline02.github.io" target="_blank">Seline's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/09/26/Back-Propagation/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Back Propagation</div></div></a></div><div class="next-post pull-right"><a href="/2022/09/24/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA%E9%83%A8%E5%88%86%E7%AC%94%E8%AE%B0/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">控制理论笔记</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Seline</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">第二章 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">2.1 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E5%8F%AF%E7%A7%BB%E6%A4%8D%E6%80%A7"><span class="toc-number">1.2.</span> <span class="toc-text">2.2 特征提取和可移植性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.2.1 特征提取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%AF%E7%A7%BB%E6%A4%8D%E6%80%A7"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2.2 数据类型可移植性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-1-%E6%95%B0%E5%AD%97%E5%88%B0%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%EF%BC%9A%E7%A6%BB%E6%95%A3%E5%8C%96"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">2.2.2.1 数字到分类数据：离散化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-4-%E4%BB%8E%E6%97%B6%E5%BA%8F%E5%88%B0%E7%A6%BB%E6%95%A3%E5%BA%8F%E5%88%97"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">2.2.2.4 从时序到离散序列</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-3-1-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">2.4.3.1 主成分分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-3-2-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">2.4.3.2 奇异值分解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-3-3-%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90"><span class="toc-number">1.2.2.5.</span> <span class="toc-text">2.4.3.3 潜在语义分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-4-%E7%94%A8%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E9%99%8D%E7%BB%B4"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.4.4 用类型转换降维</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-4-1-%E5%93%88%E5%B0%94-haar-%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">2.4.4.1 哈尔(Haar)小波变换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-4-3-%E8%B0%B1%E8%BD%AC%E6%8D%A2%E5%92%8C%E5%9B%BE%E5%B5%8C%E5%85%A5"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">2.4.4.3 谱转换和图嵌入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-2-%E9%AB%98%E7%BB%B4%E6%95%B0%E6%8D%AE%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.2.3.3.</span> <span class="toc-text">3.2.1.2 高维数据的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-3-%E5%B1%80%E9%83%A8%E4%B8%8D%E7%9B%B8%E5%85%B3%E7%89%B9%E5%BE%81%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.2.3.4.</span> <span class="toc-text">3.2.1.3 局部不相关特征的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-4-%E4%B8%8D%E5%90%8C-l-p-%E8%8C%83%E6%95%B0%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.2.3.5.</span> <span class="toc-text">3.2.1.4 不同 Lp−L_p-Lp​−范数的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-5-%E5%9F%BA%E4%BA%8E%E5%8C%B9%E9%85%8D%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">1.2.3.6.</span> <span class="toc-text">3.2.1.5 基于匹配的相似度计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-6-%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.2.3.7.</span> <span class="toc-text">3.2.1.6 数据分布的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-7-%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E5%B8%83%EF%BC%9Aisomap"><span class="toc-number">1.2.3.8.</span> <span class="toc-text">3.2.1.7 非线性分布：ISOMAP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-8-%E5%B1%80%E9%83%A8%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.2.3.9.</span> <span class="toc-text">3.2.1.8 局部数据分布的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-9-%E8%AE%A1%E7%AE%97%E8%80%83%E8%99%91"><span class="toc-number">1.2.3.10.</span> <span class="toc-text">3.2.1.9 计算考虑</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.4.</span> <span class="toc-text">3.2.2 分类数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E6%B7%B7%E5%90%88%E5%AE%9A%E9%87%8F%E5%92%8C%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.5.</span> <span class="toc-text">3.2.3 混合定量和分类数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F"><span class="toc-number">1.3.</span> <span class="toc-text">3.3 文本相似性度量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%92%8C%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.3.1 二进制和集合数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E6%97%B6%E9%97%B4%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F"><span class="toc-number">1.4.</span> <span class="toc-text">3.4 时间相似性度量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-1-2-l-p-%E8%8C%83%E6%95%B0"><span class="toc-number">1.4.0.1.</span> <span class="toc-text">3.4.1.2 LpL_pLp​-范数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-1-4-%E5%9F%BA%E4%BA%8E%E7%AA%97%E5%8F%A3%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.0.2.</span> <span class="toc-text">3.4.1.4  基于窗口的方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-%E7%A6%BB%E6%95%A3%E5%BA%8F%E5%88%97%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.4.2 离散序列相似性度量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-2-2-%E6%9C%80%E9%95%BF%E7%9A%84%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">3.4.2.2 最长的公共子序列</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E5%9B%BE%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F"><span class="toc-number">1.5.</span> <span class="toc-text">3.5 图的相似性度量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-1-%E5%8D%95%E4%B8%AA%E5%9B%BE%E4%B8%AD%E4%B8%A4%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7"><span class="toc-number">1.5.1.</span> <span class="toc-text">3.5.1 单个图中两个节点之间的相似性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-1-1-%E5%9F%BA%E4%BA%8E%E7%BB%93%E6%9E%84%E8%B7%9D%E7%A6%BB%E7%9A%84%E6%B5%8B%E9%87%8F"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">3.5.1.1 基于结构距离的测量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-1-2-%E9%9A%8F%E6%9C%BA%E8%A1%8C%E8%B5%B0%E7%9B%B8%E4%BC%BC%E5%BA%A6"><span class="toc-number">1.5.1.2.</span> <span class="toc-text">3.5.1.2 随机行走相似度</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-2-%E4%B8%A4%E4%B8%AA%E5%9B%BE%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7"><span class="toc-number">1.5.2.</span> <span class="toc-text">3.5.2 两个图之间的相似性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7-%E6%80%BB%E7%BB%93"><span class="toc-number">1.6.</span> <span class="toc-text">3.7 总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/05/08/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" title="线性代数"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="线性代数"/></a><div class="content"><a class="title" href="/2023/05/08/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" title="线性代数">线性代数</a><time datetime="2023-05-08T07:05:38.000Z" title="发表于 2023-05-08 15:05:38">2023-05-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/23/AIMA%E7%AC%94%E8%AE%B0/" title="AIMA笔记"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AIMA笔记"/></a><div class="content"><a class="title" href="/2023/03/23/AIMA%E7%AC%94%E8%AE%B0/" title="AIMA笔记">AIMA笔记</a><time datetime="2023-03-23T09:51:42.000Z" title="发表于 2023-03-23 17:51:42">2023-03-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0/" title="神经网络笔记"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="神经网络笔记"/></a><div class="content"><a class="title" href="/2023/03/21/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0/" title="神经网络笔记">神经网络笔记</a><time datetime="2023-03-21T04:32:51.000Z" title="发表于 2023-03-21 12:32:51">2023-03-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/20/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/" title="自然语言处理笔记"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自然语言处理笔记"/></a><div class="content"><a class="title" href="/2023/03/20/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/" title="自然语言处理笔记">自然语言处理笔记</a><time datetime="2023-03-20T08:31:21.000Z" title="发表于 2023-03-20 16:31:21">2023-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%AC%94%E8%AE%B0/" title="多智能体笔记"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="多智能体笔记"/></a><div class="content"><a class="title" href="/2023/03/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%AC%94%E8%AE%B0/" title="多智能体笔记">多智能体笔记</a><time datetime="2023-03-15T08:05:41.000Z" title="发表于 2023-03-15 16:05:41">2023-03-15</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Seline</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>