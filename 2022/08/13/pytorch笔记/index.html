<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>pytorch笔记 | Seline's blog</title><meta name="keywords" content="pytorch"><meta name="author" content="Seline"><meta name="copyright" content="Seline"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="同时结合了datawhale和龙曲良的pytorch教程 PyTorch基础知识张量创建Tensor  torch.tensor小写的接受实际的数据 大写的接受数据的维度（2，3），也可现有数据（[2,3]）（少用）   未初始化 数据很不规则，有可能非常大或非常小，一定要覆盖掉    torch.normal传10个均值 10个方差 最后还要reshape torch.full([10], 0)">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch笔记">
<meta property="og:url" content="https://seline02.github.io/2022/08/13/pytorch%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Seline&#39;s blog">
<meta property="og:description" content="同时结合了datawhale和龙曲良的pytorch教程 PyTorch基础知识张量创建Tensor  torch.tensor小写的接受实际的数据 大写的接受数据的维度（2，3），也可现有数据（[2,3]）（少用）   未初始化 数据很不规则，有可能非常大或非常小，一定要覆盖掉    torch.normal传10个均值 10个方差 最后还要reshape torch.full([10], 0)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2022-08-13T06:17:35.000Z">
<meta property="article:modified_time" content="2022-08-14T04:36:15.873Z">
<meta property="article:author" content="Seline">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://seline02.github.io/2022/08/13/pytorch%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-08-14 12:36:15'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Seline's blog</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-13T06:17:35.000Z" title="发表于 2022-08-13 14:17:35">2022-08-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-14T04:36:15.873Z" title="更新于 2022-08-14 12:36:15">2022-08-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="pytorch笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><em>同时结合了datawhale和龙曲良的pytorch教程</em></p>
<h2 id="PyTorch基础知识"><a href="#PyTorch基础知识" class="headerlink" title="PyTorch基础知识"></a>PyTorch基础知识</h2><h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><h4 id="创建Tensor"><a href="#创建Tensor" class="headerlink" title="创建Tensor"></a>创建Tensor</h4><p><img src="image-20220813143630114.png" alt="image-20220813143630114" style="zoom:50%;"></p>
<p><img src="image-20220813142012231.png" alt="image-20220813142012231" style="zoom:50%;"></p>
<p>torch.tensor小写的接受实际的数据 大写的接受数据的维度（2，3），也可现有数据（[2,3]）（少用）</p>
<p><img src="image-20220813142027481.png" alt="image-20220813142027481" style="zoom:50%;"></p>
<p><img src="image-20220813142039795.png" alt="image-20220813142039795" style="zoom:50%;"></p>
<p>未初始化 数据很不规则，有可能非常大或非常小，一定要覆盖掉</p>
<p><img src="image-20220813144109711.png" alt="image-20220813144109711" style="zoom:50%;"></p>
<p><img src="image-20220813144115496.png" alt="image-20220813144115496" style="zoom:50%;"></p>
<p><img src="image-20220813144120705.png" alt="image-20220813144120705" style="zoom:50%;"></p>
<p>torch.normal传10个均值 10个方差 最后还要reshape</p>
<p>torch.full([10], 0)生成长为10，全为0的tensor</p>
<p><img src="image-20220813144128423.png" alt="image-20220813144128423" style="zoom:50%;"></p>
<p>scaler shape传一个[]</p>
<p><img src="image-20220813144145874.png" alt="image-20220813144145874" style="zoom:50%;"></p>
<p>arange(0,10)生成[0,10)不包含末尾的等差数列，默认以1来递增</p>
<p>range在pytorch中不建议使用，用arange代替</p>
<p><img src="image-20220813145439485.png" alt="image-20220813145439485" style="zoom:50%;"></p>
<p>和arange不同，linspace/logspace中的step表示<strong>生成几个数字</strong>而不是步长</p>
<p><img src="image-20220813145714726.png" alt="image-20220813145714726" style="zoom:50%;"></p>
<p><img src="image-20220813145720609.png" alt="image-20220813145720609" style="zoom:50%;"></p>
<h4 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h4><p><strong>需要注意的是：索引出来的结果与原数据共享内存，修改一个，另一个会跟着修改。如果不想修改，可以考虑使用copy()等方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.rand(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 取第二列</span></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>]) </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">0.0720</span>,  <span class="number">0.0666</span>,  <span class="number">1.0336</span>, -<span class="number">0.6965</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = x[<span class="number">0</span>,:]</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :]) <span class="comment"># 源tensor也被改了了</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">3.7311</span>, <span class="number">0.9280</span>, <span class="number">1.2497</span>])</span><br><span class="line">tensor([<span class="number">3.7311</span>, <span class="number">0.9280</span>, <span class="number">1.2497</span>])</span><br></pre></td></tr></table></figure>
<p><img src="image-20220813150644560.png" alt="image-20220813150644560" style="zoom:50%;"></p>
<p>a[0,0,2,4]返回的是一个标量</p>
<p><img src="image-20220813151109470.png" alt="image-20220813151109470" style="zoom:50%;"></p>
<p><img src="image-20220813151210354.png" alt="image-20220813151210354" style="zoom:50%;"></p>
<p><img src="image-20220813151303893.png" alt="image-20220813151303893" style="zoom:50%;"></p>
<p>index_select的第一个参数是要采集的维度</p>
<p><img src="image-20220813151630940.png" alt="image-20220813151630940" style="zoom:50%;"></p>
<p><img src="image-20220813151746140.png" alt="image-20220813151746140" style="zoom:50%;"></p>
<p>…表示任意多的维度</p>
<p>a[0,…] = a[0]</p>
<p><img src="image-20220813152539157.png" alt="image-20220813152539157" style="zoom:50%;"></p>
<p>掩码用的不多，因为它会默认将数据打平</p>
<p>x.ge(0.5) ：ge指great equal，大于等于）0.5的位置置1</p>
<p><img src="image-20220813152852148.png" alt="image-20220813152852148" style="zoom:50%;"></p>
<p>take是先把所有的打平，再取打平后对应的元素</p>
<h4 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h4><p><img src="image-20220813153107459.png" alt="image-20220813153107459" style="zoom:50%;"></p>
<p><img src="image-20220813153242361.png" alt="image-20220813153242361" style="zoom:50%;"></p>
<p>张量的维度变换常见的方法有<code>torch.view()</code>和<code>torch.reshape()</code>，下面我们将介绍第一中方法<code>torch.view()</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">8</span>) <span class="comment"># -1是指这一维的维数由其他维度决定</span></span><br><span class="line"><span class="built_in">print</span>(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>]) torch.Size([<span class="number">16</span>]) torch.Size([<span class="number">2</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<p>注: <code>torch.view()</code> 返回的新<code>tensor</code>与源<code>tensor</code><strong>共享内存</strong>(其实是同一个<code>tensor</code>)，更改其中的一个，另外一个也会跟着改变。(顾名思义，view()仅仅是改变了对这个张量的观察角度)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y) <span class="comment"># 也加了了1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">1.3019</span>,  <span class="number">0.3762</span>,  <span class="number">1.2397</span>,  <span class="number">1.3998</span>],</span><br><span class="line">        [ <span class="number">0.6891</span>,  <span class="number">1.3651</span>,  <span class="number">1.1891</span>, -<span class="number">0.6744</span>],</span><br><span class="line">        [ <span class="number">0.3490</span>,  <span class="number">1.8377</span>,  <span class="number">1.6456</span>,  <span class="number">0.8403</span>],</span><br><span class="line">        [-<span class="number">0.8259</span>,  <span class="number">2.5454</span>,  <span class="number">1.2474</span>,  <span class="number">0.7884</span>]])</span><br><span class="line">tensor([ <span class="number">1.3019</span>,  <span class="number">0.3762</span>,  <span class="number">1.2397</span>,  <span class="number">1.3998</span>,  <span class="number">0.6891</span>,  <span class="number">1.3651</span>,  <span class="number">1.1891</span>, -<span class="number">0.6744</span>,</span><br><span class="line">         <span class="number">0.3490</span>,  <span class="number">1.8377</span>,  <span class="number">1.6456</span>,  <span class="number">0.8403</span>, -<span class="number">0.8259</span>,  <span class="number">2.5454</span>,  <span class="number">1.2474</span>,  <span class="number">0.7884</span>])</span><br></pre></td></tr></table></figure>
<p>上面我们说过torch.view()会改变原始张量，但是很多情况下，我们希望原始张量和变换后的张量互相不影响。为为了使创建的张量和原始张量不共享内存，我们需要使用第二种方法<code>torch.reshape()</code>， 同样可以改变张量的形状，但是此函数并不能保证返回的是其拷贝值，所以官方<strong>不推荐使用</strong>。推荐的方法是我们先用 <code>clone()</code> 创造一个<strong>张量副本</strong>然后再使用 <code>torch.view()</code>进行函数维度变换 。</p>
<p>注：使用 <code>clone()</code> 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor 。</p>
<p><img src="image-20220813153940238.png" alt="image-20220813153940238" style="zoom:50%;"></p>
<p><img src="image-20220813154507078.png" alt="image-20220813154507078" style="zoom:50%;"></p>
<p>unsqueeze第一个参数：在哪一维插入新的一维</p>
<p>原来一共4维，第一个参数取值范围是[-5,5)。参数是0~4表示在第k维之前插入一个新的维度；参数是-5 ~ -1表示在第k维之后插入</p>
<p><img src="image-20220813154703233.png" alt="image-20220813154703233" style="zoom:50%;"></p>
<p>原先的shape是[2]</p>
<p><img src="image-20220813154923635.png" alt="image-20220813154923635" style="zoom:50%;"></p>
<p>在b的后面插入2维，在前面插入1维</p>
<p><img src="image-20220813155456767.png" alt="image-20220813155456767" style="zoom:50%;"></p>
<p><img src="image-20220813155853943.png" alt="image-20220813155853943" style="zoom:50%;"></p>
<p>expand实际上并没有增加数据，推荐这种操作，节约内存</p>
<p>repeat实实在在地增加了数据，1变到4时会把数据都拷贝一遍</p>
<p><img src="image-20220813160144046.png" alt="image-20220813160144046" style="zoom:50%;"></p>
<p>仅仅原来维度上是1的能够复制</p>
<p>-1表示保持原来的维度不变</p>
<p><img src="image-20220813160420473.png" alt="image-20220813160420473" style="zoom:50%;"></p>
<p>repeat的参数和expand的参数不一样，每个数表示对应维度要<strong>复制的次数</strong></p>
<p><img src="image-20220813160525001.png" alt="image-20220813160525001" style="zoom:50%;"></p>
<p>.t方法只适用于二维的矩阵</p>
<p><img src="image-20220813160734115.png" alt="image-20220813160734115" style="zoom:50%;"></p>
<p><img src="image-20220813161212603.png" alt="image-20220813161212603" style="zoom:50%;"></p>
<p>transpose接受要交换的两个维度</p>
<p>本来是一行行存储，维度交换后存储顺序会改变</p>
<p><code>contiguous()</code>函数使其变成连续存储</p>
<p>用all函数返回每一项是否都一样，返回0，说明a和a1有不一样的</p>
<p>view会导致维度顺序关系变模糊，所以需要人为跟踪</p>
<p><img src="image-20220813162009353.png" alt="image-20220813162009353" style="zoom:50%;"></p>
<p>用permute如果遇到contiguous的错误，仍需要用contiguous函数</p>
<h4 id="拼接与拆分"><a href="#拼接与拆分" class="headerlink" title="拼接与拆分"></a>拼接与拆分</h4><p><img src="image-20220813162158992.png" alt="image-20220813162158992" style="zoom:50%;"></p>
<p>split按长度进行拆分，chunk按数量进行拆分</p>
<p><img src="image-20220813162447003.png" alt="image-20220813162447003" style="zoom:50%;"></p>
<p><img src="image-20220813162600869.png" alt="image-20220813162600869" style="zoom:50%;"></p>
<p><img src="image-20220813162818291.png" alt="image-20220813162818291" style="zoom:50%;"></p>
<p>a1: (4,3,16,32)  a2: (4,3,16,32)  实际可以理解为a1保存了图片的上半部分，a2保存了图片的下半部分，可以上下拼起来 </p>
<p><img src="image-20220813163255922.png" alt="image-20220813163255922" style="zoom:50%;"></p>
<p>stack会创建一个新的维度</p>
<p><img src="image-20220813163342138.png" alt="image-20220813163342138" style="zoom:50%;"></p>
<p><img src="image-20220813164221049.png" alt="image-20220813164221049" style="zoom:50%;"></p>
<p>split可以接受长度的参数 <code>c.split(1, dim=0)</code> 表示拆分成每个单元的长度是1</p>
<p>长度不一样的话，也可以接受一个list，<code>c.split([1, 1], dim=0)</code> 表示拆成两个单元，长度分别为1和1</p>
<p><img src="image-20220813164358265.png" alt="image-20220813164358265" style="zoom:50%;"></p>
<p>chunk按数量拆分</p>
<h4 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h4><p><img src="image-20220813164436013.png" alt="image-20220813164436013" style="zoom:50%;"></p>
<p><img src="image-20220813164554822.png" alt="image-20220813164554822" style="zoom:50%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">y = torch.rand(<span class="number">4</span>, <span class="number">3</span>) </span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line"><span class="built_in">print</span>(torch.add(x, y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式3 in-place，原值修改</span></span><br><span class="line">y.add_(x) </span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">2.8977</span>,  <span class="number">0.6581</span>,  <span class="number">0.5856</span>],</span><br><span class="line">        [-<span class="number">1.3604</span>,  <span class="number">0.1656</span>, -<span class="number">0.0823</span>],</span><br><span class="line">        [ <span class="number">2.1387</span>,  <span class="number">1.7959</span>,  <span class="number">1.5275</span>],</span><br><span class="line">        [ <span class="number">2.2427</span>, -<span class="number">0.3100</span>, -<span class="number">0.4826</span>]])</span><br><span class="line">tensor([[ <span class="number">2.8977</span>,  <span class="number">0.6581</span>,  <span class="number">0.5856</span>],</span><br><span class="line">        [-<span class="number">1.3604</span>,  <span class="number">0.1656</span>, -<span class="number">0.0823</span>],</span><br><span class="line">        [ <span class="number">2.1387</span>,  <span class="number">1.7959</span>,  <span class="number">1.5275</span>],</span><br><span class="line">        [ <span class="number">2.2427</span>, -<span class="number">0.3100</span>, -<span class="number">0.4826</span>]])</span><br><span class="line">tensor([[ <span class="number">2.8977</span>,  <span class="number">0.6581</span>,  <span class="number">0.5856</span>],</span><br><span class="line">        [-<span class="number">1.3604</span>,  <span class="number">0.1656</span>, -<span class="number">0.0823</span>],</span><br><span class="line">        [ <span class="number">2.1387</span>,  <span class="number">1.7959</span>,  <span class="number">1.5275</span>],</span><br><span class="line">        [ <span class="number">2.2427</span>, -<span class="number">0.3100</span>, -<span class="number">0.4826</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="image-20220813164923013.png" alt="image-20220813164923013" style="zoom:50%;"></p>
<p>* 是element-wise相乘，即对应位置元素相乘</p>
<p>@是matmul的重载</p>
<p><img src="image-20220813172303564.png" alt="image-20220813172303564" style="zoom:50%;"></p>
<p>pytorch中w的第一个维度是channel_out即输出的维度，第二个维度是channel_in即输入的维度</p>
<p>对于二维以上的tensor相乘：</p>
<p><img src="image-20220813172515341.png" alt="image-20220813172515341" style="zoom:50%;"></p>
<p><code>mm</code>只限于2d的矩阵相乘</p>
<p><code>matmul</code>取最后两维进行运算</p>
<p>其实就是支持多个矩阵并行相乘</p>
<p>(4,3,28,64)  (4,1,64,32) 相乘之前会先进行broadcast</p>
<p><img src="image-20220813172940147.png" alt="image-20220813172940147" style="zoom:50%;"></p>
<p><img src="image-20220813173028086.png" alt="image-20220813173028086" style="zoom:50%;"></p>
<p><img src="image-20220813173443526.png" alt="image-20220813173443526" style="zoom:50%;"></p>
<p><code>a.trunc()</code> 裁成了整数部分，<code>a.frac()</code>裁成了小数部分</p>
<p><img src="image-20220813174207685.png" alt="image-20220813174207685" style="zoom:50%;"></p>
<p>clamp用于梯度的裁剪</p>
<p>当在网络中出现training不稳定的情况，打印一下梯度的模 <code>w.grad.norm(2)</code>。小于10比较合适</p>
<p><code>grad.clamp(10)</code>小于10的都要变成10</p>
<p>对w限幅是weight clipping；对w.grad限幅是gradient clipping</p>
<p>传一个参数是限制了最小值；传两个参数是限制了最小和最大值</p>
<h4 id="统计属性"><a href="#统计属性" class="headerlink" title="统计属性"></a>统计属性</h4><p><img src="image-20220813174308746.png" alt="image-20220813174308746" style="zoom:30%;"></p>
<p><img src="image-20220813174429402.png" alt="image-20220813174429402" style="zoom:30%;"></p>
<p><img src="image-20220813174441562.png" alt="image-20220813174441562" style="zoom:50%;"></p>
<p><img src="image-20220813180103505.png" alt="image-20220813180103505" style="zoom:50%;"></p>
<p><img src="image-20220813180457598.png" alt="image-20220813180457598" style="zoom:50%;"></p>
<p><code>a.prod()</code>是product，累乘</p>
<p><img src="image-20220813180819499.png" alt="image-20220813180819499" style="zoom:50%;"></p>
<p><code>a.argmax()</code>如果不给定参数，会把数据打平，再给出位置</p>
<p><img src="image-20220813181901965.png" alt="image-20220813181901965" style="zoom:50%;"></p>
<p>如果从[4,10]经过max或argmax得到[4]，但仍想得到[4,1]这样的二维矩阵，即和原来的dimention相同，加 <code>keepdim=True</code></p>
<p><img src="image-20220813182613125.png" alt="image-20220813182613125" style="zoom:50%;"></p>
<p>topk中设置<code>largest=False</code>就是选最小的那几个</p>
<p>kthvalue是选第k小</p>
<p><img src="image-20220813183246485.png" alt="image-20220813183246485" style="zoom:50%;"></p>
<p>返回的是byteTensor，dtype是unit8</p>
<p>比较是element-wise</p>
<p>如果要返回的是true/false，用<code>torch.equal(a,b)</code></p>
<h4 id="高阶操作"><a href="#高阶操作" class="headerlink" title="高阶操作"></a>高阶操作</h4><p><img src="image-20220813183314425.png" alt="image-20220813183314425" style="zoom:50%;"></p>
<p><img src="image-20220813183649239.png" alt="image-20220813183649239" style="zoom:50%;"></p>
<p>创建一个tensor，它的源头是x或y</p>
<p><img src="image-20220813183751198.png" alt="image-20220813183751198" style="zoom:50%;"></p>
<p>tensor的值大于0.5，对应位置取a对应的值，否则取b对应的</p>
<p><img src="image-20220813194647607.png" alt="image-20220813194647607" style="zoom:50%;"></p>
<p>gather是一个查表的操作，input是一张表 [dog; cat; whale]，index是索引 [1, 0, 1, 2]</p>
<p>索引1查到cat，0查到dog，1查到cat，2查到whale</p>
<p><img src="image-20220813194952233.png" alt="image-20220813194952233" style="zoom:50%;"></p>
<p>可以把0编号编到100，1编到101… 那么[100; 101; …109]就是一张表</p>
<p><img src="image-20220813195249743.png" alt="image-20220813195249743" style="zoom:50%;"></p>
<p>可以做成映射</p>
<h3 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h3><h4 id="激活函数与Loss的梯度"><a href="#激活函数与Loss的梯度" class="headerlink" title="激活函数与Loss的梯度"></a>激活函数与Loss的梯度</h4><p><img src="image-20220813202857889.png" alt="image-20220813202857889" style="zoom:50%;"></p>
<p><img src="image-20220813203153126.png" alt="image-20220813203153126" style="zoom:50%;"></p>
<p>Tanh在rnn中用的比较多</p>
<p><img src="image-20220813203237105.png" alt="image-20220813203237105" style="zoom:50%;"></p>
<p><img src="image-20220813203249641.png" alt="image-20220813203249641" style="zoom:50%;"></p>
<p><img src="image-20220813203334864.png" alt="image-20220813203334864" style="zoom:50%;"></p>
<p><img src="image-20220813203340945.png" alt="image-20220813203340945" style="zoom:50%;"></p>
<p><img src="image-20220813203412499.png" alt="image-20220813203412499" style="zoom:50%;"></p>
<p><img src="image-20220813203606468.png" alt="image-20220813203606468" style="zoom:50%;"></p>
<p><img src="image-20220813203839334.png" alt="image-20220813203839334" style="zoom:50%;"></p>
<p>要注意是否有开根号</p>
<p>mse是不开根号的，如果用L2-norm来写就是 <code>torch.norm(y-pred, 2).pow(2)</code></p>
<p><img src="image-20220813212920728.png" alt="image-20220813212920728" style="zoom:50%;"></p>
<p><code>w.requires_grad_()</code>后因为图没有更新（图是计算一步，更新一步的） 所以要重新求mse，才能再求梯度</p>
<p>也可以在初始化时就设置为要求梯度w=torch.tensor([1], requires_grad=True)</p>
<p><img src="image-20220813212931405.png" alt="image-20220813212931405" style="zoom:50%;"></p>
<p>可以用<code>torch.autograd.grad</code>求梯度，会返回一个list</p>
<p>也可以直接在loss节点上调用backward，但后一种不会直接返回梯度信息，即不会返回list之类的，会赋在每一个变量上，用w.grad看</p>
<p><img src="image-20220813212939861.png" alt="image-20220813212939861" style="zoom:50%;"></p>
<p>softmax:</p>
<p><img src="image-20220813212952044.png" alt="image-20220813212952044" style="zoom:50%;"></p>
<p><img src="image-20220813212956743.png" alt="image-20220813212956743" style="zoom:50%;"></p>
<p><img src="image-20220813213002092.png" alt="image-20220813213002092" style="zoom:50%;"></p>
<p><img src="image-20220813213008464.png" alt="image-20220813213008464" style="zoom:50%;"></p>
<p>dim=…[batch, feature] 希望在feature维度上做softmax操作，要设置 <code>dim=0</code></p>
<p>使用了一次<code>p.backward()</code>后，图的信息会被清除掉，再调用一次会报错</p>
<p>设置retain_graph=True后图不会被清除，即<code>p.backward(retain_graph=True)</code>，调用了backward()后还可以再调用backward()</p>
<p><code>torch.autograd.grad()</code>的第一个参数必须是一个标量，如p[1],p[2]</p>
<p>i 和 j 相等时，梯度是正的，其他是负的</p>
<h4 id="2D函数优化实例"><a href="#2D函数优化实例" class="headerlink" title="2D函数优化实例"></a>2D函数优化实例</h4><p><img src="image-20220814102542178.png" alt="image-20220814102542178" style="zoom:50%;"></p>
<p><img src="image-20220814122226447.png" alt="image-20220814122226447" style="zoom:50%;"></p>
<p><img src="image-20220814122508477.png" alt="image-20220814122508477" style="zoom:50%;"></p>
<p><img src="image-20220813213017927.png" alt="image-20220813213017927" style="zoom:50%;"></p>
<p>调用<code>optimizer = torch.optim.Adam([x], lr=1e-3)</code>和<code>optimizer.step()</code>会自动地更新 $x^{‘}=x-0.001\nabla x,y^{‘}=y-0.001\nabla y$</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://seline02.github.io">Seline</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://seline02.github.io/2022/08/13/pytorch%E7%AC%94%E8%AE%B0/">https://seline02.github.io/2022/08/13/pytorch%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://seline02.github.io" target="_blank">Seline's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/08/10/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">变分推断</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Seline</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">PyTorch基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.</span> <span class="toc-text">张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BATensor"><span class="toc-number">1.1.1.</span> <span class="toc-text">创建Tensor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87"><span class="toc-number">1.1.2.</span> <span class="toc-text">索引与切片</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2"><span class="toc-number">1.1.3.</span> <span class="toc-text">维度变换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5%E4%B8%8E%E6%8B%86%E5%88%86"><span class="toc-number">1.1.4.</span> <span class="toc-text">拼接与拆分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number">1.1.5.</span> <span class="toc-text">数学运算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%B1%9E%E6%80%A7"><span class="toc-number">1.1.6.</span> <span class="toc-text">统计属性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E9%98%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.7.</span> <span class="toc-text">高阶操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="toc-number">1.2.</span> <span class="toc-text">自动求导</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8ELoss%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.2.1.</span> <span class="toc-text">激活函数与Loss的梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2D%E5%87%BD%E6%95%B0%E4%BC%98%E5%8C%96%E5%AE%9E%E4%BE%8B"><span class="toc-number">1.2.2.</span> <span class="toc-text">2D函数优化实例</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/08/13/pytorch%E7%AC%94%E8%AE%B0/" title="pytorch笔记"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="pytorch笔记"/></a><div class="content"><a class="title" href="/2022/08/13/pytorch%E7%AC%94%E8%AE%B0/" title="pytorch笔记">pytorch笔记</a><time datetime="2022-08-13T06:17:35.000Z" title="发表于 2022-08-13 14:17:35">2022-08-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/10/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/" title="变分推断"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="变分推断"/></a><div class="content"><a class="title" href="/2022/08/10/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/" title="变分推断">变分推断</a><time datetime="2022-08-10T04:22:33.000Z" title="发表于 2022-08-10 12:22:33">2022-08-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="transformer论文笔记"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="transformer论文笔记"/></a><div class="content"><a class="title" href="/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="transformer论文笔记">transformer论文笔记</a><time datetime="2022-08-08T10:09:23.000Z" title="发表于 2022-08-08 18:09:23">2022-08-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/08/hello-world/" title="Hello World"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2022/08/08/hello-world/" title="Hello World">Hello World</a><time datetime="2022-08-08T09:54:45.020Z" title="发表于 2022-08-08 17:54:45">2022-08-08</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Seline</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>