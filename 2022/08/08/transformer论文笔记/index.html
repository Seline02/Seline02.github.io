<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="训练快 纯注意力机制 rnn时序，难以并行 用卷积神经网络对比较长的序列难以建模，卷积做计算的时候，每次看一个比较小的窗口，如果两个像素离得很远，需要很多层才能看到，但一个好的地方是，可以做多个输出通道，一个输出通道可以认为是去识别不同的模式 -&gt; Multi-Head Attention 解码的时候只能一个个地生成：auto-regressive自回归  Add连过去的作为残差连接 一个">
<meta property="og:type" content="article">
<meta property="og:title" content="transformer论文笔记">
<meta property="og:url" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="训练快 纯注意力机制 rnn时序，难以并行 用卷积神经网络对比较长的序列难以建模，卷积做计算的时候，每次看一个比较小的窗口，如果两个像素离得很远，需要很多层才能看到，但一个好的地方是，可以做多个输出通道，一个输出通道可以认为是去识别不同的模式 -&gt; Multi-Head Attention 解码的时候只能一个个地生成：auto-regressive自回归  Add连过去的作为残差连接 一个">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181024468.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181538290.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181642639.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181703718.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181713456.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181722146.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181730490.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181740780.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181745545.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181754453.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181804578.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181816220.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181824646.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181833238.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181843818.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181851387.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181856916.png">
<meta property="og:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181907992.png">
<meta property="article:published_time" content="2022-08-08T10:09:23.000Z">
<meta property="article:modified_time" content="2022-08-08T10:31:43.620Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220808181024468.png">

<link rel="canonical" href="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>transformer论文笔记 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/08/transformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          transformer论文笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-08 18:09:23 / 修改时间：18:31:43" itemprop="dateCreated datePublished" datetime="2022-08-08T18:09:23+08:00">2022-08-08</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="image-20220808181024468.png" alt="image-20220808181024468" style="zoom:50%;"></p>
<p>训练快</p>
<p>纯注意力机制</p>
<p>rnn时序，难以并行</p>
<p>用卷积神经网络对比较长的序列难以建模，卷积做计算的时候，每次看一个比较小的窗口，如果两个像素离得很远，需要很多层才能看到，但一个好的地方是，可以做多个输出通道，一个输出通道可以认为是去识别不同的模式 -&gt; Multi-Head Attention</p>
<p>解码的时候只能一个个地生成：auto-regressive自回归</p>
<p><img src="image-20220808181538290.png" alt="image-20220808181538290" style="zoom:50%;"></p>
<p>Add连过去的作为残差连接</p>
<p>一个layer（block）两个子层：multi-head self-attention mechanism + simple, position-wise fully connected feed-forward network</p>
<p><img src="image-20220808181642639.png" alt="image-20220808181642639" style="zoom:50%;"></p>
<p>残差连接，因为要保证维度一样，每一层都固定512</p>
<p>batch norm VS LayerNorm</p>
<p>batch norm：</p>
<p><img src="image-20220808181703718.png" alt="image-20220808181703718" style="zoom:50%;"></p>
<p>二维输入，每一行是一个样本，每一列是一个特征</p>
<p>整个数据扫一遍后，所有数据的均值方差存起来，预测的时候用</p>
<p>预测的时候会计算全局的均值</p>
<p>batch norm还会学一个 $\lambda,\beta$出来，通过学习放成任意均值、方差</p>
<p>对于高维的：</p>
<p><img src="image-20220808181713456.png" alt="image-20220808181713456" style="zoom:50%;"></p>
<p>蓝色的切出来，拉成一条，做均值方差</p>
<p>layer norm：</p>
<p>对每一行做均值为0，方差为1</p>
<p><img src="image-20220808181722146.png" alt="image-20220808181722146" style="zoom:50%;"></p>
<p>黄色的切法</p>
<p>layer norm用的比较多，因为时序模型中，样本的长度可能会发生变化</p>
<p><img src="image-20220808181730490.png" alt="image-20220808181730490" style="zoom:50%;"></p>
<p>蓝色画阴影的是有效值。样本长度变化比较大时，做小批量时，算出来的均值方差抖动比较大。且做预测前要把全局的均值方差记录下来，如果碰到一个新的预测样本特别长，之前算的均值方差不太好用。</p>
<p>但对于layer norm是每个样本自己算均值和方差，不需要存全局的均值方差，稳定一些</p>
<p><img src="image-20220808181740780.png" alt="image-20220808181740780" style="zoom:50%;"></p>
<p><img src="image-20220808181745545.png" alt="image-20220808181745545" style="zoom:50%;"></p>
<p>$Q$: n× $d_k$，K：m× $d_k$，n个样本，n个query，$QK^T$就是一个 n×m的矩阵，每一行是一个query与其他所有key的内积值，除以根号 $d_k$ 再做softmax。softmax是分别对每一行做，行与行之间是独立的，得到权重</p>
<p>再乘 V 后，得到 n× $d_v$的输出，每一行是一个输出</p>
<p>$d_k$不是很大时，除不除无所谓，当 $d_k$ 比较大时，两个向量比较长，做点积，结果可能很大，值相对的差距会变大，做softmax后就有可能有值接近1，剩下的值会更靠近0，算梯度时会比较小，会跑不动</p>
<p><img src="image-20220808181754453.png" alt="image-20220808181754453" style="zoom:50%;"></p>
<p>左图的mask指对于 $q_t$ 和 $k_t$ 及其之后计算的那些值换成一个非常大的负数，如-1e10，进入softmax时会变成0，output时只用了对应的 $v_1$ 到 $v_{t-1}$ 。</p>
<p>multi-head attention:</p>
<p>与其做一个单个的注意力函数，不如把query、key、value投影到低维，投影h次，再做h次的注意力函数，把每一个函数的输出并在一起，再投影回来得到最终的输出</p>
<p>右图，V,K,Q先进入线性层，就是投影到低的维度，再做左图的scaled dot-product attention。这里做h次，会得到h个输出。再把这些向量全部合并再一次，再做一次线性的投影回到multi-head attention。</p>
<p>如果只是左图，没有什么可学的参数。为了识别不同的模式，希望有一些不一样的计算像素的方法。</p>
<p>如果用的是加性的注意力机制，里面还是有参数可以学的，也许还是可以学到这些东西。</p>
<p>右图的方法中投影的w矩阵是可以学的。像卷积神经网络中有多个输出通道</p>
<p><img src="image-20220808181804578.png" alt="image-20220808181804578" style="zoom:50%;"></p>
<p>$W^Q,Q^K,W^V$是可以学习的，把Q,K,V投影到d维上再做注意力函数</p>
<p>因为有残差连接，输入输出大小一样，这里投影的时候就投成输出的维度除以h</p>
<p><img src="image-20220808181816220.png" alt="image-20220808181816220" style="zoom:50%;"></p>
<p>三个不一样的注意力层</p>
<p><img src="image-20220808181824646.png" alt="image-20220808181824646" style="zoom:50%;"></p>
<p>这一层的key和value来自编码器的输出，query来自解码器下一个attention的输入</p>
<p>编码器的输出是n个长为d的向量，解码器的下面一层masked multi-head attention的输出也是n个长为d的向量</p>
<p><img src="image-20220808181833238.png" alt="image-20220808181833238" style="zoom:50%;"></p>
<p>绿色的作为query</p>
<p>这个attention干的事情就是有效地把编码器的一些输出根据我想要的东西拎出来</p>
<p>3.3节讲的是蓝色的feed-forward</p>
<p><img src="image-20220808181843818.png" alt="image-20220808181843818" style="zoom:50%;"></p>
<p>对每一个词作用相同的MLP，就是point-wise</p>
<p>线性层+relu+线性层</p>
<p>W1把512投影成2048，W2又把2048投影成512</p>
<p><img src="image-20220808181851387.png" alt="image-20220808181851387" style="zoom:50%;"></p>
<p><img src="image-20220808181856916.png" alt="image-20220808181856916" style="zoom:50%;"></p>
<p>把权重乘了根号 $d_{model}$ 。因为在学embedding时会把每一个向量的L2-norm学成相对比较小的，如1，不管维度多大。但之后要加positional encoding，它不会随着长度变长把norm固定住。所以乘根号 $d_{model}$后，使两者在scale上差不多</p>
<p>positional encoding 在输入中加入时序信息</p>
<p><img src="image-20220808181907992.png" alt="image-20220808181907992" style="zoom:50%;"></p>
<p>Sequential Operations衡量并行度</p>
<p>Maximum Path Length表示信息从一个数据点走到另外一个数据点要走多远（越短越好）</p>
<p>self-Attention(restricted)指只和最近的r个做query</p>
<p>attention对模型的假设更少，导致需要更多的数据更大的模型才能训练出来</p>
<p>transformer的模型比较大</p>
<p>实验：</p>
<p>用的是37000个token的一个字典，而且是在英语和德语之间共享的，不再为英语构造一个字典，不再为德语构造一个字典。好处是整个编码器和解码器的embedding可以用一个，模型更加简单。即编码器和解码器的embedding是共享权重的</p>
<p>对模型做正则化：</p>
<p>residual dropout：对每一个子层，就是多头注意力层和之后的MLP层，每个层的输出之后，在他进入残差连接之前和在进入layer norm之前，用了dropout，dropout率是0.1，也就是把这些输出的10%的那些元素只乘0.1，剩下的乘1.1。在词嵌入加positional encoding时也用了dropout。使用了大量dropout层。</p>
<p>label smoothing：用softmax去学一个东西的时候，让正确的label的softmax值逼近于1，但sofymax很难逼近于1，因为softmax是一个很soft的东西，当输出无限大时，才能逼近于1.可以让正确的那个softmax值往下降一点，这篇中降得比较大，降到了0.1，即置信度0.1就行。但模型不那么确信，可以提升精度和BLEU的分数</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/08/08/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
