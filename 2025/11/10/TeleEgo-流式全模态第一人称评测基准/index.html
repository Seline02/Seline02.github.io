<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>TeleEgo|流式全模态第一人称评测基准 | Seline's blog</title><meta name="author" content="Seline"><meta name="copyright" content="Seline"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="背景与动机​    第一人称AI助手指的是能够通过可穿戴设备（如智能眼镜、头戴相机）从用户的第一人称视角持续观察世界，理解用户正在做什么、看到什么、与谁交流，并在需要时提供智能帮助的AI系统。与传统的语音助手或聊天机器人不同，第一人称AI助手需要“看到”用户眼前的场景、“听到”周围的对话、“记住”过去发生的事情，并在恰当的时机主动或被动地给出建议。要真正实用，这些助手必须同时具备三项紧密整合的核心">
<meta property="og:type" content="article">
<meta property="og:title" content="TeleEgo|流式全模态第一人称评测基准">
<meta property="og:url" content="https://seline02.github.io/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/index.html">
<meta property="og:site_name" content="Seline&#39;s blog">
<meta property="og:description" content="背景与动机​    第一人称AI助手指的是能够通过可穿戴设备（如智能眼镜、头戴相机）从用户的第一人称视角持续观察世界，理解用户正在做什么、看到什么、与谁交流，并在需要时提供智能帮助的AI系统。与传统的语音助手或聊天机器人不同，第一人称AI助手需要“看到”用户眼前的场景、“听到”周围的对话、“记住”过去发生的事情，并在恰当的时机主动或被动地给出建议。要真正实用，这些助手必须同时具备三项紧密整合的核心">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://seline02.github.io/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/logo.png">
<meta property="article:published_time" content="2025-11-10T04:58:16.000Z">
<meta property="article:modified_time" content="2025-12-05T07:20:56.276Z">
<meta property="article:author" content="Seline">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://seline02.github.io/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/logo.png"><link rel="shortcut icon" href="/img/robot.jpeg"><link rel="canonical" href="https://seline02.github.io/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'TeleEgo|流式全模态第一人称评测基准',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-12-05 15:20:56'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 6.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/user_pic.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/logo.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Seline's blog</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">TeleEgo|流式全模态第一人称评测基准</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-10T04:58:16.000Z" title="发表于 2025-11-10 12:58:16">2025-11-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-05T07:20:56.276Z" title="更新于 2025-12-05 15:20:56">2025-12-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="TeleEgo|流式全模态第一人称评测基准"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="背景与动机"><a href="#背景与动机" class="headerlink" title="背景与动机"></a>背景与动机</h1><p>​    <span style="color:#CD5C5C; font-weight:bold;">第一人称AI助手</span>指的是能够通过可穿戴设备（如智能眼镜、头戴相机）从用户的第一人称视角持续观察世界，理解用户正在做什么、看到什么、与谁交流，并在需要时提供智能帮助的AI系统。与传统的语音助手或聊天机器人不同，第一人称AI助手需要“看到”用户眼前的场景、“听到”周围的对话、“记住”过去发生的事情，并在恰当的时机主动或被动地给出建议。要真正实用，这些助手必须同时具备三项紧密整合的核心能力：</p>
<ul>
<li><span style="color:#CD5C5C; font-weight:bold;">记忆能力</span>，能够保留并回忆不断增长的过去信息流；</li>
<li><span style="color:#CD5C5C; font-weight:bold;">流式决策能力</span>，能够在连续的音视频输入中及时做出判断；</li>
<li><span style="color:#CD5C5C; font-weight:bold;">多模态理解能力</span>，能够以统一的方式解释摄像头看到的内容、麦克风听到的声音以及用户通过语言表达的意图。</li>
</ul>
<p><img src="image-20251110165232750.png" alt="image-20251110165232750" style="zoom:80%;"></p>
<p>现有benchmark在评估第一人称AI助手的这些能力时存在三大缺陷：</p>
<ul>
<li><span style="color:#0080FF; font-weight:bold;">评估模式割裂，不能同时测记忆和实时性能</span>：部分benchmark仅测试离线长期记忆（如X-LeBench），部分仅测试短窗口流式理解（如StreamingBench、VStream-QA）；</li>
<li><span style="color:#0080FF; font-weight:bold;">缺乏真实第一人称流式场景</span>：大多数数据集使用第三人称或静态视频（如OVBench、StreamingBench、OVO-Bench），回避了自我运动和视角变化的挑战；少数第一人称数据集时长过短且局限于模态和特定任务（如ODV-Bench仅5-90秒的驾驶场景）；</li>
<li><span style="color:#0080FF; font-weight:bold;">缺乏长时连续的真实世界记录</span>：多数benchmark由短片段或图像集构成（EgoThink，ODV-Bench），缺少统一的时间轴。</li>
</ul>
<p><br></p>
<p>针对以上的问题，TeleEgo做了什么？</p>
<ul>
<li>收集<span style="color:#CD5C5C; font-weight:bold;">真实场景的长时连续数据</span>：5个志愿者戴着第一人称相机连续拍摄3天（每人14小时以上），覆盖工作、生活、社交、外出等各种场景，把所有视频、音频、对话记录都对齐到一条统一的时间轴上——这样就能问“你早上9点做了什么？”和“下午3点那件事后来怎么样了？”这类跨越几小时甚至几天的问题；</li>
<li>设计了3291道<span style="color:#CD5C5C; font-weight:bold;">不同类别的QA测试题</span>：分成记忆（能不能记住过去）、理解（能不能看懂当前）、跨记忆推理（能不能把不同时间的事情联系起来）三大类，每道题都标注了答案需要的证据在视频的哪个时间点，这样就能精准知道模型到底哪方面能力不行；</li>
<li>提出<span style="color:#CD5C5C; font-weight:bold;">2个指标</span>，用“<span style="color:#CD5C5C; font-weight:bold;">流式评估</span>”的方式测试：<ul>
<li><strong>RTA（实时准确率）</strong>：让模型像真人一样边看视频边回答问题，而且必须在限定时间内答出来，模拟真实助手需要及时响应；</li>
<li><strong>MPT（记忆持续时间）</strong>：答对一道题之后，过一段时间再问同样的问题，但不给原始画面，看模型能“记住”多久，来测AI助手的“健忘程度”。</li>
</ul>
</li>
</ul>
<p><strong>一句话概括</strong>，TeleEgo是首个全模态第一人称benchmark，基于多场景、多任务、多模态的长时视频流，通过流式评估方式全面测试AI助手的记忆、理解和跨记忆推理能力。</p>
<p><br></p>
<hr>
<p><br></p>
<h1 id="数据集制作"><a href="#数据集制作" class="headerlink" title="数据集制作"></a>数据集制作</h1><h2 id="数据采集设计"><a href="#数据采集设计" class="headerlink" title="数据采集设计"></a>数据采集设计</h2><p><img src="image-20251111000448405.png" alt="image-20251111000448405" style="zoom:80%;"></p>
<p>我们招募了<strong>5名参与者</strong>，男女平衡，来自不同地区、有不同文化背景和性格，尽量代表普通大众，这种设计避免了以往数据集的单一性问题（如单一家庭、单一实验室环境）。每位参与者需要佩戴<strong>第一人称可穿戴相机</strong>（egocentric wearable camera）连续录制<strong>3天</strong>，按照预定义的场景脚本，覆盖<strong>4大生活主题</strong>，既有独处也有群体互动，既有室内也有室外环境，以此来确保数据的时间连续性和情境多样性。视频平均时长<strong>14.4小时</strong>。</p>
<p>为了让AI助手真正能理解人类的日常生活，我们精心挑选了四类最常见的生活场景。每类场景都有自己的特点，也对应着不同的AI能力考验：</p>
<ul>
<li><p><strong>工作与学习</strong>：比如做演讲、开会、写代码、学习、前台接待等</p>
<ul>
<li>特点：知识驱动型任务，需频繁使用工具、切换任务；</li>
<li>AI能力挑战：追踪会议进度、记录关键信息、提供任务提醒——像一个智能助理；</li>
</ul>
</li>
<li><p><strong>社交活动</strong>：比如打牌（UNO、麻将、扑克）、打游戏、打台球等</p>
<ul>
<li>特点：多人互动，轮次明确，手势语言并用，状态动态变化；</li>
<li>AI能力挑战：理解游戏规则、识别社交信号、把握互动节奏——不仅要“看懂”还要“懂规则”；</li>
</ul>
</li>
<li><p><strong>生活日常</strong>：比如买菜、做饭、散步、健身等</p>
<ul>
<li>特点：流程半结构化，涉及大量物品操作和习惯性动作；</li>
<li>AI能力挑战：识别活动阶段、理解动作序列、推断隐含状态——掌握“生活常识”；</li>
</ul>
</li>
<li><p><strong>外出文化</strong>：比如下馆子吃饭、约会、逛博物馆</p>
<ul>
<li>特点：复杂公共环境，光照音频多变，社交礼仪丰富；</li>
<li>AI能力挑战：应对环境干扰、理解文化语境、处理突发状况——真实世界的“压力测试”</li>
</ul>
</li>
</ul>
<p><br><br></p>
<h2 id="数据处理流程"><a href="#数据处理流程" class="headerlink" title="数据处理流程"></a>数据处理流程</h2><p><img src="image-20251121160402924.png" alt="image-20251121160402924" style="zoom:50%;"></p>
<p>原始的第一人称录制数据需要经过系统化处理，才能转化为可用于评测AI模型的标准化数据集。</p>
<p><strong>隐私保护处理</strong>： 由于录制内容涉及真实人物和场景，我们对所有数据进行了严格的去标识化处理：对视频中的人脸进行模糊处理，移除非参与者的语音内容，遮盖任何可能泄露隐私的视觉或音频信息。这套处理方案在最大程度保护隐私的同时，保持了数据的生态真实性——AI仍能理解场景中的动作、物品和交互关系。</p>
<p><strong>多模态数据对齐</strong>： TeleEgo的一大创新在于<strong>全局时间轴对齐机制</strong>。不同于传统数据集的片段化处理，我们构建了一个统一的时间坐标系：</p>
<ul>
<li><strong>时间轴构建</strong>：每个参与者3天的所有录制内容被映射到一条连续时间轴上，精度达到毫秒级</li>
<li><strong>多流同步</strong>：视频帧、音频采样、语音识别结果通过时间戳严格同步，确保跨模态信息的时序一致性</li>
<li><strong>事件锚定</strong>：关键事件（如”开始做饭”、”会议结束”）被标记为时间锚点，支持跨时段的事件关联查询</li>
</ul>
<p>这种设计让AI可以回答“上午10点做了什么”或“午饭后发生了什么”这类需要时间推理的问题，真正模拟了人类的情景记忆机制。</p>
<p><strong>双层文本标注体系</strong>：为增强数据的语义丰富度，我们构建了双层标注体系：</p>
<ul>
<li><strong>第一层：语音转录标注</strong><ul>
<li>将多人对话场景中的口语交流自动转录为文本，随后进行人工校验并标注说话人身份，形成完整的时序对话记录。</li>
</ul>
</li>
<li><strong>第二层：视觉叙述标注</strong><ul>
<li>参与者对正在进行的活动和关键环境细节进行口头描述，如”正在切菜”、”会议室有5人参会”等，关键动作和状态变化被详细记录，形成动作序列描述，场景切换和注意力转移被明确标注，帮助AI理解视角变化。</li>
</ul>
</li>
</ul>
<p><strong>QA题目构建与验证</strong>：</p>
<ul>
<li>基于处理好的多模态数据，我们使用GPT-4o分析多模态数据，理解场景语境和事件逻辑，根据12个认知维度的定义，生成覆盖不同能力的问题。生成的题目随后需要经过人工验证环节：核对事实准确性、确认时间戳标注正确、检查问题表述清晰度、验证答案的唯一性和正确性。</li>
</ul>
<p>通过这一系列处理流程，我们将<strong>70多小时</strong>的原始多人录制数据转化为<strong>3291道高质量测试题</strong>，每道题都有精确的时间定位、多模态证据支撑，确保了评测的准确性和可解释性。</p>
<p><br><br></p>
<h2 id="Benchmark任务设计"><a href="#Benchmark任务设计" class="headerlink" title="Benchmark任务设计"></a>Benchmark任务设计</h2><p><img src="image-20251121160447393.png" alt="image-20251121160447393" style="zoom:50%;"></p>
<p>TeleEgo的任务设计围绕评估第一人称AI助手的多维认知能力展开，构建了一个层次化的评测框架。我们将AI助手的核心能力分解为相互关联但又各有侧重的三大核心认知维度，精心设计了12类细粒度问答任务：</p>
<ul>
<li><p><strong>Memory（记忆）- 58.8%题目</strong></p>
<p>记忆维度评估模型对时间锚定信息的保持和检索能力。这一维度包含5个子任务：</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>子任务</th>
<th>英文名称</th>
<th>测试内容</th>
<th>示例问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>超长期记忆</td>
<td>Ultra-long Memory</td>
<td>测试10-60分钟跨度的事件记忆</td>
<td>“回想近50分钟前，这份纸质文档是如何来我身边的？”</td>
</tr>
<tr>
<td>短期记忆</td>
<td>Short-term Memory</td>
<td>评估几分钟内的瞬时状态回忆</td>
<td>“我刚刚完成了哪项动作？”</td>
</tr>
<tr>
<td>长期记忆</td>
<td>Long-term Memory</td>
<td>检验数小时前的活动细节保持</td>
<td>“对比前两局五子棋，先手顺序的正确描述是？”</td>
</tr>
<tr>
<td>实体追踪</td>
<td>Entity Tracking</td>
<td>持续跟踪特定对象或人物的状态变化</td>
<td>“我（P1）的对家是谁？”</td>
</tr>
<tr>
<td>时间比较与间隔</td>
<td>Temporal Comparison &amp; Interval</td>
<td>判断事件的先后顺序和时间间隔</td>
<td>“P3打出8筒先于P5打出8筒。对吗？”</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p><strong>Understanding（理解）- 27.3%题目</strong></p>
<p>在语境与意图理解维度中，我们聚焦于超越表层视觉描述的高层语义推理，包含四类任务：</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>子任务</th>
<th>英文名称</th>
<th>测试内容</th>
<th>示例问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>意图推理</td>
<td>Intent Inference</td>
<td>推断行为背后的目的和动机</td>
<td>“P5 是否表达了此局形势对他不利的态度？”</td>
</tr>
<tr>
<td>因果理解</td>
<td>Causal Understanding</td>
<td>识别事件间的因果关系链</td>
<td>“为什么大家在 D3-16:54:50 左右笑了？”</td>
</tr>
<tr>
<td>跨模态理解</td>
<td>Cross-modal Understanding</td>
<td>整合视觉、听觉、文本信息形成统一理解</td>
<td>“我说“好冷呀”之后，紧接着在画面中的对应动作是什么？”</td>
</tr>
<tr>
<td>多步推理</td>
<td>Multi-step Reasoning</td>
<td>通过多个推理步骤得出结论</td>
<td>“以下哪些行为组成了“P4就第二阶段求解→记录要点”的多步链条？”</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p><strong>Cross-Memory Reasoning（跨记忆推理）- 13.9%题目</strong></p>
<p>在跨记忆事件推理维度，我们设计了三类高阶任务，要求模型在长期记忆与语境理解的基础上进行全局性整合：</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>子任务</th>
<th>英文名称</th>
<th>测试内容</th>
<th>示例问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>跨实体关系</td>
<td>Cross-entity Relation</td>
<td>推断不同实体间的动态关系演变</td>
<td>“在定制与打包过程中，谁在为谁制作新杯子？”</td>
</tr>
<tr>
<td>时间链理解</td>
<td>Temporal Chain Understanding</td>
<td>理解跨越多个时间段的事件序列</td>
<td>“按时间顺序描述从P4打出6筒开始-新一局开始拿牌的关键动作链。”</td>
</tr>
<tr>
<td>跨时间因果</td>
<td>Cross-temporal Causality</td>
<td>识别时间上分离但因果相关的事件</td>
<td>“服务员在 D3-16:57:55 剪开我面前的食物后，最终发生了什么？”</td>
</tr>
</tbody>
</table>
</div>
<p>为平衡评测的标准化和表达性，我们采用四种互补的问答格式：单选题、多选题、判断题、开放式问答。</p>
<p><br></p>
<hr>
<p><br></p>
<h1 id="数据集结构讲解"><a href="#数据集结构讲解" class="headerlink" title="数据集结构讲解"></a>数据集结构讲解</h1><p>TeleEgo的数据集设计围绕<strong>流式评估</strong>的核心需求，将多天、多场景的第一人称视频整合为统一的时间轴，并通过精细的元数据标注支持跨时间段的记忆推理评测。</p>
<h2 id="目录结构总览"><a href="#目录结构总览" class="headerlink" title="目录结构总览"></a>目录结构总览</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">./TeleEgo/teleego_data</span><br><span class="line">├── QAs                           # 问答数据集</span><br><span class="line">│   ├── merged_P1_A.json          # 参与者P1的QA数据</span><br><span class="line">│   ├── merged_P2_A.json</span><br><span class="line">│   ├── merged_P3_A.json</span><br><span class="line">│   ├── merged_P4_A.json</span><br><span class="line">│   └── merged_P5_A.json</span><br><span class="line">├── outputs                       # 各模型的评测输出</span><br><span class="line">│   ├── gemini25_pro</span><br><span class="line">│   ├── gpt-4o</span><br><span class="line">│   ├── minicpm_o</span><br><span class="line">│   ├── qwen25_omni</span><br><span class="line">│   ├── qwen25_vl</span><br><span class="line">│   └── videochat-online</span><br><span class="line">└── video_merged                  # 合并后的长视频及时间轴</span><br><span class="line">    ├── merged_P1.mp4             # P1的3天视频合并为单个文件</span><br><span class="line">    ├── merged_P2.mp4</span><br><span class="line">    ├── merged_P3.mp4</span><br><span class="line">    ├── merged_P4.mp4</span><br><span class="line">    ├── merged_P5.mp4</span><br><span class="line">    ├── timeline_P1.json          # P1的时间轴映射文件</span><br><span class="line">    ├── timeline_P2.json</span><br><span class="line">    ├── timeline_P3.json</span><br><span class="line">    ├── timeline_P4.json</span><br><span class="line">    └── timeline_P5.json</span><br></pre></td></tr></table></figure>
<p>数据集包含三个核心组件：<strong>合并视频</strong>（<code>merged_Px.mp4</code>）、<strong>时间轴文件</strong>（<code>timeline_Px.json</code>）和<strong>问答数据</strong>（<code>merged_Px_A.json</code>）。三者通过统一的时间戳系统关联，支持流式评测。</p>
<p><br></p>
<p><br></p>
<h2 id="时间轴文件详解"><a href="#时间轴文件详解" class="headerlink" title="时间轴文件详解"></a>时间轴文件详解</h2><p>时间轴文件（timeline_Px.json）是TeleEgo的核心创新之一，它建立了<strong>原始录制时间</strong>与<strong>合并视频时间</strong>之间的双向映射，使得问题可以用自然的”第几天几点几分”格式标注，而评测时能精确定位到合并视频的对应位置。</p>
<p><strong>完整结构示例</strong></p>
<p>以P1为例，其<code>timeline_P1.json</code>的结构如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;profile&quot;</span><span class="punctuation">:</span> <span class="string">&quot;P1&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;dir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/path/to/videos/P1&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;day_prefix_used&quot;</span><span class="punctuation">:</span> <span class="string">&quot;day&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;videos_count&quot;</span><span class="punctuation">:</span> <span class="number">9</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;mapping_by_input_label&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> ... <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;intervals_array&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span> ... <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input_labels_provided&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span> ... <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;merged_total_duration_seconds&quot;</span><span class="punctuation">:</span> <span class="number">48418</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;description_source&quot;</span><span class="punctuation">:</span> <span class="string">&quot;profile_list&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>字段详解：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>字段名</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>profile</code></td>
<td>string</td>
<td>参与者编号，如”P1”</td>
</tr>
<tr>
<td><code>dir</code></td>
<td>string</td>
<td>原始视频文件所在目录</td>
</tr>
<tr>
<td><code>day_prefix_used</code></td>
<td>string</td>
<td>时间标签的日期前缀格式，如”day”表示使用”day1-09:30:00”格式</td>
</tr>
<tr>
<td><code>videos_count</code></td>
<td>int</td>
<td>该参与者的原始视频片段数量</td>
</tr>
<tr>
<td><code>mapping_by_input_label</code></td>
<td>object</td>
<td><strong>核心映射表</strong>：每个原始视频的详细元数据</td>
</tr>
<tr>
<td><code>intervals_array</code></td>
<td>array</td>
<td>视频片段的时间区间列表（简化版）</td>
</tr>
<tr>
<td><code>input_labels_provided</code></td>
<td>array</td>
<td>所有视频的起始时间标签列表</td>
</tr>
<tr>
<td><code>merged_total_duration_seconds</code></td>
<td>float</td>
<td>合并视频的总时长（秒），P1约13.4小时</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<p><strong>mapping_by_input_label 详解</strong></p>
<p>这是时间轴文件最核心的部分，为每个原始视频片段提供完整的时间映射和场景描述。以P1第一天下午的会议视频为例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;D1-P1-pm-1-meeting-1_video_h264_fixed30.mp4&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;start_label&quot;</span><span class="punctuation">:</span> <span class="string">&quot;day1-14:00:00&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;end_label&quot;</span><span class="punctuation">:</span> <span class="string">&quot;day1-15:30:02&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;duration_seconds&quot;</span><span class="punctuation">:</span> <span class="number">5402</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;merged_offset_start_seconds&quot;</span><span class="punctuation">:</span> <span class="number">6090</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;merged_offset_end_seconds&quot;</span><span class="punctuation">:</span> <span class="number">11492</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;P1（星小凝）：女，相机佩戴者（我）；P2（星小凌）：男，体型中等，短发。身着米棕色竖格/条纹短袖衬衫，左手佩戴手表；P3（星小琪）：女，身着黑色短袖上衣（V 领处有红色滚边），胸前有小白色标识；P4（星小龙）：男，体型高瘦，侧身而坐，身着浅灰色外套，内搭浅色上衣，短发；P5（星小仁）：男，体型中等，侧面视角，身着黑色短袖 T 恤，佩戴骨传导耳机，左手戴手表。&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>各字段含义：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>字段名</th>
<th>说明</th>
<th>示例值</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>start_label</code></td>
<td>原始录制的开始时间（自然时间格式）</td>
<td>“day1-14:00:00”</td>
</tr>
<tr>
<td><code>end_label</code></td>
<td>原始录制的结束时间</td>
<td>“day1-15:30:02”</td>
</tr>
<tr>
<td><code>duration_seconds</code></td>
<td>该片段的时长（秒）</td>
<td>5402（约1.5小时）</td>
</tr>
<tr>
<td><code>merged_offset_start_seconds</code></td>
<td>该片段在合并视频中的起始位置（秒）</td>
<td>6090</td>
</tr>
<tr>
<td><code>merged_offset_end_seconds</code></td>
<td>该片段在合并视频中的结束位置（秒）</td>
<td>11492</td>
</tr>
<tr>
<td><code>description</code></td>
<td><strong>人物先验描述</strong>：场景中所有人物的外观特征</td>
<td>见上方示例</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<p><strong>人物先验描述 (description) 的作用</strong></p>
<p><code>description</code>字段提供了该视频片段中所有出场人物的详细外观描述，这是TeleEgo的一个重要设计：</p>
<ol>
<li><strong>身份识别</strong>：由于第一人称视角无法看到佩戴者自己，需要通过外观描述帮助模型识别”谁是谁”</li>
<li><strong>上下文注入</strong>：在流式评测时，当视频切换到新片段，系统会将该片段的description作为system prompt注入，为模型提供人物身份先验</li>
<li><strong>实体追踪支持</strong>：支持”Entity Tracking”类任务，模型需要持续追踪特定人物的行为</li>
</ol>
<p><br></p>
<p><strong>评测代码中的使用方式</strong>（摘自<code>evaluate_qwen25_omni.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seg = timeline_idx.find_segment_for_offset(video_sec)</span><br><span class="line"><span class="keyword">if</span> seg <span class="keyword">and</span> seg[<span class="string">&quot;video&quot;</span>] <span class="keyword">not</span> <span class="keyword">in</span> announced_videos:</span><br><span class="line">    announced_videos.add(seg[<span class="string">&quot;video&quot;</span>])</span><br><span class="line">    sys_text = <span class="string">f&quot;【当前子视频段信息】人物/场景描述：<span class="subst">&#123;seg.get(<span class="string">&#x27;description&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="keyword">or</span> <span class="string">&#x27;（无）&#x27;</span>&#125;</span>。&quot;</span></span><br><span class="line">    sys_msg = &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: [&#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>, <span class="string">&quot;text&quot;</span>: sys_text&#125;]&#125;</span><br></pre></td></tr></table></figure>
<p><br></p>
<p><strong>时间映射机制</strong></p>
<p>时间轴文件实现了双向时间映射：</p>
<ul>
<li><p>原始时间 → 合并视频时间</p>
</li>
<li><p>合并视频时间 → 原始片段</p>
</li>
</ul>
<p><img src="url_1764311608_b82bc3db_b65422a0.jpeg" alt="image" style="zoom:60%;"></p>
<p><br><br><br></p>
<h2 id="问答数据详解"><a href="#问答数据详解" class="headerlink" title="问答数据详解"></a>问答数据详解</h2><p>问答数据 （merged_Px_A.json）是一个JSON数组，每个元素代表一道测试题目。P1共包含593道题目，覆盖12个认知子类别。</p>
<p><strong>问题结构示例</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;category&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Memory&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;subcategory&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Short-term Memory&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;QA_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mc_single&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;question&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我在新建PPT的第一页打上的英文标题是？&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;options&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="string">&quot;A. paper reading&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;B. introduction&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;C. related work&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;D. content&quot;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;answer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;single&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;A&quot;</span><span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;evidence&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;新建空白PPT后在第一页输入&#x27;paper reading&#x27;。&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;timestep&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;start&quot;</span><span class="punctuation">:</span> <span class="string">&quot;D1-09:32:46&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;end&quot;</span><span class="punctuation">:</span> <span class="string">&quot;D1-09:33:06&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;modalities&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;visual&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>字段详解：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>字段名</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>index</code></td>
<td>int</td>
<td>问题编号（1-593）</td>
</tr>
<tr>
<td><code>category</code></td>
<td>string</td>
<td>认知维度大类：Memory / Understanding / Cross-Memory Reasoning</td>
</tr>
<tr>
<td><code>subcategory</code></td>
<td>string</td>
<td>细分子类别，共12种</td>
</tr>
<tr>
<td><code>QA_type</code></td>
<td>string</td>
<td>题目类型：<code>mc_single</code>(单选) / <code>mc_multi</code>(多选) / <code>binary</code>(判断) / <code>open_ended</code>(开放)</td>
</tr>
<tr>
<td><code>question</code></td>
<td>string</td>
<td>问题文本</td>
</tr>
<tr>
<td><code>options</code></td>
<td>array</td>
<td>选项列表（仅选择题有）</td>
</tr>
<tr>
<td><code>answer</code></td>
<td>object</td>
<td>标准答案，包含<code>type</code>和<code>value</code></td>
</tr>
<tr>
<td><code>evidence</code></td>
<td>object</td>
<td>证据信息，包含描述和时间戳</td>
</tr>
<tr>
<td><code>modalities</code></td>
<td>array</td>
<td>所需模态：<code>visual</code> / <code>audio</code></td>
</tr>
</tbody>
</table>
</div>
<p>answer字段结构：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>type值</th>
<th>说明</th>
<th>value示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>single</code></td>
<td>单选题答案</td>
<td><code>[&quot;A&quot;]</code></td>
</tr>
<tr>
<td><code>multiple</code></td>
<td>多选题答案</td>
<td><code>[&quot;A&quot;, &quot;B&quot;, &quot;D&quot;]</code></td>
</tr>
<tr>
<td><code>boolean</code></td>
<td>判断题答案</td>
<td><code>[true]</code> 或 <code>[false]</code></td>
</tr>
<tr>
<td><code>text</code></td>
<td>开放式答案</td>
<td><code>[&quot;关键步骤描述...&quot;]</code></td>
</tr>
</tbody>
</table>
</div>
<p><code>evidence</code>字段是流式评测的关键：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;evidence&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;语音中我说&#x27;去打印一份吧&#x27;，随后在浏览器里点击打印并等待。&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestep&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;start&quot;</span><span class="punctuation">:</span> <span class="string">&quot;D1-09:31:03&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;end&quot;</span><span class="punctuation">:</span> <span class="string">&quot;D1-09:31:21&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>timestep.end</code></strong>：决定问题的<strong>触发时间点</strong>——模型必须在看到<code>end</code>时刻的视频后才能被提问</li>
<li><strong><code>timestep.start</code></strong>：答案所需证据的起始时间——模型需要记住从<code>start</code>到<code>end</code>之间的内容</li>
<li><strong>记忆跨度</strong>：<code>end - start</code>反映了该题的记忆难度，从几秒（短期记忆）到几十分钟（超长期记忆）不等</li>
</ul>
<p><br></p>
<p><br></p>
<h2 id="P1视频片段一览"><a href="#P1视频片段一览" class="headerlink" title="P1视频片段一览"></a>P1视频片段一览</h2><p>根据<code>timeline_P1.json</code>，P1的3天活动包含9个视频片段：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>视频文件</th>
<th>时间</th>
<th>时长</th>
<th>场景</th>
<th>合并偏移</th>
</tr>
</thead>
<tbody>
<tr>
<td>D1-P1-am-ppt</td>
<td>Day1 09:30-11:11</td>
<td>1.7h</td>
<td>单人PPT制作</td>
<td>0-6090s</td>
</tr>
<tr>
<td>D1-P1-pm-1-meeting-1</td>
<td>Day1 14:00-15:30</td>
<td>1.5h</td>
<td>5人会议</td>
<td>6090-11492s</td>
</tr>
<tr>
<td>D1-P1-pm-2-meeting-2</td>
<td>Day1 16:00-17:08</td>
<td>1.1h</td>
<td>5人会议</td>
<td>11492-15604s</td>
</tr>
<tr>
<td>D2-P1-am-walking</td>
<td>Day2 10:00-11:01</td>
<td>1h</td>
<td>2人散步</td>
<td>15604-19265s</td>
</tr>
<tr>
<td>D2-P1-pm-1-computer_game</td>
<td>Day2 13:00-14:42</td>
<td>1.7h</td>
<td>4人电脑游戏</td>
<td>19265-25425s</td>
</tr>
<tr>
<td>D2-P1-pm-2-uno</td>
<td>Day2 17:00-18:51</td>
<td>1.9h</td>
<td>5人UNO</td>
<td>25425-32124s</td>
</tr>
<tr>
<td>D3-P1-pm-1-majiang</td>
<td>Day3 13:00-14:46</td>
<td>1.8h</td>
<td>5人麻将</td>
<td>32124-38493s</td>
</tr>
<tr>
<td>D3-P1-pm-2-poker</td>
<td>Day3 15:00-16:25</td>
<td>1.4h</td>
<td>5人扑克</td>
<td>38493-43626s</td>
</tr>
<tr>
<td>D3-P1-pm-3-dinner</td>
<td>Day3 16:30-17:49</td>
<td>1.3h</td>
<td>6人晚餐</td>
<td>43626-48418s</td>
</tr>
</tbody>
</table>
</div>
<p><strong>总计：9个片段，约13.4小时，覆盖工作、社交、生活三大场景类型。</strong></p>
<p><br></p>
<hr>
<p><br></p>
<h1 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h1><p>TeleEgo设计了两个互补的核心指标，共同评估AI助手在流式场景下的综合表现。这两个指标不仅关注”答对了多少”，更关注”何时答对”和”记住多久”。</p>
<h2 id="Real-Time-Accuracy-RTA-实时准确率"><a href="#Real-Time-Accuracy-RTA-实时准确率" class="headerlink" title="Real-Time Accuracy (RTA) - 实时准确率"></a>Real-Time Accuracy (RTA) - 实时准确率</h2><p><strong>定义与计算</strong></p>
<p>RTA衡量模型在限定时间窗口内正确回答问题的能力：</p>
<script type="math/tex; mode=display">\text{RTA} = \frac{\text{在决策窗口内首次答对的题目数}}{\text{总题目数}} \times 100%</script><ul>
<li><strong>决策窗口</strong>：每道题触发后的5秒时间限制</li>
<li><strong>首次回答原则</strong>：只有第一次回答会被评判，防止多次猜测</li>
<li><strong>时间约束</strong>：超时未答或过早回答都视为错误</li>
</ul>
<p>与传统的离线QA准确率不同，RTA引入了时间维度的严格约束：</p>
<ul>
<li><p><strong>模拟真实交互</strong>：真实场景中，用户不会等待AI思考很久</p>
</li>
<li><p><strong>防止作弊</strong>：避免模型通过看到未来内容来回答过去的问题</p>
</li>
<li><p><strong>综合能力考察</strong>：同时测试理解能力和响应速度</p>
</li>
</ul>
<p><strong>评分细则</strong></p>
<ul>
<li><strong>单选题/判断题</strong>：精确匹配选项</li>
<li><strong>多选题</strong>：所有选项完全正确才得分</li>
<li><strong>开放式问答</strong>：使用GPT-4o进行0-5分评分</li>
</ul>
<p><br></p>
<p><br></p>
<h2 id="Memory-Persistence-Time-MPT-记忆持续时间"><a href="#Memory-Persistence-Time-MPT-记忆持续时间" class="headerlink" title="Memory Persistence Time (MPT) - 记忆持续时间"></a>Memory Persistence Time (MPT) - 记忆持续时间</h2><p><img src="image-20251130180604543.png" alt="image-20251130180604543" style="zoom:50%;"></p>
<p><strong>定义与计算</strong></p>
<p>MPT量化模型保持记忆的时长能力。对于在时刻$t^<em>$首次答对的题目，其MPT定义为从$t^</em>$到首次recall失败的时间间隔。</p>
<p><strong>MPT实现机制</strong></p>
<p>对于每个在时刻 $t^{<em>}$ 正确回答的题目，我们安排最多10轮recall评估，分别在$t^{</em>} + r\Delta$时刻进行（其中$\Delta = 60$秒，$r = 1, 2, …, 10$）。在每次recall评估时：</p>
<ul>
<li><strong>原始证据不会重播</strong>：模型只能访问当前正在播放的视频流</li>
<li><strong>早停机制</strong>：一旦某轮评估失败，该题目将从后续轮次中移除</li>
<li><strong>MPT计算</strong>：该题的MPT即为从$t^{*}$到首次失败评估的时间间隔</li>
</ul>
<p><strong>特殊情况处理</strong></p>
<ul>
<li><strong>从未答对</strong>：如果初始问答就错误，MPT记为0</li>
<li><strong>全部通过</strong>：如果10轮recall全部正确（即持续600秒无遗忘），MPT记为600秒（上限值）</li>
<li><strong>视频结束后的recall</strong>：即使视频播放完毕，系统仍会停留在最后一帧，继续逐秒计时，直到所有已安排的recall问题回答完成</li>
</ul>
<p><br></p>
<hr>
<p><br></p>
<h1 id="流式评测实现"><a href="#流式评测实现" class="headerlink" title="流式评测实现"></a>流式评测实现</h1><p>在智能眼镜、头戴设备等实际应用中，模型只能：</p>
<ul>
<li>按时间顺序逐帧接收视频</li>
<li>在需要回答问题时立即响应</li>
<li>无法”回放”之前的画面</li>
</ul>
<p>TeleEgo的评测方式正是模拟这种真实的第一人称实时场景：模型必须像真实AI助手一样”边看边答”，要面对”看过就要记住”的挑战。每次只给模型当前帧，模型必须依靠自身的记忆和理解能力来回答关于过去的问题。</p>
<p>本节将详细介绍流式评测的数据流、不同模型的实现方式差异，以及关键代码逻辑。</p>
<p><br></p>
<p><br></p>
<h2 id="流式评测的整体架构"><a href="#流式评测的整体架构" class="headerlink" title="流式评测的整体架构"></a>流式评测的整体架构</h2><p>流式评测模拟了真实第一人称AI助手的工作场景：视频帧和音频持续输入，问题在特定时间点触发，模型必须在限定时间内基于已观看的内容给出回答。</p>
<p><strong>数据流总览</strong>:</p>
<p><img src="url_1764310851_92cc6cbb_4e852398.png" alt="image" style="zoom:60%;"></p>
<p><br></p>
<p><br></p>
<h2 id="核心评测流程"><a href="#核心评测流程" class="headerlink" title="核心评测流程"></a>核心评测流程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伪代码：流式评测主循环</span></span><br><span class="line"><span class="keyword">while</span> current_sec &lt; video_duration <span class="keyword">or</span> qa_queue_not_empty:</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. 提取当前秒的视频帧和音频</span></span><br><span class="line">    frame, audio = extract_unit(video, current_sec)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 检查是否有问题需要触发</span></span><br><span class="line">    questions = get_questions_at(current_sec)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> questions:</span><br><span class="line">        <span class="comment"># 3a. 无问题：仅观察/预填充</span></span><br><span class="line">        model.observe(frame, audio)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 3b. 有问题：提问并等待回答</span></span><br><span class="line">        <span class="keyword">for</span> question <span class="keyword">in</span> questions:</span><br><span class="line">            answer = model.answer(question, timeout=5s)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 4. 评估答案</span></span><br><span class="line">            is_correct = evaluate(answer, ground_truth)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 5. 若答对，安排recall测试</span></span><br><span class="line">            <span class="keyword">if</span> is_correct <span class="keyword">and</span> phase == <span class="string">&quot;initial&quot;</span>:</span><br><span class="line">                schedule_recall(question, delays=[60s, 120s, ...])</span><br><span class="line">    </span><br><span class="line">    current_sec += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p><br></p>
<p><br></p>
<h2 id="不同模型的流式实现方式"><a href="#不同模型的流式实现方式" class="headerlink" title="不同模型的流式实现方式"></a>不同模型的流式实现方式</h2><p>TeleEgo支持评测多种多模态大模型，但不同模型的流式能力差异显著。我们将其分为<strong>真正流式</strong>和<strong>伪流式</strong>两类。</p>
<p><mark> 我们默认不简单粗暴地累积历史对话和视频（<code>args.use_history=False</code>），对于伪流式的模型，每次只传入当前帧和音频，因为我们希望考察的是模型自身的记忆能力，而非依赖将未经任何压缩或动态管理的原始历史简单堆叠到上下文中——后者只是把”记忆”的负担转嫁给了上下文窗口，本质上是一种外部存储而非模型内部的记忆机制。真正的流式记忆能力应当体现在模型对历史信息的压缩、筛选与动态管理上，而非简单地将所有历史无差别地塞入输入。</mark></p>
<p>以下先举例几种伪流式的模型：</p>
<h3 id="伪流式：VideoChat-Online"><a href="#伪流式：VideoChat-Online" class="headerlink" title="伪流式：VideoChat-Online"></a>伪流式：VideoChat-Online</h3><p>VideoChat-Online 提供了两种模型变体：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>说明</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>VideoChatOnline_Stream</td>
<td>带 Pyramid Memory Bank</td>
<td>对长视频进行层次化压缩</td>
</tr>
<tr>
<td>VideoChatOnline_IT</td>
<td>Instruction Tuning 版本</td>
<td>依赖文本对话历史</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<h4 id="VideoChatOnline-Stream的Pyramid-Memory-Bank机制"><a href="#VideoChatOnline-Stream的Pyramid-Memory-Bank机制" class="headerlink" title="VideoChatOnline_Stream的Pyramid Memory Bank机制"></a>VideoChatOnline_Stream的Pyramid Memory Bank机制</h4><p>官方将 <code>VideoChatOnline_Stream</code> 称为”流式”模型，因为它内部实现了一个层次化的 Pyramid Memory Bank（短期/中期/长期三层记忆）。当某层容量满了，会将最相似的两帧合并，降采样后移到下一层。这种设计让模型能在有限的 token 预算内”记住”更长的视频。</p>
<p>视频特征提取分为三层函数，在每次次调用 <code>chat()</code> 时，都会进行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">extract_feature (最外层：分批处理入口)</span><br><span class="line">    ↓</span><br><span class="line">extract_feature_bank (中间层：ViT 编码)</span><br><span class="line">    ↓</span><br><span class="line">extract_feature_stream (最内层：Memory Bank 更新)</span><br></pre></td></tr></table></figure>
<p><strong>extract_feature</strong>：分批处理，防止显存溢出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_feature</span>(<span class="params">self, pixel_values, num_patches, is_video</span>):</span><br><span class="line">    self.memorybank = <span class="literal">None</span>                    <span class="comment"># 初始化</span></span><br><span class="line">    stride = <span class="number">1000</span>                             <span class="comment"># 每批最多1000帧</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(pixel_values), stride):</span><br><span class="line">        ret_vit_embeds, scale, indices = self.extract_feature_bank(</span><br><span class="line">            pixel_values[i : i + stride],</span><br><span class="line">            start_id=i,                       <span class="comment"># 全局帧索引</span></span><br><span class="line">        )</span><br><span class="line">    self.memorybank = <span class="literal">None</span>                    <span class="comment"># 清理</span></span><br><span class="line">    <span class="keyword">return</span> ret_vit_embeds, scale, indices</span><br></pre></td></tr></table></figure>
<p><strong>extract_feature_bank</strong>：调用 ViT 编码图像，然后送入 Memory Bank</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_feature_bank</span>(<span class="params">self, pixel_values, num_patches, is_video, start_id=<span class="number">0</span></span>):</span><br><span class="line">    <span class="comment"># ViT 编码</span></span><br><span class="line">    vit_embeds = self.vision_model(pixel_values, ...).last_hidden_state</span><br><span class="line">    <span class="comment"># 调用 extract_feature_stream 更新 Memory Bank</span></span><br><span class="line">    partial_vit_embeds, scale, indices = self.extract_feature_stream(</span><br><span class="line">        partial_vit_embeds, partial_cls_tokens, start_id=start_id</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> ...</span><br></pre></td></tr></table></figure>
<p><strong>extract_feature_stream</strong>：逐帧更新 Memory Bank</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_feature_stream</span>(<span class="params">self, vit_embeds, cls_tokens, start_id</span>):</span><br><span class="line">    <span class="comment"># 首次调用时创建 Memory Bank</span></span><br><span class="line">    <span class="keyword">if</span> self.memorybank <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        self.memorybank = HierarchicalMemoryBank(</span><br><span class="line">            [self.long_bank, self.mid_bank, self.short_bank],</span><br><span class="line">            [<span class="number">256</span>, <span class="number">64</span>, <span class="number">16</span>]  <span class="comment"># 各层的 token 数</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 逐帧塞入 Memory Bank</span></span><br><span class="line">    <span class="keyword">for</span> i, vit_embed <span class="keyword">in</span> <span class="built_in">enumerate</span>(vit_embeds):</span><br><span class="line">        self.memorybank.update_memory(pooled_vit_embed, i + start_id, <span class="literal">None</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出当前 Memory Bank 的全部内容</span></span><br><span class="line">    ret_vit_embeds, indices = self.memorybank.output_by_time_order()</span><br><span class="line">    <span class="keyword">return</span> ret_vit_embeds, scale, indices</span><br></pre></td></tr></table></figure>
<p><code>extract_feature</code>结束循环后最后一次返回的 <code>ret_vit_embeds</code> 是整个 memory bank 的内容，包含了从所有帧中筛选/压缩出来的代表性帧。</p>
<p>Memory Bank 的一个重要特点是：<strong>它只在单次 <code>chat()</code> 调用内部存在</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_feature</span>(<span class="params">self, pixel_values, ...</span>):</span><br><span class="line">    self.memorybank = <span class="literal">None</span>      <span class="comment"># ← 开始时清空</span></span><br><span class="line">    <span class="comment"># ... 处理所有帧 ...          # self.memorybank 在循环中是持续累积、压缩、更新</span></span><br><span class="line">    self.memorybank = <span class="literal">None</span>      <span class="comment"># ← 结束时清空</span></span><br></pre></td></tr></table></figure>
<p>所以它的”Stream”指的是：每次调用 <code>chat()</code> 时，Memory Bank 模拟了一个滑动窗口式的记忆压缩过程——新帧进来，旧帧被压缩或丢弃。</p>
<p><strong>但这不是我们想要的流式</strong>，因为：</p>
<ul>
<li>整个过程发生在<strong>单次 chat() 调用内部</strong></li>
<li>调用结束后 <code>self.memorybank = None</code> 全部清空</li>
<li>下次调用要<strong>重新处理所有帧</strong>，两次调用之间无法维持记忆状态</li>
</ul>
<p><br></p>
<h4 id="VideoChatOnline-IT-的对话历史机制"><a href="#VideoChatOnline-IT-的对话历史机制" class="headerlink" title="VideoChatOnline_IT 的对话历史机制"></a>VideoChatOnline_IT 的对话历史机制</h4><p>VideoChatOnline_IT 采用了更直接的方式——通过<strong>文本对话历史</strong>来维护上下文：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># modeling_videochat_online_finetune.py 中的 chat 方法（省略版）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">self, tokenizer, pixel_values, question, ..., history=<span class="literal">None</span></span>):</span><br><span class="line">    history = [] <span class="keyword">if</span> history <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> history</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将历史对话拼接到 prompt</span></span><br><span class="line">    <span class="keyword">for</span> old_question, old_answer <span class="keyword">in</span> history:</span><br><span class="line">        template.append_message(template.roles[<span class="number">0</span>], old_question)</span><br><span class="line">        template.append_message(template.roles[<span class="number">1</span>], old_answer)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 添加当前问题</span></span><br><span class="line">    template.append_message(template.roles[<span class="number">0</span>], question)</span><br><span class="line">    template.append_message(template.roles[<span class="number">1</span>], <span class="literal">None</span>)</span><br><span class="line">    </span><br><span class="line">    query = template.get_prompt()  <span class="comment"># 获取完整prompt</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成回答后，更新历史</span></span><br><span class="line">    history.append((question, response))</span><br><span class="line">    <span class="keyword">return</span> response, history</span><br></pre></td></tr></table></figure>
<p>对话历史以 <code>[(question1, answer1), (question2, answer2), ...]</code> 的形式存储。每次调用时，历史对话被拼接成文本加到 prompt 中，而视频帧则通过 <code>&lt;image&gt;</code> 占位符标记位置，需要<strong>每次重新传入 <code>pixel_values</code></strong>。</p>
<p><br></p>
<h4 id="官方评测脚本分析"><a href="#官方评测脚本分析" class="headerlink" title="官方评测脚本分析"></a>官方评测脚本分析</h4><p><strong>evaluate_online_stream_single.py</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pred, history = model.chat(</span><br><span class="line">    tokenizer,</span><br><span class="line">    pixel_values[: clip[<span class="number">1</span>]],  <span class="comment"># ← 传入问题时刻之前的【所有帧】</span></span><br><span class="line">    question,</span><br><span class="line">    generation_config,</span><br><span class="line">    num_patches_list=num_patches_list[: clip[<span class="number">1</span>]],</span><br><span class="line">    history=<span class="literal">None</span>,  <span class="comment"># ← 不维护对话历史</span></span><br><span class="line">    ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>特点</strong>：</p>
<ul>
<li>使用 <code>VideoChatOnline_Stream</code> 模型</li>
<li>每个问题都传入<strong>从视频开头到问题时刻的所有帧</strong></li>
<li>不维护多轮对话历史</li>
</ul>
<p><br></p>
<p><strong>evaluate_online_stream_multiturn.py</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pred, history = model.chat(</span><br><span class="line">    tokenizer,</span><br><span class="line">    pixel_values[: clip[<span class="number">1</span>]],  <span class="comment"># ← 同样传入所有历史帧</span></span><br><span class="line">    question,</span><br><span class="line">    generation_config,</span><br><span class="line">    num_patches_list=num_patches_list[: clip[<span class="number">1</span>]],</span><br><span class="line">    history=history,  <span class="comment"># ← 维护对话历史</span></span><br><span class="line">    return_history=<span class="literal">True</span>,</span><br><span class="line">    ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>特点</strong>：</p>
<ul>
<li>使用 <code>VideoChatOnline_IT</code> 模型</li>
<li>每个问题传入所有历史帧 + 累积的对话历史</li>
<li>随着视频推进，<code>pixel_values[:clip[1]]</code> 越来越长，prompt 也会越来越长</li>
</ul>
<p><br></p>
<p><strong>evaluate_online_sliding_window.py</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只传入滑动窗口内的帧</span></span><br><span class="line">start_frame = <span class="built_in">max</span>(<span class="number">0</span>, end_frame - <span class="built_in">int</span>(sliding_window * video_fps))</span><br><span class="line">frame_idx = <span class="built_in">list</span>(<span class="built_in">range</span>(end_frame, start_frame - <span class="number">1</span>, -step))</span><br></pre></td></tr></table></figure>
<p><strong>特点</strong>：</p>
<ul>
<li>使用 <code>VideoChatOnline_IT</code> 模型</li>
<li>只能看到窗口内的帧</li>
<li>不维护对话历史</li>
</ul>
<p><br></p>
<h4 id="我们的评测设计"><a href="#我们的评测设计" class="headerlink" title="我们的评测设计"></a>我们的评测设计</h4><p>综上分析，VideoChat-Online<strong>在TeleEgo的应用场景下</strong>属于“伪流式”。VideoChat-Online 官方的评测方式在我们的长时实时场景下也不可行，原因有二：</p>
<ol>
<li><strong>每次传入所有历史帧</strong>：模型不需要”记住”任何东西，因为所有历史信息都在当前输入中。这是一种”开卷考试”——答案就在眼前，只需要找到它。</li>
<li><strong>上下文长度不可持续</strong>：即使采用累积对话历史的方式，随着视频推进，历史帧数和对话文本都在不断增长。对于 TeleEgo 这种长达数小时的视频，累积的 token 数量将远超任何模型的上下文窗口限制。这种”无限制保存所有历史”的方式在长时场景下根本不可行。</li>
</ol>
<p>在真实的第一人称实时场景中（如智能眼镜、头戴设备），视频流是逐帧到达的，用户随时可能提出问题。此时模型不可能”暂停时间”去重新处理之前几小时的所有帧——它必须依靠自身对过去信息的<strong>记忆</strong>来回答问题。TeleEgo 想考察的是：当过去的画面已经消失、只剩下当前帧时，模型能否回忆起之前发生的事情？ 这才是真正的记忆能力，也是第一人称实时场景的核心挑战。因此，我们的评测代码采用了<strong>严格流式</strong>的方式，每次只传入当前时刻的音画：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate_videochat_online.py 核心逻辑</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 逐秒输入，每次只传当前帧</span></span><br><span class="line"><span class="comment"># videochat-online无法处理音频信息，因此我们在chat时传递语音转写文字信息</span></span><br><span class="line">image, audio = utils.build_single_unit(video, video_sec, asr_txt=<span class="literal">True</span>)</span><br><span class="line">image = image_transform(image).unsqueeze(<span class="number">0</span>).to(torch.bfloat16).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用对话历史（args.use_history 默认为 False）</span></span><br><span class="line">pred = model.chat(</span><br><span class="line">    tokenizer,</span><br><span class="line">    image,           <span class="comment"># ← 只传入当前帧</span></span><br><span class="line">    question,</span><br><span class="line">    gen_cfg,</span><br><span class="line">    num_patches_list=[<span class="number">1</span>],</span><br><span class="line">    history=<span class="literal">None</span>,    <span class="comment"># ← 考察模型自身的记忆，而非外挂存储</span></span><br><span class="line">    ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这里 <code>history=None</code> 意味着我们不将历史对话原样拼接到输入中——模型需要依靠自身的能力来”记住”之前看到的内容，而非完全依赖外部存储。</p>
<p><br></p>
<p><br></p>
<h3 id="伪流式：Qwen2-5-Omni"><a href="#伪流式：Qwen2-5-Omni" class="headerlink" title="伪流式：Qwen2.5-Omni"></a>伪流式：Qwen2.5-Omni</h3><p>Qwen2.5-Omni 是阿里推出的端到端多模态模型，支持文本、图像、音频、视频的输入和文本、语音的输出。与 MiniCPM-o 不同，<strong>Qwen2.5-Omni 没有提供显式的流式输入接口</strong>——没有类似 <code>start_session()</code>、<code>input_video_stream()</code> 这样的方法，也没有跨调用保持的 session 机制。</p>
<h4 id="官方的多轮对话方式"><a href="#官方的多轮对话方式" class="headerlink" title="官方的多轮对话方式"></a>官方的多轮对话方式</h4><p>从<a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/multi_round_omni_chatting.ipynb">官方示例</a>可以看到，Qwen2.5-Omni 的多轮对话通过累积 <code>conversations</code> 列表实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Omni Chatting Round 1</span></span><br><span class="line">conversations = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: [&#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>, <span class="string">&quot;text&quot;</span>: <span class="string">&quot;You are Qwen...&quot;</span>&#125;]&#125;</span><br><span class="line">]</span><br><span class="line">conversations.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: [&#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;video&quot;</span>, <span class="string">&quot;video&quot;</span>: video_path_1&#125;]&#125;)</span><br><span class="line">response, audio = inference(conversations)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Omni Chatting Round 2：将上一轮的回复和新视频都加入列表</span></span><br><span class="line">conversations.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: response[<span class="number">0</span>].split(<span class="string">&quot;\n&quot;</span>)[-<span class="number">1</span>]&#125;)</span><br><span class="line">conversations.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: [&#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;video&quot;</span>, <span class="string">&quot;video&quot;</span>: video_path_2&#125;]&#125;)</span><br><span class="line">response, audio = inference(conversations)</span><br></pre></td></tr></table></figure>
<p>这与 VideoChat-Online 的对话历史机制本质相同：<strong>将所有历史对话和视频原样存储，每次推理时全部传入</strong>。对于 TeleEgo 这种长达数小时的视频流，这种方式会导致 token 数量持续增长，很快超出上下文窗口限制。</p>
<p><br></p>
<h4 id="接口层面的限制"><a href="#接口层面的限制" class="headerlink" title="接口层面的限制"></a>接口层面的限制</h4><p>严格来说，Qwen2.5-Omni 底层也支持 <code>past_key_values</code>——这是 Transformer 模型的通用机制。但问题在于：<strong>它没有提供封装好的”逐帧输入-持续积累-按需生成”接口</strong>。</p>
<p>TeleEgo 的第一人称实时场景需要的是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">理想的流式处理：</span><br><span class="line">t=1s:  Frame₁ → prefill →  （只转化成记忆，不生成）</span><br><span class="line">t=2s:  Frame₂ → prefill →  （只转化成记忆，不生成）</span><br><span class="line">t=3s:  Frame₃ → prefill →  （只转化成记忆，不生成）</span><br><span class="line">...</span><br><span class="line">t=100s: 用户提问 → 基于记忆 → generate 回答</span><br></pre></td></tr></table></figure>
<p>MiniCPM-o 提供了 <code>streaming_prefill()</code> 和 <code>streaming_generate()</code> 两个独立的方法，可以实现”存存存…然后在特定时刻 generate”的模式。而 Qwen2.5-Omni 没有这样的接口分离。它的”流式”更多体现在<strong>语音输出的流式生成</strong>（边生成文本边合成语音），而非<strong>视频输入的流式处理</strong>。每次调用都是完整的”输入→生成”流程，无法做到”只输入不生成”的增量积累。</p>
<p>因此在 TeleEgo 的评测中，我们每次都需要完整地调用 <code>generate()</code>，无法利用增量计算的优势。</p>
<p><br></p>
<h4 id="我们的评测设计-1"><a href="#我们的评测设计-1" class="headerlink" title="我们的评测设计"></a>我们的评测设计</h4><p>与 VideoChat-Online 类似，Qwen2.5-Omni 在 TeleEgo 场景下属于伪流式：</p>
<ol>
<li><strong>无跨调用记忆</strong>：每次 <code>generate()</code> 调用都是独立的推理过程，模型内部没有持久化的记忆模块</li>
<li><strong>历史累积不可行</strong>：唯一让模型”记住”历史的方式是累积 <code>conversations</code>，但这会导致 token 数量随时间线性增长，几分钟后就会超出上下文限制</li>
<li><strong>全量重编码</strong>：即使累积历史，每次调用也需要重新编码所有内容，没有增量计算的优化</li>
</ol>
<p>正如前面所提到过的，我们默认不简单粗暴地累积历史对话和视频（<code>args.use_history=False</code>），每次只传入当前帧和音频：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate_qwen25_omni.py 核心逻辑</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建当前时刻的输入</span></span><br><span class="line">image, audio = utils.build_single_unit(video, video_sec)</span><br><span class="line">unit_content = &#123;</span><br><span class="line">    <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">    <span class="string">&quot;content&quot;</span>: [</span><br><span class="line">        &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;video&quot;</span>, <span class="string">&quot;video&quot;</span>: [image]&#125;,  <span class="comment"># 单帧图像，以列表形式传入</span></span><br><span class="line">        &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;audio&quot;</span>, <span class="string">&quot;audio&quot;</span>: audio&#125;     <span class="comment"># 1秒音频</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不累积历史，只传当前帧</span></span><br><span class="line">conversations = [sys_msg] + [unit_content]</span><br></pre></td></tr></table></figure>
<p>这里有几个技术细节需要说明：</p>
<p><strong>1. 单帧输入的处理</strong></p>
<p><code>&quot;video&quot;: [image]</code> 传入的是单帧图像列表。Qwen2.5-Omni 内部有 <code>FRAME_FACTOR = 2</code> 的限制，要求帧数必须是 2 的倍数。当传入单帧时，模型会自动复制成 2 帧以满足这一要求。</p>
<p><strong>2. 音视频 Token 对齐</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">text = processor.apply_chat_template(conversations, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line">text = text[<span class="number">0</span>].replace(<span class="string">&quot;&lt;|audio_bos|&gt;&lt;|AUDIO|&gt;&lt;|audio_eos|&gt;&quot;</span>, <span class="string">&quot;&quot;</span>)  <span class="comment"># 删除独立的音频占位符</span></span><br><span class="line"></span><br><span class="line">audios, images, videos = process_mm_info(conversations, use_audio_in_video=<span class="literal">False</span>)</span><br><span class="line">inputs = processor(text=text, audio=audios, images=images, videos=videos,</span><br><span class="line">                   return_tensors=<span class="string">&quot;pt&quot;</span>, padding=<span class="literal">True</span>,</span><br><span class="line">                   use_audio_in_video=<span class="literal">True</span>,      <span class="comment"># 音频与视频交错编码</span></span><br><span class="line">                   seconds_per_chunk=<span class="number">1.0</span>)        <span class="comment"># 1秒为一个chunk</span></span><br></pre></td></tr></table></figure>
<p>这段代码的逻辑是：</p>
<ul>
<li><code>apply_chat_template</code> 会为单独传入的 <code>&quot;audio&quot;</code> 生成 <code>&lt;|audio_bos|&gt;&lt;|AUDIO|&gt;&lt;|audio_eos|&gt;</code> 占位符，但这个占位符<strong>没有与视频帧时序对齐</strong></li>
<li>后续 <code>processor(..., use_audio_in_video=True)</code> 会重新生成<strong>与视频帧交错对齐</strong>的音频占位符</li>
<li>因此需要手动删除前者，避免重复</li>
</ul>
<p><strong>3. process_mm_info 的参数</strong></p>
<p><code>process_mm_info(conversations, use_audio_in_video=False)</code> 中设为 <code>False</code>，是因为我们的 <code>&quot;video&quot;</code> 字段传入的是帧列表 <code>[image]</code> 而非视频文件路径，音频通过单独的 <code>&quot;audio&quot;</code> 字段传入。实际的音视频对齐在后续的 <code>processor()</code> 调用中通过 <code>use_audio_in_video=True</code> 完成。</p>
<p><strong>4. TMRope 时序对齐</strong></p>
<p>Qwen2.5-Omni 使用 TMRope（Time-aware Multimodal Rotary Position Embedding）编码多模态输入的时序关系。设置 <code>seconds_per_chunk=1.0</code> 后，1 秒音频和 1 帧图像就会被对齐到同一个 chunk 中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;|vision_bos|&gt;&lt;|audio_bos|&gt;</span><br><span class="line">&lt;|video_placeholder|&gt;...&lt;|video_placeholder|&gt;   ← 视频token</span><br><span class="line">&lt;|audio_placeholder|&gt;...&lt;|audio_placeholder|&gt;   ← 音频token（与上面时序对齐）</span><br><span class="line">&lt;|audio_eos|&gt;&lt;|vision_eos|&gt;</span><br></pre></td></tr></table></figure>
<p><br></p>
<p><br></p>
<h3 id="流式模型：MiniCPM-o-2-6"><a href="#流式模型：MiniCPM-o-2-6" class="headerlink" title="流式模型：MiniCPM-o 2.6"></a>流式模型：MiniCPM-o 2.6</h3><p>与前面两个模型不同，MiniCPM-o 提供了显式的流式输入接口，支持”逐 chunk 输入”的使用模式。</p>
<h4 id="显式的流式接口"><a href="#显式的流式接口" class="headerlink" title="显式的流式接口"></a>显式的流式接口</h4><p>MiniCPM-o 提供了三个核心方法：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>reset_session()</code></td>
<td>重置会话，清空 KV Cache</td>
</tr>
<tr>
<td><code>streaming_prefill()</code></td>
<td>增量输入一个 chunk（1帧图像 + 1秒音频），更新 KV Cache</td>
</tr>
<tr>
<td><code>streaming_generate()</code></td>
<td>基于当前 KV Cache 生成回答</td>
</tr>
</tbody>
</table>
</div>
<p> <code>streaming_prefill</code> 的核心实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># modeling_minicpmo.py 中的 streaming_prefill 方法（简化版）</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.inference_mode()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">streaming_prefill</span>(<span class="params">self, session_id, msgs, tokenizer, ...</span>):</span><br><span class="line">    <span class="comment"># 1. 处理当前 chunk 的视觉和音频特征</span></span><br><span class="line">    model_inputs[<span class="string">&quot;inputs_embeds&quot;</span>], _ = self.get_vllm_embedding(model_inputs)</span><br><span class="line">    inputs_embeds = self.get_omni_embedding(model_inputs, ...)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 计算 attention_mask，包含历史 KV Cache 的长度</span></span><br><span class="line">    <span class="keyword">if</span> self.llm_past_key_values <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        cache_length = self.llm_past_key_values[<span class="number">0</span>][<span class="number">0</span>].shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        cache_length = <span class="number">0</span></span><br><span class="line">    attention_mask = torch.ones((<span class="number">1</span>, cache_length + inputs_embeds.shape[<span class="number">1</span>]), ...)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 前向传播，只处理当前 chunk</span></span><br><span class="line">    outputs = self.llm(</span><br><span class="line">        past_key_values=self.llm_past_key_values,  <span class="comment"># ← 传入历史 KV Cache</span></span><br><span class="line">        inputs_embeds=inputs_embeds,               <span class="comment"># ← 只传入当前 chunk 的 embedding</span></span><br><span class="line">        attention_mask=attention_mask,</span><br><span class="line">        use_cache=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 保存更新后的 KV Cache</span></span><br><span class="line">    self.llm_past_key_values = outputs[<span class="string">&quot;past_key_values&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>与其他模型的对比：</p>
<ul>
<li><strong>Qwen2.5-Omni</strong>：底层有 <code>past_key_values</code>，但没有暴露 “只 prefill 不 generate” 的接口</li>
<li><strong>VideoChat-Online</strong>：Memory Bank 在每次 <code>chat()</code> 结束后清空，无法跨调用保持</li>
<li><strong>MiniCPM-o</strong>：<code>streaming_prefill()</code> 和 <code>streaming_generate()</code> 分离，支持”存存存…然后 generate”</li>
</ul>
<p><br></p>
<h4 id="流式接口的局限"><a href="#流式接口的局限" class="headerlink" title="流式接口的局限"></a>流式接口的局限</h4><p>虽然 MiniCPM-o 提供了显式的流式接口，但它<strong>没有设计动态的记忆管理机制</strong>。KV Cache 只是维持”记忆”的方法之一，而 MiniCPM-o 只是简单地让 KV Cache 不断增长，没有任何压缩、淘汰或滑动窗口策略。</p>
<p>这导致了两个问题：</p>
<ol>
<li><strong>显存溢出</strong>：随着视频输入时间增长，KV Cache 膨胀，最终会出现 CUDA Out of Memory</li>
<li><strong>生成质量下降</strong>：即使显存足够，过长的 KV Cache 也会导致模型注意力分散，出现胡言乱语的情况</li>
</ol>
<p><br></p>
<h4 id="我们的评测设计-2"><a href="#我们的评测设计-2" class="headerlink" title="我们的评测设计"></a>我们的评测设计</h4><p>由于 MiniCPM-o 没有内置的记忆管理，我们在评测代码中<strong>手动设置了 session 窗口大小</strong>，每隔一段时间重置记忆：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate_minicpm_o.py 核心逻辑</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化会话</span></span><br><span class="line">model.reset_session()</span><br><span class="line">session_id = <span class="string">f&quot;streaming_session&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> current_sec <span class="keyword">in</span> <span class="built_in">range</span>(video_duration):</span><br><span class="line">    <span class="comment"># 每10分钟手动重置会话，避免 OOM 和生成质量下降</span></span><br><span class="line">    <span class="keyword">if</span> current_sec % <span class="number">600</span> == <span class="number">0</span>:</span><br><span class="line">        model.reset_session()</span><br><span class="line">        session_id = <span class="string">f&quot;streaming_session_<span class="subst">&#123;current_sec&#125;</span>s&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建当前 chunk：1帧图像 + 1秒音频</span></span><br><span class="line">    image, audio = utils.build_single_unit(video, video_sec)</span><br><span class="line">    unit_content = [sys_text, <span class="string">&quot;&lt;unit&gt;&quot;</span>, image, audio]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> questions_at_current:</span><br><span class="line">        <span class="comment"># 没有问题时：只 prefill，不 generate</span></span><br><span class="line">        _ = model.streaming_prefill(</span><br><span class="line">            session_id=session_id,</span><br><span class="line">            msgs=[&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: unit_content&#125;],</span><br><span class="line">            tokenizer=tokenizer</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 有问题时：prefill 当前 chunk + 问题，然后 generate</span></span><br><span class="line">        <span class="keyword">for</span> question <span class="keyword">in</span> questions_at_current:</span><br><span class="line">            unit_input = unit_content + [question]</span><br><span class="line">            _ = model.streaming_prefill(</span><br><span class="line">                session_id=session_id,</span><br><span class="line">                msgs=[&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: unit_input&#125;],</span><br><span class="line">                tokenizer=tokenizer</span><br><span class="line">            )</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 基于累积的 KV Cache 生成回答</span></span><br><span class="line">            pred, elapsed_time = stream_with_timeout(session_id)</span><br></pre></td></tr></table></figure>
<p>其中 <code>stream_with_timeout</code> 调用了 <code>streaming_generate</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">stream_with_timeout</span>(<span class="params">session_id</span>):</span><br><span class="line">    res = model.streaming_generate(</span><br><span class="line">        session_id=session_id,</span><br><span class="line">        tokenizer=tokenizer,</span><br><span class="line">        generate_audio=<span class="literal">False</span>,</span><br><span class="line">        max_new_tokens=args.max_new_tokens,</span><br><span class="line">        stopping_criteria=stopping_criteria  <span class="comment"># 5秒超时</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    pred = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> chunk <span class="keyword">in</span> res:</span><br><span class="line">        pred += chunk.get(<span class="string">&quot;text&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pred, elapsed_time</span><br></pre></td></tr></table></figure>
<p>MiniCPM-o 是我们评测的三个模型中唯一提供真正流式接口的。它支持逐 chunk 增量输入，但由于缺乏动态记忆管理，实际使用时需要手动控制 session 窗口大小。这也反映了当前流式多模态模型的一个共性问题：如何在有限资源下高效管理长时记忆，仍是一个开放的研究方向。</p>
<p><br></p>
<p><br></p>
<h2 id="关键代码逻辑详解"><a href="#关键代码逻辑详解" class="headerlink" title="关键代码逻辑详解"></a>关键代码逻辑详解</h2><h3 id="时间戳映射（TimelineIndex）"><a href="#时间戳映射（TimelineIndex）" class="headerlink" title="时间戳映射（TimelineIndex）"></a>时间戳映射（TimelineIndex）</h3><p>问题的触发时间以自然语言格式标注（如”D1-14:30:00”），需要转换为合并视频中的秒数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TimelineIndex</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">label_to_merged_seconds</span>(<span class="params">self, label: <span class="built_in">str</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;将 &#x27;D1-14:30:00&#x27; 转换为合并视频中的秒数&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 1. 解析为绝对秒数</span></span><br><span class="line">        abs_s = self._parse_day_label(label)  </span><br><span class="line">        <span class="comment"># D1-14:30:00 → (1-1)*86400 + 14*3600 + 30*60 = 52200</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 查找包含该时间的视频片段</span></span><br><span class="line">        <span class="keyword">for</span> seg <span class="keyword">in</span> self.segments:</span><br><span class="line">            <span class="keyword">if</span> seg[<span class="string">&quot;start_abs&quot;</span>] &lt;= abs_s &lt;= seg[<span class="string">&quot;end_abs&quot;</span>]:</span><br><span class="line">                <span class="comment"># 3. 计算在合并视频中的位置</span></span><br><span class="line">                offset = abs_s - seg[<span class="string">&quot;start_abs&quot;</span>]</span><br><span class="line">                <span class="keyword">return</span> seg[<span class="string">&quot;merged_offset_start_seconds&quot;</span>] + offset</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p><br></p>
<h3 id="问题队列管理"><a href="#问题队列管理" class="headerlink" title="问题队列管理"></a>问题队列管理</h3><p>问题按触发时间排序，支持动态插入recall问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化队列（仅initial问题）</span></span><br><span class="line">qa_queue = []</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> qa_items:</span><br><span class="line">    t_end = extract_timestamp(item)  <span class="comment"># 从evidence.timestep.end获取</span></span><br><span class="line">    qa_queue.append((t_end, item, <span class="string">&quot;initial&quot;</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">qa_queue.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])  <span class="comment"># 按时间排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主循环中动态插入recall</span></span><br><span class="line"><span class="keyword">if</span> correct <span class="keyword">and</span> phase == <span class="string">&quot;initial&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):  <span class="comment"># 最多10轮recall</span></span><br><span class="line">        recall_t = t_end + r * <span class="number">60</span>  <span class="comment"># 每60秒recall一次</span></span><br><span class="line">        qa_queue.append((recall_t, item, <span class="string">&quot;recall&quot;</span>, r))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 重新排序队列</span></span><br><span class="line">    qa_queue.sort(key=<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">2</span>] == <span class="string">&quot;recall&quot;</span>))</span><br></pre></td></tr></table></figure>
<p><br></p>
<h3 id="人物先验注入"><a href="#人物先验注入" class="headerlink" title="人物先验注入"></a>人物先验注入</h3><p>当视频切换到新片段时，将人物描述作为system prompt注入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">seg = timeline_idx.find_segment_for_offset(current_sec)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> seg <span class="keyword">and</span> seg[<span class="string">&quot;video&quot;</span>] <span class="keyword">not</span> <span class="keyword">in</span> announced_videos:</span><br><span class="line">    announced_videos.add(seg[<span class="string">&quot;video&quot;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 注入人物先验描述</span></span><br><span class="line">    sys_text = <span class="string">f&quot;【当前子视频段信息】人物/场景描述：<span class="subst">&#123;seg[<span class="string">&#x27;description&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># MiniCPM-o: 作为unit内容的一部分</span></span><br><span class="line">    unit_content = [sys_text, <span class="string">&quot;&lt;unit&gt;&quot;</span>, image, audio]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># VideoChat-Online: 设置为model.system_message</span></span><br><span class="line">    model.system_message = sys_text</span><br></pre></td></tr></table></figure>
<p><br></p>
<h3 id="5秒决策窗口"><a href="#5秒决策窗口" class="headerlink" title="5秒决策窗口"></a>5秒决策窗口</h3><p>通过自定义StoppingCriteria实现超时控制：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TimeLimitStoppingCriteria</span>(<span class="title class_ inherited__">StoppingCriteria</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, max_time_seconds: <span class="built_in">float</span></span>):</span><br><span class="line">        self.start_time = time.time()</span><br><span class="line">        self.max_time_seconds = max_time_seconds</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, input_ids, scores, **kwargs</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 超过5秒则停止生成</span></span><br><span class="line">        <span class="keyword">return</span> (time.time() - self.start_time) &gt;= self.max_time_seconds</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">stopping_criteria = StoppingCriteriaList([</span><br><span class="line">    TimeLimitStoppingCriteria(max_time_seconds=<span class="number">5.0</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">output = model.generate(..., stopping_criteria=stopping_criteria)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h3 id="答案解析与评估"><a href="#答案解析与评估" class="headerlink" title="答案解析与评估"></a>答案解析与评估</h3><p>根据题目类型采用不同的解析和评估策略：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse_prediction</span>(<span class="params">text: <span class="built_in">str</span>, qtype: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;解析模型输出为标准格式&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> qtype == <span class="string">&quot;mc_single&quot;</span>:</span><br><span class="line">        <span class="comment"># 单选题：提取第一个大写字母</span></span><br><span class="line">        letters = re.findall(<span class="string">r&quot;[A-Z]&quot;</span>, text.upper())</span><br><span class="line">        <span class="keyword">return</span> letters[:<span class="number">1</span>] <span class="keyword">if</span> letters <span class="keyword">else</span> []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> qtype == <span class="string">&quot;mc_multi&quot;</span>:</span><br><span class="line">        <span class="comment"># 多选题：提取所有大写字母并去重</span></span><br><span class="line">        letters = re.findall(<span class="string">r&quot;[A-Z]&quot;</span>, text.upper())</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">dict</span>.fromkeys(letters))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> qtype == <span class="string">&quot;binary&quot;</span>:</span><br><span class="line">        <span class="comment"># 判断题：识别True/False</span></span><br><span class="line">        truish = &#123;<span class="string">&quot;true&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="string">&quot;是&quot;</span>, <span class="string">&quot;对&quot;</span>, <span class="string">&quot;yes&quot;</span>&#125;</span><br><span class="line">        falsish = &#123;<span class="string">&quot;false&quot;</span>, <span class="string">&quot;f&quot;</span>, <span class="string">&quot;否&quot;</span>, <span class="string">&quot;不对&quot;</span>, <span class="string">&quot;no&quot;</span>&#125;</span><br><span class="line">        low = text.lower().strip()</span><br><span class="line">        <span class="keyword">if</span> low <span class="keyword">in</span> truish: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> low <span class="keyword">in</span> falsish: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 开放式：返回原文</span></span><br><span class="line">        <span class="keyword">return</span> text.strip()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_item</span>(<span class="params">gt: <span class="built_in">dict</span>, pred</span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估答案正确性&quot;&quot;&quot;</span></span><br><span class="line">    qtype = gt.get(<span class="string">&quot;QA_type&quot;</span>, <span class="string">&quot;&quot;</span>).lower()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> qtype == <span class="string">&quot;mc_single&quot;</span>:</span><br><span class="line">        gt_letter = gt[<span class="string">&quot;answer&quot;</span>][<span class="string">&quot;value&quot;</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;correct&quot;</span>: pred == [gt_letter], <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;accuracy&quot;</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> qtype == <span class="string">&quot;mc_multi&quot;</span>:</span><br><span class="line">        gt_set = <span class="built_in">set</span>(gt[<span class="string">&quot;answer&quot;</span>][<span class="string">&quot;value&quot;</span>])</span><br><span class="line">        pred_set = <span class="built_in">set</span>(pred) <span class="keyword">if</span> pred <span class="keyword">else</span> <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;correct&quot;</span>: gt_set == pred_set, <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;exact_set_match&quot;</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> qtype == <span class="string">&quot;binary&quot;</span>:</span><br><span class="line">        gt_bool = gt[<span class="string">&quot;answer&quot;</span>][<span class="string">&quot;value&quot;</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;correct&quot;</span>: pred == gt_bool, <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;accuracy&quot;</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 开放式：token overlap &gt;= 25% 或 LLM评分</span></span><br><span class="line">        <span class="keyword">return</span> evaluate_open_ended(gt, pred)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h3 id="MPT追踪机制"><a href="#MPT追踪机制" class="headerlink" title="MPT追踪机制"></a>MPT追踪机制</h3><p>记录每道题的首次答对时间和recall结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">mpt_tracker = &#123;&#125;  <span class="comment"># &#123;index: (t_star, [True, True, False, ...])&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始问题答对</span></span><br><span class="line"><span class="keyword">if</span> correct <span class="keyword">and</span> phase == <span class="string">&quot;initial&quot;</span>:</span><br><span class="line">    mpt_tracker[item[<span class="string">&quot;index&quot;</span>]] = (t_end, [<span class="literal">True</span>])</span><br><span class="line">    <span class="comment"># 安排recall...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Recall问题结果</span></span><br><span class="line"><span class="keyword">if</span> phase == <span class="string">&quot;recall&quot;</span>:</span><br><span class="line">    mpt_tracker[item[<span class="string">&quot;index&quot;</span>]][<span class="number">1</span>].append(correct)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 早停：一旦失败，移除后续recall</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> correct:</span><br><span class="line">        remove_future_recalls(item[<span class="string">&quot;index&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算MPT</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_mpt</span>(<span class="params">tracker_entry</span>):</span><br><span class="line">    t_star, results = tracker_entry</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> results[<span class="number">0</span>]:  <span class="comment"># 初始就错，MPT=0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, r <span class="keyword">in</span> <span class="built_in">enumerate</span>(results[<span class="number">1</span>:], <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> r:  <span class="comment"># 第i轮recall失败</span></span><br><span class="line">            <span class="keyword">return</span> i * <span class="number">60</span>  <span class="comment"># MPT = i * Δ</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">600</span>  <span class="comment"># 全部通过，MPT=600秒（上限）</span></span><br></pre></td></tr></table></figure>
<p><br></p>
<hr>
<p><br></p>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="RTA-指标结果"><a href="#RTA-指标结果" class="headerlink" title="RTA 指标结果"></a>RTA 指标结果</h2><p>我们在 TeleEgo 数据集上评测了多个多模态模型，结果如下表所示：</p>
<p><img src="image-20251129024632998.png" alt="image-20251129024632998" style="zoom:80%;"></p>
<p><br></p>
<p><br></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="伪流式模型"><a href="#伪流式模型" class="headerlink" title="伪流式模型"></a>伪流式模型</h3><p>VideoChat-Online、Qwen2.5-VL、Qwen2.5-Omni 三个模型在我们的严格流式评测下，每次只能看到当前帧，无法访问历史信息。</p>
<p><strong>音频输入能力的差异</strong></p>
<p>这三个模型在音频处理能力上存在显著差异：</p>
<ul>
<li><strong>Qwen2.5-Omni</strong>：是真正的 Omni 模型，支持音频的直接输入，能够端到端地理解语音内容</li>
<li><strong>VideoChat-Online、Qwen2.5-VL</strong>：不支持音频直接输入，我们只能通过 ASR（语音识别）将音频转写为文字，再作为文本加入 prompt</li>
</ul>
<p>语音转写的过程不可避免地引入误差——口语中的停顿、语气词、背景噪音都可能导致转写错误，进而影响模型的理解。</p>
<p><strong>结果分析</strong></p>
<ul>
<li><strong>Understanding 类任务表现尚可</strong>：这类任务主要依赖当前帧的理解能力，Qwen2.5-Omni 在 II（Immediate Identification）上达到 72.51%</li>
<li><strong>Memory 类任务表现较差</strong>：由于无法维持跨帧记忆，这类需要回忆过去信息的任务准确率普遍偏低</li>
<li><strong>Cross-Memory Reasoning 类任务表现较差</strong>：需要整合多个时间点的信息进行推理，伪流式模型在此类任务上表现也较弱</li>
</ul>
<p>三个伪流式模型中，Qwen2.5-Omni 表现最好（Overall 46.96%），这得益于两方面：一是其音视频联合理解能力——即使只看当前帧，音频中的语音信息也能提供额外线索；二是避免了 ASR 转写带来的误差累积。</p>
<p><br></p>
<h3 id="流式模型：MiniCPM-o"><a href="#流式模型：MiniCPM-o" class="headerlink" title="流式模型：MiniCPM-o"></a>流式模型：MiniCPM-o</h3><p>MiniCPM-o 是唯一支持真正流式输入的模型。我们测试了不同 session 窗口长度下的表现：</p>
<p><strong>关键发现：session_length 存在最优区间</strong></p>
<ul>
<li><strong>1min 窗口效果最佳</strong>：Overall 达到 54.1%，是所有模型中最高的</li>
<li><strong>窗口过大反而有害</strong>：3min 降至 50.57%，5min 降至 44.34%，10min 仅 30.15%</li>
</ul>
<p>这一现象的原因是：<strong>MiniCPM-o 没有设计动态的记忆管理机制</strong>。随着 KV Cache 不断增长：</p>
<ol>
<li><strong>注意力稀释</strong>：模型需要在越来越长的历史中寻找相关信息，导致注意力分散</li>
<li><strong>生成质量崩塌</strong>：session_length 超过 3 分钟后，模型开始出现文不对题、胡言乱语的现象</li>
</ol>
<p>下图展示了 session_length 设置为不同值时的 RTA：</p>
<p><img src="image-20251129024427285.png" alt="image-20251129024427285" style="zoom:80%;"></p>
<p>从表格可以看出，<strong>1分钟左右是最优的 session 窗口区间</strong>。这个区间既能保留足够的短期记忆来回答问题，又不会因为 KV Cache 过长而导致性能下降。</p>
<p><br></p>
<p><strong>长 session 下的生成紊乱现象</strong></p>
<p>当 session_length 设置过大时，MiniCPM-o 的生成质量会明显下降，出现文不对题、输出混乱的现象。以下是一个典型示例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;category&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Cross-Memory Reasoning&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;question&quot;</span><span class="punctuation">:</span> <span class="string">&quot;在&quot;</span>打篮球<span class="string">&quot;游戏中，我们各自控制的角色颜色是？&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;options&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="string">&quot;A. 我黄色、p1红色&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;B. 我红色、p1黄色&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;C. 我蓝色、p1紫色&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="string">&quot;D. 我绿色、p1蓝色&quot;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;gold&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;pred_text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;|listen|&gt; to the song 【当前子视频段信息】人物/场景描述：P1（星小凝）：女，体型偏瘦，身着&quot;</span><span class="punctuation">,</span></span><br><span class="line">  ...</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>可以看到，模型的输出完全偏离了问题：它没有尝试回答选择题，而是输出了一段混乱的内容——包含特殊 token <code>&lt;|listen|&gt;</code>、无关的英文片段、以及系统提示词的碎片。这表明在长时间运行后，模型的注意力机制已经无法正常工作，KV Cache 中积累的大量历史信息干扰了正常的推理过程。</p>
<p><br></p>
<h3 id="API-模型：GPT-4o-与-Gemini-2-5-Pro"><a href="#API-模型：GPT-4o-与-Gemini-2-5-Pro" class="headerlink" title="API 模型：GPT-4o 与 Gemini 2.5 Pro"></a>API 模型：GPT-4o 与 Gemini 2.5 Pro</h3><p>GPT-4o 和 Gemini 2.5 Pro 通过 API 调用，其内部实现对我们是黑盒。</p>
<p><strong>评测方式</strong></p>
<p>对于 GPT-4o，我们每次 API 调用只传入当前帧和问题；对于 Gemini 2.5 Pro，我们使用了 <code>chat.send_message_stream()</code> 接口。Gemini 的 Chat SDK 会自动在内部维护对话历史。为了考察模型本身的记忆能力而非 SDK 提供的历史累积功能，我们在每次调用后手动清空历史：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate_gemini25_pro.py</span></span><br><span class="line">response = chat.send_message_stream(unit_content)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> args.use_history:</span><br><span class="line">    chat._comprehensive_history = []</span><br><span class="line">    chat._curated_history = []</span><br></pre></td></tr></table></figure>
<p><strong>评测范围说明</strong></p>
<p>由于 TeleEgo 的视频时长较长（单条视频长达十多个小时），而 API 模型部署在云端，每次请求都涉及网络传输和排队等待，逐秒调用的评测流程耗时较长。目前我们仅对 GPT-4o 和 Gemini 2.5 Pro 各完成了 P2 一条视频的评测。在后续工作中，我们会补全其余视频的评测结果。</p>
<p><strong>结果分析</strong></p>
<p>GPT-4o 的 Understanding 能力突出（II 达到 81.81%，CU 达到 81.58%），显著领先其他模型。这是因为 Understanding 类任务主要基于语义理解和常识推理，不需要精确的时间定位，模型可以利用其强大的”通用知识”来辅助判断。</p>
<p><br></p>
<h3 id="Understanding-与-Memory-的能力错位"><a href="#Understanding-与-Memory-的能力错位" class="headerlink" title="Understanding 与 Memory 的能力错位"></a>Understanding 与 Memory 的能力错位</h3><p>从结果中可以观察到一个普遍现象：<strong>几乎所有模型的 Understanding 表现都显著优于 Memory 和 Cross-Memory Reasoning</strong>。</p>
<p>这种能力错位并非偶然。Understanding 类任务（如场景识别、动作理解、常识推理）很大程度上可以依赖模型的<strong>先验知识</strong>来”自圆其说”——即使没有看到完整的上下文，模型也能根据当前帧的视觉线索，结合自身训练时学到的世界知识，给出合理的推测。例如，看到厨房场景就可以推测”在做饭”，看到办公桌就可以推测”在工作”。</p>
<p>而 Memory 类任务则完全不同。当问题是”刚才放下的钥匙在哪里”或”之前遇到的那个人叫什么名字”时，模型无法依靠先验知识来回答——这些信息是<strong>特定于当前视频的</strong>，必须通过真正的”记忆”才能获取。没有记忆机制的支撑，模型只能依赖当前帧碰巧包含的信息，或者干脆猜测。特别是涉及实体追踪的任务——这类任务需要<strong>持续追踪</strong>目标，不是一次性识别就能完成，而是需要<strong>跨时间维持</strong>对实体状态的感知。</p>
<p>Cross-Memory Reasoning 也很具挑战性：它不仅需要记住多个时间点的信息，还需要将它们整合起来进行推理。例如”早上说要买的东西，下午买了吗？”这类问题需要同时回忆早上的对话内容和下午的购物行为，然后进行对比。</p>
<p>这一发现揭示了当前多模态模型的核心局限：它们更像是”理解者”而非”记忆者”——能够理解当下看到的内容，却难以记住过去发生的事情。</p>
<p><br></p>
<p><br></p>
<h2 id="关于-MPT-指标"><a href="#关于-MPT-指标" class="headerlink" title="关于 MPT 指标"></a>关于 MPT 指标</h2><p>我们在指标设计中提出了 MPT（Memory Persistence Time）来衡量记忆持久性。然而，<strong>当前评测结果中并未包含 MPT 指标</strong>。</p>
<p>原因在于：MPT 指标要求模型在首次答对问题后，能够在后续的 recall 测试中持续答对。这需要模型具备<strong>真正的长期记忆能力</strong>——不仅能在信息刚出现时正确回答，还能在信息消失很久后依然记得。</p>
<p>从我们的评测结果来看，当前模型都不适合进行 MPT 评测：</p>
<p><strong>伪流式模型</strong>（VideoChat-Online、Qwen2.5-VL、Qwen2.5-Omni）：每次调用都是独立的，根本没有跨调用的记忆。在这种情况下测 MPT 没有意义——模型第一次可能因为当前帧恰好包含相关信息而答对，后续的 recall 测试即使答对，也只是基于相同的”猜测策略”而非真正的”记忆”。换句话说，如果一个没有记忆的模型在 initial 和 recall 阶段都答对了，我们无法区分这是”记住了”还是”又蒙对了”。</p>
<p><strong>API 模型</strong>（GPT-4o、Gemini 2.5 Pro）：其内部实现对我们是黑盒。我们通过 API 调用进行评测，这与面向用户的聊天界面（如 ChatGPT App、Gemini Web UI）可能存在差异。在我们的评测设置下（不简单粗暴地累积历史），它们面临与伪流式模型相同的问题。</p>
<p><strong>流式模型</strong>（MiniCPM-o）：虽然支持流式输入和跨调用的 KV Cache，但由于其缺乏动态记忆管理，我们不得不每隔几分钟手动重置 session。这导致了一个尴尬的情况：如果某个问题的 initial 测试和 recall 测试恰好跨越了 session 重置的边界，那么模型的记忆在两次测试之间被人为清空了——此时即使 recall 失败，我们也无法判断是模型”遗忘”了，还是因为 session 重置导致的。这使得 MPT 的测量结果不具备可比性。</p>
<p><strong>MPT 是一个面向未来的指标</strong>。当前在模型层面真正实现流式处理、具备长期记忆能力的模型还非常稀少。我们相信，随着流式多模态模型的发展，会有更多模型具备真正的长期记忆能力——能够在不重置 session 的情况下持续运行数小时，同时保持稳定的生成质量。届时，MPT 将成为评估这些模型的重要指标。我们将 MPT 的评测框架和代码开源，供后续研究者使用。</p>
<p><br></p>
<p><br></p>
<h2 id="其他模型的适配问题"><a href="#其他模型的适配问题" class="headerlink" title="其他模型的适配问题"></a>其他模型的适配问题</h2><p>我们也尝试将一些其他视频理解模型适配到 TeleEgo 的流式评测框架中，但发现存在一些根本性的不兼容。</p>
<h3 id="M3-Agent"><a href="#M3-Agent" class="headerlink" title="M3-Agent"></a>M3-Agent</h3><p>M3-Agent 采用逐 clip 输入的方式，默认以 30 秒为一个 clip，每个 clip 提取对话、动作等信息存入记忆模块。这种设计在处理 30 分钟左右的视频时是合理的——30 秒的片段通常包含完整的对话轮次和有意义的动作序列。</p>
<p>但如果将 M3-Agent 的信息提取方式应用到 TeleEgo 的评测pipeline中，会遇到<strong>粒度不匹配</strong>的问题：M3-Agent 的记忆提取模块是为 30 秒的完整片段设计的，期望从中提取出完整的对话内容和动作描述。而在 TeleEgo 的 1 秒粒度场景下，按照这种做法对每个 1 秒片段进行信息提取，难以获得有意义的结构化记忆——<strong>这并非 1 秒输入本身没有价值，而是 M3-Agent 的提取方式不适配这一粒度</strong>。</p>
<p><br></p>
<h3 id="StreamChat"><a href="#StreamChat" class="headerlink" title="StreamChat"></a>StreamChat</h3><p>StreamChat 采用分层记忆存储（Hierarchical Memory Storage）的设计，包括短期记忆、长期记忆和对话记忆。其中，长期记忆的构建依赖于对视频片段进行 <strong>captioning</strong>——将一段视频的视觉特征压缩为文本描述（text clues），作为后续检索的索引。</p>
<p>这种方式在我们的pipeline中仍是<strong>伪流式</strong>。问题在于：captioning 需要调用 MLLM 进行文本生成（自回归逐 token decode），这比单纯的前向传播（prefill）慢得多。按照我们对流式的定义，模型需要能够逐秒接收输入。如果每一秒都要 generate 一段文本摘要，推理速度将远远跟不上实时输入的速度。</p>
<p>这些模型的设计思路各有其合理性，但都基于一个隐含假设：<strong>视频输入是预先分段的</strong>。而 TeleEgo 的评测场景——14 小时的连续的视频流、随时可能出现的问题、1 秒粒度的实时输入——对模型提出了更严格的要求。</p>
<p><br></p>
<hr>
<p><br></p>
<h1 id="小结与展望"><a href="#小结与展望" class="headerlink" title="小结与展望"></a>小结与展望</h1><p>通过对多个多模态大模型在 TeleEgo 上的评测与分析，我们发现当前模型在流式视频理解任务上仍面临显著挑战。</p>
<p><strong>伪流式模型</strong>（VideoChat-Online、Qwen2.5-VL、Qwen2.5-Omni）虽然能够处理视频输入，但本质上是”一次性”的推理模式——每次调用都是独立的，模型无法在多次调用之间保持对视频流的持续感知。这意味着随着视频时长增加，要么需要重复编码大量历史帧导致计算开销激增，要么只能丢弃早期信息导致记忆能力受限。</p>
<p><strong>流式模型</strong>（MiniCPM-o）展现了真正的流式处理能力：通过 KV Cache 的跨调用持久化，模型能够增量处理视频流而无需重复编码历史内容。然而，由于缺乏动态的记忆管理机制，随着 session 时长增加，注意力逐渐稀释、生成质量下降，实际可用的 session 长度被限制在 1-2 分钟左右。</p>
<p><br></p>
<p>TeleEgo 的评测结果揭示了当前多模态大模型在流式场景下的能力边界，也指明了未来研究的方向。</p>
<p><strong>动态记忆管理</strong>是实现长时间流式理解的关键。理想的模型应当能够在持续接收视频流的同时，智能地决定哪些信息值得长期保留、哪些可以压缩或遗忘。这不仅是工程优化问题，更涉及到对”记忆”本质的建模——如何在有限的计算资源下，维持对长时间跨度事件的连贯理解。</p>
<p><strong>MPT（Memory Persistence Time）指标</strong>目前对所有测试模型都不适用，但它代表了我们对未来模型的期待：能够在不重置 session 的情况下持续运行数小时，同时保持稳定的生成质量和可靠的记忆能力。当这样的模型出现时，MPT 将成为衡量其记忆持久性的重要指标。</p>
<p>从更宏观的视角看，<strong>流式全模态第一人称理解</strong>不仅是技术挑战，更是通向具身智能和实时人机交互的必经之路。一个能够持续观察环境、积累经验、并在需要时调用相关记忆的 AI 系统，将在机器人、智能助手、增强现实等领域发挥重要作用。TeleEgo 作为一个评测基准，希望能为这一方向的研究提供有价值的参考。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://seline02.github.io">Seline</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://seline02.github.io/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/">https://seline02.github.io/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://seline02.github.io" target="_blank">Seline's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/logo.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/user_pic.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Seline</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Seline02" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:17805745984@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E5%8A%A8%E6%9C%BA"><span class="toc-number">1.</span> <span class="toc-text">背景与动机</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%B6%E4%BD%9C"><span class="toc-number">2.</span> <span class="toc-text">数据集制作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.1.</span> <span class="toc-text">数据采集设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="toc-number">2.2.</span> <span class="toc-text">数据处理流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Benchmark%E4%BB%BB%E5%8A%A1%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.3.</span> <span class="toc-text">Benchmark任务设计</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BB%93%E6%9E%84%E8%AE%B2%E8%A7%A3"><span class="toc-number">3.</span> <span class="toc-text">数据集结构讲解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E6%80%BB%E8%A7%88"><span class="toc-number">3.1.</span> <span class="toc-text">目录结构总览</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E8%BD%B4%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3"><span class="toc-number">3.2.</span> <span class="toc-text">时间轴文件详解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E8%AF%A6%E8%A7%A3"><span class="toc-number">3.3.</span> <span class="toc-text">问答数据详解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#P1%E8%A7%86%E9%A2%91%E7%89%87%E6%AE%B5%E4%B8%80%E8%A7%88"><span class="toc-number">3.4.</span> <span class="toc-text">P1视频片段一览</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">4.</span> <span class="toc-text">评估指标</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Real-Time-Accuracy-RTA-%E5%AE%9E%E6%97%B6%E5%87%86%E7%A1%AE%E7%8E%87"><span class="toc-number">4.1.</span> <span class="toc-text">Real-Time Accuracy (RTA) - 实时准确率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Memory-Persistence-Time-MPT-%E8%AE%B0%E5%BF%86%E6%8C%81%E7%BB%AD%E6%97%B6%E9%97%B4"><span class="toc-number">4.2.</span> <span class="toc-text">Memory Persistence Time (MPT) - 记忆持续时间</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B5%81%E5%BC%8F%E8%AF%84%E6%B5%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.</span> <span class="toc-text">流式评测实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E5%BC%8F%E8%AF%84%E6%B5%8B%E7%9A%84%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="toc-number">5.1.</span> <span class="toc-text">流式评测的整体架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E8%AF%84%E6%B5%8B%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.</span> <span class="toc-text">核心评测流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B5%81%E5%BC%8F%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F"><span class="toc-number">5.3.</span> <span class="toc-text">不同模型的流式实现方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%AA%E6%B5%81%E5%BC%8F%EF%BC%9AVideoChat-Online"><span class="toc-number">5.3.1.</span> <span class="toc-text">伪流式：VideoChat-Online</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#VideoChatOnline-Stream%E7%9A%84Pyramid-Memory-Bank%E6%9C%BA%E5%88%B6"><span class="toc-number">5.3.1.1.</span> <span class="toc-text">VideoChatOnline_Stream的Pyramid Memory Bank机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#VideoChatOnline-IT-%E7%9A%84%E5%AF%B9%E8%AF%9D%E5%8E%86%E5%8F%B2%E6%9C%BA%E5%88%B6"><span class="toc-number">5.3.1.2.</span> <span class="toc-text">VideoChatOnline_IT 的对话历史机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%98%E6%96%B9%E8%AF%84%E6%B5%8B%E8%84%9A%E6%9C%AC%E5%88%86%E6%9E%90"><span class="toc-number">5.3.1.3.</span> <span class="toc-text">官方评测脚本分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%84%E6%B5%8B%E8%AE%BE%E8%AE%A1"><span class="toc-number">5.3.1.4.</span> <span class="toc-text">我们的评测设计</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%AA%E6%B5%81%E5%BC%8F%EF%BC%9AQwen2-5-Omni"><span class="toc-number">5.3.2.</span> <span class="toc-text">伪流式：Qwen2.5-Omni</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%98%E6%96%B9%E7%9A%84%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E6%96%B9%E5%BC%8F"><span class="toc-number">5.3.2.1.</span> <span class="toc-text">官方的多轮对话方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A5%E5%8F%A3%E5%B1%82%E9%9D%A2%E7%9A%84%E9%99%90%E5%88%B6"><span class="toc-number">5.3.2.2.</span> <span class="toc-text">接口层面的限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%84%E6%B5%8B%E8%AE%BE%E8%AE%A1-1"><span class="toc-number">5.3.2.3.</span> <span class="toc-text">我们的评测设计</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%81%E5%BC%8F%E6%A8%A1%E5%9E%8B%EF%BC%9AMiniCPM-o-2-6"><span class="toc-number">5.3.3.</span> <span class="toc-text">流式模型：MiniCPM-o 2.6</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%98%BE%E5%BC%8F%E7%9A%84%E6%B5%81%E5%BC%8F%E6%8E%A5%E5%8F%A3"><span class="toc-number">5.3.3.1.</span> <span class="toc-text">显式的流式接口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E5%BC%8F%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%B1%80%E9%99%90"><span class="toc-number">5.3.3.2.</span> <span class="toc-text">流式接口的局限</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%84%E6%B5%8B%E8%AE%BE%E8%AE%A1-2"><span class="toc-number">5.3.3.3.</span> <span class="toc-text">我们的评测设计</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E4%BB%A3%E7%A0%81%E9%80%BB%E8%BE%91%E8%AF%A6%E8%A7%A3"><span class="toc-number">5.4.</span> <span class="toc-text">关键代码逻辑详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E6%88%B3%E6%98%A0%E5%B0%84%EF%BC%88TimelineIndex%EF%BC%89"><span class="toc-number">5.4.1.</span> <span class="toc-text">时间戳映射（TimelineIndex）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E9%98%9F%E5%88%97%E7%AE%A1%E7%90%86"><span class="toc-number">5.4.2.</span> <span class="toc-text">问题队列管理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E7%89%A9%E5%85%88%E9%AA%8C%E6%B3%A8%E5%85%A5"><span class="toc-number">5.4.3.</span> <span class="toc-text">人物先验注入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E7%A7%92%E5%86%B3%E7%AD%96%E7%AA%97%E5%8F%A3"><span class="toc-number">5.4.4.</span> <span class="toc-text">5秒决策窗口</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%94%E6%A1%88%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="toc-number">5.4.5.</span> <span class="toc-text">答案解析与评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPT%E8%BF%BD%E8%B8%AA%E6%9C%BA%E5%88%B6"><span class="toc-number">5.4.6.</span> <span class="toc-text">MPT追踪机制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">6.</span> <span class="toc-text">实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RTA-%E6%8C%87%E6%A0%87%E7%BB%93%E6%9E%9C"><span class="toc-number">6.1.</span> <span class="toc-text">RTA 指标结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90"><span class="toc-number">6.2.</span> <span class="toc-text">分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%AA%E6%B5%81%E5%BC%8F%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.2.1.</span> <span class="toc-text">伪流式模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%81%E5%BC%8F%E6%A8%A1%E5%9E%8B%EF%BC%9AMiniCPM-o"><span class="toc-number">6.2.2.</span> <span class="toc-text">流式模型：MiniCPM-o</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#API-%E6%A8%A1%E5%9E%8B%EF%BC%9AGPT-4o-%E4%B8%8E-Gemini-2-5-Pro"><span class="toc-number">6.2.3.</span> <span class="toc-text">API 模型：GPT-4o 与 Gemini 2.5 Pro</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Understanding-%E4%B8%8E-Memory-%E7%9A%84%E8%83%BD%E5%8A%9B%E9%94%99%E4%BD%8D"><span class="toc-number">6.2.4.</span> <span class="toc-text">Understanding 与 Memory 的能力错位</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E-MPT-%E6%8C%87%E6%A0%87"><span class="toc-number">6.3.</span> <span class="toc-text">关于 MPT 指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%80%82%E9%85%8D%E9%97%AE%E9%A2%98"><span class="toc-number">6.4.</span> <span class="toc-text">其他模型的适配问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#M3-Agent"><span class="toc-number">6.4.1.</span> <span class="toc-text">M3-Agent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#StreamChat"><span class="toc-number">6.4.2.</span> <span class="toc-text">StreamChat</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B"><span class="toc-number">7.</span> <span class="toc-text">小结与展望</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/" title="TeleEgo|流式全模态第一人称评测基准"><img src="/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/logo.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TeleEgo|流式全模态第一人称评测基准"/></a><div class="content"><a class="title" href="/2025/11/10/TeleEgo-%E6%B5%81%E5%BC%8F%E5%85%A8%E6%A8%A1%E6%80%81%E7%AC%AC%E4%B8%80%E4%BA%BA%E7%A7%B0%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86/" title="TeleEgo|流式全模态第一人称评测基准">TeleEgo|流式全模态第一人称评测基准</a><time datetime="2025-11-10T04:58:16.000Z" title="发表于 2025-11-10 12:58:16">2025-11-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Seline</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>